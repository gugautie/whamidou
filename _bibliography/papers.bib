---
---

@ARTICLE{9465693,
abbr={OJSP},
selected={false},
bibtex_show={true},
award={true},
author={Nasiri, Fatemeh and Hamidouche, Wassim and Morin, Luce and Dhollande, Nicolas and Cocherel, Gildas},
journal={IEEE Open Journal of Signal Processing}, title={A CNN-based Prediction-Aware Quality Enhancement Framework for VVC},
year={2021},
volume={},
number={},
pages={1-1},
abstract={This paper presents a framework for Convolutional Neural Network (CNN)-based quality enhancement task, by taking advantage of coding information in the compressed video signal. The motivation is that normative decisions made by the encoder can significantly impact the type and strength of artifacts in the decoded images. In this paper, the main focus has been put on decisions defining the prediction signal in intra and inter frames. This information has been used in the training phase as well as input to help the process of learning artifacts that are specific to each coding type. Furthermore, to retain a low memory requirement for the proposed method, one model is used for all Quantization Parameters (QPs) with a QP-map, which is also shared between luma and chroma components. In addition to the Post Processing (PP) approach, the In-Loop Filtering (ILF) codec integration has also been considered, where the characteristics of the Group of Pictures (GoP) are taken into account to boost the performance. The proposed CNN-based Quality Enhancement (QE) framework has been implemented on top of the Versatile Video Coding (VVC) Test Model (VTM-10). Experiments show that the prediction-aware aspect of the proposed method improves the coding efficiency gain of the default CNN-based QE method by 1.52%, in terms of BD-BR, at the same network complexity compared to the default CNN-based QE filter.},
keywords={Encoding;Training;Video coding;Task analysis;Image reconstruction;Quantization (signal);Image coding;CNN;VVC;Quality Enhancement;In-Loop Filtering;Post-Processing},
doi={10.1109/OJSP.2021.3092598},
ISSN={2644-1322},
pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9465693},
month={},}

@ARTICLE{9399291,
abbr={TMM},
selected={true},
bibtex_show={true},
author={Bakir, Nader and Hamidouche, Wassim and Fezza, Sid Ahmed and Samrout, Khouloud and Deforges, Olivier},
journal={IEEE Transactions on Multimedia}, title={Light Field Image Coding Using VVC standard and View Synthesis based on Dual Discriminator GAN},
year={2021},
volume={},
number={},
pages={1-1},
abstract={Light field (LF) technology is considered as a promising way for providing a high-quality virtual reality (VR) content. However, such an imaging technology produces a large amount of data requiring efficient LF image compression solutions. In this paper, we propose a LF image coding method based on a view synthesis and view quality enhancement techniques. Instead of transmitting all the LF views, only a sparse set of reference views are encoded and transmitted, while the remaining views are synthesized at the decoder side. The transmitted views are encoded using the versatile video coding (VVC) standard and are used as reference views to synthesize the dropped views. The selection of non-reference dropped views is performed using a rate-distortion optimization based on the VVC temporal scalability. The dropped views are reconstructed using the LF dual discriminator GAN (LF-D2GAN) model. In addition, to ensure that the quality of the views is consistent, at the decoder, a quality enhancement procedure is performed on the reconstructed views allowing smooth navigation across views. Experimental results show that the proposed method provides high coding performance and overcomes the state-of-the-art LF image compression methods by -36.22% in terms of BD-BR and 1.35 dB in BD-PSNR.},
keywords={Image coding;Encoding;Cameras;Decoding;Transforms;Transform coding;Standards;Light Field;View Synthesis;Deep Learning;VVC;Coding Structure;RDO;Quality Enhancement},
doi={10.1109/TMM.2021.3068563},
ISSN={1941-0077},
month={},}
@ARTICLE{9122430,
abbr={TMM},
selected={false},
bibtex_show={true},
author={Chao, Fang-Yi and Zhang, Lu and Hamidouche, Wassim and Déforges, Olivier},
journal={IEEE Transactions on Multimedia}, title={A Multi-FoV Viewport-Based Visual Saliency Model Using Adaptive Weighting Losses for 360° Images},
year={2021},
volume={23},
number={},
pages={1811-1826},
abstract={360° media allows observers to explore the scene in all directions. The consequence is that the human visual attention is guided by not only the perceived area in the viewport but also the overall content in 360$^\circ$. In this paper, we propose a method to estimate the 360° saliency map which extracts salient features from the entire 360° image in each viewport in three different Field of Views (FoVs). Our model is first pretrained with a large-scale 2D image dataset to enable the interpretation of semantic contents, then fine-tuned with a relative small 360$^\circ$ image dataset. A novel weighting loss function attached with stretch weighted maps is introduced to adaptively weight the losses of three evaluation metrics and attenuate the impact of stretched regions in equirectangular projection during training process. Experimental results demonstrate that our model achieves better performance with the integration of three FoVs and its diverse viewport images. Results also show that the adaptive weighting losses and stretch weighted maps effectively enhance the evaluation scores compared to the fixed weighting losses solutions. Comparing to other state of the art models, our method surpasses them on three different datasets and ranks the top using 5 performance evaluation metrics on the Salient360! benchmark set. The code is available at https://github.com/FannyChao/MV-SalGAN360.},
keywords={Two dimensional displays;Feature extraction;Adaptation models;Visualization;Measurement;Predictive models;Videos;Human eye fixation;saliency;omnidirectional image;convolutional neural network;deep learning},
doi={10.1109/TMM.2020.3003642},
ISSN={1941-0077},
month={},}

@article{NCACVIP,
abbr={Springer},
selected={true},
bibtex_show={true},
author={Anouar Kherchouche and 
Sid Ahmed Fezza and 
Wassim Hamidouche},
journal={Neural Computing and Applications}, title={Detect and defense against adversarial examples in deep learning using natural scene statistics and adaptive denoising},
year={2021},
volume={},
number={},
pages={1-1},
abstract={Despite the enormous performance of deep neural networks (DNNs), recent studies have shown their vulnerability to adversarial examples (AEs), i.e., carefully perturbed inputs designed to fool the targeted DNN. Currently, the literature is rich with many effective attacks to craft such AEs. Meanwhile, many defense strategies have been developed to mitigate this vulnerability. However, these latter showed their effectiveness against specific attacks and does not generalize well to different attacks. In this paper, we propose a framework for defending DNN classifier against adversarial samples. The proposed method is based on a two-stage framework involving a separate detector and a denoising block. The detector aims to detect AEs by characterizing them through the use of natural scene statistic (NSS), where we demonstrate that these statistical features are altered by the presence of adversarial perturbations. The denoiser is based on block matching 3D (BM3D) filter fed by an optimum threshold value estimated by a convolutional neural network (CNN) to project back the samples detected as AEs into their data manifold. We conducted a complete evaluation on three standard datasets, namely MNIST, CIFAR-10 and Tiny-ImageNet. The experimental results show that the proposed defense method outperforms the state-of-the-art defense techniques by improving the robustness against a set of attacks under black-box, gray-box and white-box settings. The source code is available at: https://​github.​com/​kherchouche-anouar/​2DAE.},
doi={10.1007/s00521-021-06330-x},
ISSN={1433-3058},
month={July},}


@INPROCEEDINGS{9483966,
abbr={EUVIP},
bibtex_show={true},
author={Fezza, Sid Ahmed and Keita, Mamadou and Hamidouche, Wassim},
booktitle={2021 9th European Workshop on Visual Information Processing (EUVIP)}, title={Visual Quality and Security Assessment of Perceptually Encrypted Images Based on Multi-Output Deep Neural Network},
year={2021},
volume={},
number={},
pages={1-6},
abstract={Encryption has became an indispensable technique for image/video-based applications. This has led to the development of many image encryption algorithms, such as perceptual/selective encryption methods which represent an effective way for the security and confidentiality of images. However, few studies focus on visual security metric, which is very important tool for evaluating the effectiveness of these encryption methods. Most of the adopted metrics are the classical randomness-based measures or the objective image quality assessment metrics. However, these metrics showed their limits as a visual security metric, because they do not deal with the content intelligibility, which is one of the key security requirements. Consequently, in this paper, we propose a no-reference (NR) visual security metric for perceptually encrypted images based on multi-output learning called VSMML. The proposed metric consists of a convolutional neural network (CNN) taking as input an encrypted image and providing two outputs corresponding to the visual security (VS) and visual quality (VQ) scores. Experiments were performed on two publicly perceptually encrypted image databases and the results show that the proposed metric significantly outperforms the state-of-the-art methods for visual security and quality assessment tasks. The source code is available at: https://github.com/Mamadou-Keita/VSMML.},
keywords={Measurement;Image quality;Visualization;Image databases;Image color analysis;Information processing;Tools;Visual security assessment;Image quality;Perceptual encryption;Scrambling;Multi-output;Deep learning},
doi={10.1109/EUVIP50544.2021.9483966},
ISSN={2471-8963},
month={June},}

@inproceedings{10.1145/3441110.3441153,
abbr={DASIP},
bibtex_show={true},
author = {Haggui, Naouel and Belghith, Fatma and Hamidouche, Wassim and Masmoudi, Nouri and Nezan, Jean-Fran\c{c}ois},
title = {Multiple Transform Selection Concept Modeling and Implementation Using Interface Based SDF Graphs},
year = {2021},
isbn = {9781450389013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441110.3441153},
doi = {10.1145/3441110.3441153},
abstract = { Recent studies predict that video data accounts for 82% of Internet traffic by 2022.
This fact has motivated MPEG to define a new Video Coding Standard called Versatile
Video Coding (VVC), which will be released by the end of 2020. VVC will offer the
possibility to handle new video formats and to improve significantly video compression
over its predecessor HEVC. Indeed, the objective is to reduce the necessary bit rate
by half, at equivalent quality. These advances require the use of more complex algorithms,
although the increase in complexity has been limited throughout the standardization
process. In order to decrease the complexity of VVC and consequently the coding execution
time, several methods have been introduced at different stages of the encoder. The
aim of this paper is to explore the available parallelism of VVC to accelerate the
coding and the decoding processes. This paper focuses on the transformation block
and more specifically the new concept of Multiple Transform Selection (MTS) introduced
by VVC. Moreover, a study of several granularity levels of Interface-Based Synchronous
Dataflow (IBSDF) models and their impact on the performances obtained on x86 architectures
is presented. IBSDF dataflow graph has been developed to reveal the available parallelism
of MTS. The PREESM fast prototyping tool is then used for the mapping and the scheduling
of MTS on virtual and real parallel architectures and for generating efficient parallel
implementations on real architectures. PREESM has been used in this work to explore
the potential parallelism offered by MTS and to prove the efficiency of MTS on multicore
x86 architectures. Experimental results show a speed-up close to the optimum.},
booktitle = {Workshop on Design and Architectures for Signal and Image Processing (14th Edition)},
pages = {60–67},
numpages = {8},
keywords = {PREESM, Granularity, Versatile Video Coding, Multiple Transform Selection, Dataflow},
location = {Budapest, Hungary},
series = {DASIP '21}
}



@article{9305217,
abbr={TCSVT},
selected={true},
bibtex_show={true},
author={Herrou, Glenn and Bonnineau, Charles and Bonnineau, Charles and Hamidouche, Wassim and Dumenil, Patrick and Fournier, Jérôme and Morin, Luce},
journal={IEEE Transactions on Circuits and Systems for Video Technology}, title={Quality-driven Variable Frame-Rate for Green Video Coding in Broadcast Applications},
year={2020},
volume={},
number={},
pages={1-1},
abstract={The Digital Video Broadcasting (DVB) has proposed to introduce the Ultra-High Definition services in three phases: UHD-1 phase 1, UHD-1 phase 2 and UHD-2. The UHD-1 phase 2 specification includes several new features such as High Dynamic Range (HDR) and High Frame-Rate (HFR). It has been shown in several studies that HFR (+100 fps) enhances the perceptual quality and that this quality enhancement is content-dependent. On the other hand, HFR brings several challenges to the transmission chain including codec complexity increase and bit-rate overhead, which may delay or even prevent its deployment in the broadcast echo-system. In this paper, we propose a Variable Frame Rate (VFR) solution to determine the minimum (critical) frame-rate that preserves the perceived video quality of HFR video. The frame-rate determination is modeled as a 3-class classification problem which consists in dynamically and locally selecting one frame-rate among three: 30, 60 and 120 frames per second. Two random forests classifiers are trained with a ground truth carefully built by experts for this purpose. The subjective results conducted on ten HFR video contents, not included in the training set, clearly show the efficiency of the proposed solution enabling to locally determine the lowest possible frame-rate while preserving the quality of the HFR content. Moreover, our VFR solution enables significant bit-rate savings and complexity reductions at both encoder and decoder sides.},
keywords={UHDTV;Encoding;Complexity theory;Cameras;Visualization;Video recording;TV;High Frame-Rate (HFR);variable frame-rate;Ultra-High Definition (UHD);High Efficiency Video Coding (HEVC)},
doi={10.1109/TCSVT.2020.3046881},
pdf={https://arxiv.org/pdf/2012.14796.pdf},
ISSN={1558-2205},
month={June},}

@article{SMPTEcharles,
abbr={SMPTE},
selected={false},
bibtex_show={true},
author={Charles Bonnineau and Jean-Yves Aubié and Wassim Hamidouche and  Olivier  Déforges and Jean-FrançoisTravers and Naty Sidaty},
journal={SMPTE Motion Imaging Journal. }, title={An Objective Evaluation of Codecs and Post-Processing Tools for 8K VideoCompression},
year={2020},
volume={},
number={},
pages={},
abstract={With the deployment of the latest Ultra High Definition Television (UHDTV) system, Quality of Experience (QoE) of users is expected to be improved through the introduction of new features to the existing High Definition Television (HDTV) system, such as High Dynamic Range (HDR), wider color gamut, High Frame-Rate (HFR) and higher spatial resolutions including 4K (3840x2160) and 8K (7680x4320). The delivery of such video formats on current broadcast infrastructures is a real challenge and requires efficient compression methods to reach the available bandwidth while ensuring high video quality. On the other hand, with the outstanding performance of AI-based spatial up-scalers, TV manufacturers have recently included them as a feature in their products. As 8K contents are still uncommon, it can be useful to reconstruct high-quality 8K from a lower resolution to exploit the screen capabilities. Moreover, these filters can also be used to reduce the high bitrate requirements of 8K by downscaling the original 8K signal before encoding and reconstruct it after decoding. In this paper, we propose to evaluate different video coding scenarios for 8K compression using VVC and HEVC standards through an objective study. Tested configurations include: 8K source encoded with HEVC, and 8K and 4K sources encoded with VVC and then upscaled with two methods: a Lanczos filter and a deep-learning-based Super-Resolution method called SRFBN. All configurations are tested on a set of 8K sequences using the verification model of the VVC and HEVC standards.},
keywords={UHDTV;Encoding;Complexity theory;Cameras;Visualization;Video recording;TV;High Frame-Rate (HFR);variable frame-rate;Ultra-High Definition (UHD);High Efficiency Video Coding (HEVC)},
month={September},}




@ARTICLE{8907817,
abbr={TCSVT},
selected={false},
bibtex_show={true},
author={Kammoun, Ahmed and Hamidouche, Wassim and Philippe, Pierrick and Déforges, Olivier and Belghith, Fatma and Masmoudi, Nouri and Nezan, Jean-François},
journal={IEEE Transactions on Circuits and Systems for Video Technology}, title={Forward-Inverse 2D Hardware Implementation of Approximate Transform Core for the VVC Standard},
year={2020},
volume={30},
number={11},
pages={4340-4354},
abstract={The future video coding standard named Versatile Video Coding (VVC) is expected by the end of 2020. VVC will enable better coding efficiency than the current High Efficiency Video Coding (HEVC) standard. This coding gain is brought by several coding tools. The Multiple Transform Selection (MTS) is one of the key coding tools that have been introduced in VVC. The MTS concept relies on three transform types including Discrete Cosine Transform (DCT)-II, Discrete Sine Transform (DST)-VII and DCT-VIII. Unlike the DCT-II that has fast computing algorithms, the DST-VII and DCT-VIII rely on more complex matrix multiplication. In this paper an approximation approach is proposed to reduce the computational cost of the DST-VII and DCT-VIII. The approximation consists in applying adjustment stages, based on sparse block-band matrices, to a variant of DCT-II family mainly DCT-II and its inverse. Genetic algorithm is used to derive the optimal coefficients of the adjustment matrices. Moreover, an efficient hardware implementation of the forward and inverse approximate transform module is proposed. The architecture design includes a pipelined and reconfigurable forward-inverse DCT-II core transform as it is the main core for DST-VII and DCT-VIII computations. The proposed 32-point 1D architecture including low cost adjustment stages allows the processing of a video in 2K and 4K resolutions at 1095 and 273 frames per second, respectively. A unified 2D implementation of forward-inverse DCT-II, approximate DST-VII and DCT-VIII is also presented. The synthesis results show that the design is able to sustain a video in 2K and 4K resolutions at 386 and 96 frames per second, respectively, while using only 12% of Alms, 22% of registers and 30% of DSP blocks of the Arria10 SoC platform.},
keywords={Transforms;Hardware;Encoding;Two dimensional displays;Video coding;Standards;Computer architecture;Versatile video coding;hardware implementation;approximation;DCT-II;adjustment stages;FPGA},
doi={10.1109/TCSVT.2019.2954749},
ISSN={1558-2205},
month={Nov},}
@ARTICLE{8854328,
abbr={TB},
selected={false},
bibtex_show={true},
author={Biatek, Thibaud and Hamidouche, Wassim and Cabarat, Pierre-Loup and Travers, Jean-François and Déforges, Olivier},
journal={IEEE Transactions on Broadcasting}, title={Scalable Video Coding for Backward-Compatible 360° Video Delivery Over Broadcast Networks},
year={2020},
volume={66},
number={2},
pages={322-332},
abstract={Recently, the coding and transmission of immersive 360° video has been intensely studied. The technologies provided by standards developing organizations mainly address requirements coming from over-the-top services. The terrestrial broadcast remains in many countries the mainstream medium to deliver high quality contents to a wide audience. To enable seamless introduction of immersive 360° video services over terrestrial broadcast, the deployed technologies shall fulfill requirements such as backward compatibility to legacy receivers and high bandwidth efficiency. While bandwidth efficiency is addressed by existing techniques, none of them enables legacy video services decoding. In this paper, a novel scalable coding scheme is proposed to enable immersive 360° video services introduction over broadcast networks. The experiments show that the proposed scalable coding scheme provides substantial coding gains of 14.99% compared to simulcast coding and introduces a limited coding overhead of 5.15% compared to 360° single-layer coding. A real-time decoding implementation is proposed, highlighting the relevance of the proposed design. Eventually, an end-to-end demonstrator illustrates how the proposed solution could be integrated in a real terrestrial broadcast environment.},
keywords={Encoding;Streaming media;Decoding;Two dimensional displays;Bandwidth;Video coding;Transform coding;Video coding;scalability;HEVC;SHVC;UHD;4K;360??;broadcast;broadband},
doi={10.1109/TBC.2019.2941073},
ISSN={1557-9611},
month={June},}
@ARTICLE{8826595,
abbr={TIP},
abbr.award={true},
selected={false},
bibtex_show={true},
author={Amestoy, Thomas and Mercat, Alexandre and Hamidouche, Wassim and Menard, Daniel and Bergeron, Cyril},
journal={IEEE Transactions on Image Processing}, title={Tunable VVC Frame Partitioning Based on Lightweight Machine Learning},
year={2020},
volume={29},
number={},
pages={1313-1328},
abstract={Block partition structure is a critical module in video coding scheme to achieve significant gap of compression performance. Under the exploration of the future video coding standard, named Versatile Video Coding (VVC), a new Quad Tree Binary Tree (QTBT) block partition structure has been introduced. In addition to the QT block partitioning defined in High Efficiency Video Coding (HEVC) standard, new horizontal and vertical BT partitions are enabled, which drastically increases the encoding time compared to HEVC. In this paper, we propose a lightweight and tunable QTBT partitioning scheme based on a Machine Learning (ML) approach. The proposed solution uses Random Forest classifiers to determine for each coding block the most probable partition modes. To minimize the encoding loss induced by misclassification, risk intervals for classifier decisions are introduced in the proposed solution. By varying the size of risk intervals, tunable trade-off between encoding complexity reduction and coding loss is achieved. The proposed solution implemented in the JEM-7.0 software offers encoding complexity reductions ranging from 30% to 70% in average for only 0.7% to 3.0% Bjøntegaard Delta Rate (BD-BR) increase in Random Access (RA) coding configuration, with very slight overhead induced by Random Forest. The proposed solution based on Random Forest classifiers is also efficient to reduce the complexity of the Multi-Type Tree (MTT) partitioning scheme under the VTM-5.0 software, with complexity reductions ranging from 25% to 61% in average for only 0.4% to 2.2% BD-BR increase.},
keywords={Complexity theory;Image coding;Encoding;Copper;High efficiency video coding;Radio frequency;Video compression;VVC;JEM;VTM;QTBT;complexity reduction;machine learning;random forest},
doi={10.1109/TIP.2019.2938670},
ISSN={1941-0042},
month={},}

@Article{cryptography4020018,
abbr={MDPI},
selected={false},
bibtex_show={true},
AUTHOR = {Abu Taha, Mohammed and Hamidouche, Wassim and Sidaty, Naty and Viitanen, Marko and Vanne, Jarno and El Assad, Safwan and Deforges, Olivier},
TITLE = {Privacy Protection in Real Time HEVC Standard Using Chaotic System},
JOURNAL = {Cryptography},
VOLUME = {4},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {18},
URL = {https://www.mdpi.com/2410-387X/4/2/18},
ISSN = {2410-387X},
ABSTRACT = {Video protection and access control have gathered steam over recent years. However, the most common methods encrypt the whole video bit stream as unique data without taking into account the structure of the compressed video. These full encryption solutions are time and power consuming and, thus, are not aligned with the real-time applications. In this paper, we propose a Selective Encryption (SE) solution for Region of Interest (ROI) security based on the tile concept in High Efficiency Video Coding (HEVC) standards and selective encryption of all sensitive parts in videos. The SE solution depends on a chaos-based stream cipher that encrypts a set of HEVC syntax elements normatively, that is, the bit stream can be decoded with a standard HEVC decoder, and a secret key is only required for ROI decryption. The proposed ROI encryption solution relies on the independent tile concept in HEVC that splits the video frame into independent rectangular areas. Tiles are used to pull out the ROI from the background and only the tiles figuring the ROI are encrypted. In inter coding, the independence of tiles is guaranteed by limiting the motion vectors of non-ROI to use only the unencrypted tiles in the reference frames. Experimental results have shown that the encryption solution performs secure video encryption in a real time context, with a diminutive bit rate and complexity overheads.},
DOI = {10.3390/cryptography4020018}
}

@Article{Sidaty2020,
abbr={Springer},
selected={false},
bibtex_show={true},
AUTHOR = {Sidaty, Naty and Heulot, Julien and Hamidouche, Wassim and Pelcat, Maxime and Menard, Daniel},
TITLE = {Software HEVC video decoder: towards an energy saving for mobile applications},
JOURNAL = {Multimedia Tools and Applications},
VOLUME = {79},
YEAR = {2020},
NUMBER = {37},
pages={26861–26884},
URL = {https://doi.org/10.1007/s11042-020-09025-y},
ISSN = {1573-7721},
ABSTRACT = {With the explosive growth of mobile video consumption over the Internet, delivering video at high quality while controlling the energy consumption of embedded decoding devices is becoming a primary concern. In this context, this paper demonstrates that tailored energy savings strategies, coupled with a video quality assessment protocol, have the potential to reduce the energy consumption of video decoding. We investigate on a real-time, optimized HEVC decoder, the relationship between the perceived mobile video quality and the energy consumption of the decoder. In addition to low level optimizations for ARM Neon platforms, two energy reduction methods named Approximate and Skipping have been investigated. Results show that energy savings of up to 20% can be achieved by using these methods with the same subjective perceived video quality. These subjective results confirm objective measurements, using PSNR and SSIM metrics, that depict a limited video quality degradation.},
DOI = {10.1007/s11042-020-09025-y}
}


@INPROCEEDINGS{9301766,
abbr={VCIP},
selected={false},
bibtex_show={true},
author={Chao, Fang-Yi and Ozcinar, Cagri and Zhang, Lu and Hamidouche, Wassim and Deforges, Olivier and Smolic, Aljosa},
booktitle={2020 IEEE International Conference on Visual Communications and Image Processing (VCIP)}, title={Towards Audio-Visual Saliency Prediction for Omnidirectional Video with Spatial Audio},
year={2020},
volume={1},
number={1},
pages={355--358},
abstract={Omnidirectional videos (ODVs) with spatial audio enable viewers to perceive 360° directions of audio and visual signals during the consumption of ODVs with head-mounted displays (HMDs). By predicting salient audio-visual regions, ODV systems can be optimized to provide an immersive sensation of audio-visual stimuli with high-quality. Despite the intense recent effort for ODV saliency prediction, the current literature still does not consider the impact of auditory information in ODVs. In this work, we propose an audio-visual saliency (AVS360) model that incorporates 360° spatial-temporal visual representation and spatial auditory information in ODVs. The proposed AVS360 model is composed of two 3D residual networks (ResNets) to encode visual and audio cues. The first one is embedded with a spherical representation technique to extract 360° visual features, and the second one extracts the features of audio using the log mel-spectrogram. We emphasize sound source locations by integrating audio energy map (AEM) generated from spatial audio description (i.e., ambisonics) and equator viewing behavior with equator center bias (ECB). The audio and visual features are combined and fused with AEM and ECB via attention mechanism. Our experimental results show that the AVS360 model has significant superiority over five state-of-the-art saliency models. To the best of our knowledge, it is the first w ork that develops the audio-visual saliency model in ODVs. The code will be publicly available to foster future research on audio-visual saliency in ODVs.},
keywords={Visualization;Feature extraction;Two dimensional displays;Three-dimensional displays;Solid modeling;Predictive models;MONOS devices;Audio-visual saliency;spatial sound;ambisonics;omnidirectional video (ODV);virtual reality (VR)},
doi={10.1109/VCIP49819.2020.9301766},
ISSN={2642-9357},
month={Dec},}


@INPROCEEDINGS{9301884,
abbr={VCIP},
selected={false},
bibtex_show={true},
author={Nasiri, Fatemeh and Hamidouche, Wassim and Morin, Luce and Dhollande, Nicolas and Cocherel, Gildas},
booktitle={2020 IEEE International Conference on Visual Communications and Image Processing (VCIP)}, title={Prediction-Aware Quality Enhancement of VVC Using CNN},
year={2020},
volume={},
number={},
pages={310-313},
abstract={The upcoming video coding standard, Versatile Video Coding (VVC), has shown great improvement compared to its predecessor, High Efficiency Video Coding (HEVC), in terms of bitrate saving. Despite its substantial performance, compressed videos might still suffer from quality degradation at low bitrates due to coding artifacts such as blockiness, blurriness and ringing. In this work, we exploit Convolutional Neural Networks (CNN) to enhance quality of VVC coded frames after decoding in order to reduce low bitrate artifacts. The main contribution of this work is the use of coding information from the compressed bitstream. More precisely, the prediction information of intra frames is used for training the network in addition to the reconstruction information. The proposed method is applied on both luminance and chrominance components of intra coded frames of VVC. Experiments on VVC Test Model (VTM) show that, both in low and high bitrates, the use of coding information can improve the BD-rate performance by about 1% and 6% for luma and chroma components, respectively.},
keywords={Encoding;Training;Convolutional codes;Image reconstruction;Distortion;Streaming media;Decoding;CNN;Intra VVC;quality enhancement},
doi={10.1109/VCIP49819.2020.9301884},
ISSN={2642-9357},
month={Dec},}


@INPROCEEDINGS{9287056,
abbr={MMSP},
selected={false},
bibtex_show={true},
author={Kherchouche, Anouar and Fezza, Sid Ahmed and Hamidouche, Wassim and Déforges, Olivier},
booktitle={2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)}, title={Natural Scene Statistics for Detecting Adversarial Examples in Deep Neural Networks},
year={2020},
volume={},
number={},
pages={1-6},
abstract={The deep neural networks (DNNs) have been adopted in a wide spectrum of applications. However, it has been demonstrated that their are vulnerable to adversarial examples (AEs): carefully-crafted perturbations added to a clean input image. These AEs fool the DNNs which classify them incorrectly. Therefore, it is imperative to develop a detection method of AEs allowing the defense of DNNs. In this paper, we propose to characterize the adversarial perturbations through the use of natural scene statistics. We demonstrate that these statistical properties are altered by the presence of adversarial perturbations. Based on this finding, we design a classifier that exploits these scene statistics to determine if an input is adversarial or not. The proposed method has been evaluated against four prominent adversarial attacks and on three standards datasets. The experimental results have shown that the proposed detection method achieves a high detection accuracy, even against strong attacks, while providing a low false positive rate.},
keywords={Perturbation methods;Neural networks;Tools;Signal processing;Feature extraction;Robustness;Standards;Adversarial examples;deep neural networks;detection;natural scene statistics},
doi={10.1109/MMSP48831.2020.9287056},
ISSN={2473-3628},
month={Sep.},}
@INPROCEEDINGS{9287049,
abbr={MMSP},
selected={false},
bibtex_show={true},
author={Ladune, Théo and Philippe, Pierrick and Hamidouche, Wassim and Zhang, Lu and Déforges, Olivier},
booktitle={2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)}, title={Optical Flow and Mode Selection for Learning-based Video Coding},
year={2020},
volume={},
number={},
pages={1-6},
abstract={This paper introduces a new method for inter-frame coding based on two complementary autoencoders: MOFNet and CodecNet. MOFNet aims at computing and conveying the Optical Flow and a pixel-wise coding Mode selection. The optical flow is used to perform a prediction of the frame to code. The coding mode selection enables competition between direct copy of the prediction or transmission through CodecNet.The proposed coding scheme is assessed under the Challenge on Learned Image Compression 2020 (CLIC20) P-frame coding conditions, where it is shown to perform on par with the state-of-the-art video codec ITU/MPEG HEVC. Moreover, the possibility of copying the prediction enables to learn the optical flow in an end-to-end fashion i.e. without relying on pre-training and/or a dedicated loss term.},
keywords={Video coding;Optical losses;Image coding;Streaming media;Task analysis;Video codecs;Optical flow;Video Coding;Deep Learning;Mode Selection;Optical Flow},
doi={10.1109/MMSP48831.2020.9287049},
ISSN={2473-3628},
month={Sep.},}
@INPROCEEDINGS{9286717,
abbr={IPTA},
selected={false},
bibtex_show={true},
author={Nasiri, Fatemeh and Hamidouche, Wassim and Morin, Luce and Cocherel, Gildas and Dhollande, Nicolas},
booktitle={2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA)}, title={A Study on the Impact of Training Data in CNN-Based Super-Resolution for Low Bitrate End-to-End Video Coding},
year={2020},
volume={},
number={},
pages={1-5},
abstract={In this study, the effectiveness of Super Resolution (SR) methods based on Convolutional Neural Network (CNN) in low bitrate video coding, with a focus on the Versatile Video Coding Standard (VVC), is investigated. Video transmission over networks with limited bandwidth is a common challenge for different applications. One solution is to adopt SR methods where the main principle is to spatially down-sample the input sequence prior to the encoding, then up-sampling the decoded sequence before displaying it. For a fixed target bandwidth, a finer quantization is applied on the low-resolution sequence compared to high-resolution, so that the high quality reconstructed pixels help in retrieving the lost information. However, most CNN-based SR methods are designed for single images and merely focus on the original input signal. Therefore, their trained networks lack understanding of compression artifacts. In this study, we test a hypothesis that training CNN-based SR methods with compressed sequences outperforms training with uncompressed ones. The assumption is that such training allows the SR methods to learn compression artifacts and differentiate them from actual texture information. To this end, state-of-the-art CNN-based SR methods are tested with compressed and uncompressed training set. Experiments show that the use of compressed training data brings, on average, an additional bitrate saving of 6%, in terms of BD-Rate.},
keywords={Training;Video coding;Image coding;Bit rate;Training data;Bandwidth;Convolutional neural networks;Convolutional Neural Networks;Super Resolution;Low Bitrate Video Coding},
doi={10.1109/IPTA50016.2020.9286717},
ISSN={2154-512X},
month={Nov},}
@INPROCEEDINGS{9231841,
abbr={MLSP},
selected={false},
bibtex_show={true},
author={Ladune, Théo and Philippe, Pierrick and Hamidouche, Wassim and Zhang, Lu and Déforges, Olivier},
booktitle={2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)}, title={ModeNet: Mode Selection Network for Learned Video Coding},
year={2020},
volume={},
number={},
pages={1-6},
abstract={In this paper, a mode selection network (ModeNet) is proposed to enhance deep learning-based video compression. Inspired by traditional video coding, ModeNet purpose is to enable competition among several coding modes. The proposed ModeNet learns and conveys a pixel-wise partitioning of the frame, used to assign each pixel to the most suited coding mode. ModeNet is trained alongside the different coding modes to minimize a rate-distortion cost. It is a flexible component which can be generalized to other systems to allow competition between different coding tools. Mod-eNet interest is studied on a P-frame coding task, where it is used to design a method for coding a frame given its prediction. ModeNet-based systems achieve compelling performance when evaluated under the Challenge on Learned Image Compression 2020 (CLIC20) P-frame coding track conditions.},
keywords={Image coding;Training;Channel coding;Rate-distortion;Entropy;Convolutional codes;Video compression;Video Coding;Autoencoder;Mode Selection},
doi={10.1109/MLSP49062.2020.9231841},
ISSN={1551-2541},
month={Sep.},}
@INPROCEEDINGS{9191158,
abbr={ICIP},
selected={false},
bibtex_show={true},
author={Zhang, Yi and Zhang, Lu and Hamidouche, Wassim and Deforges, Olivier},
booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, title={A Fixation-Based 360° Benchmark Dataset For Salient Object Detection},
year={2020},
volume={},
number={},
pages={3458-3462},
abstract={Fixation prediction (FP) in panoramic contents has been widely investigated along with the booming trend of virtual reality (VR) applications. However, another issue within the field of visual saliency, salient object detection (SOD), has been seldom explored in 360° or omnidirectional) images due to the lack of datasets representative of real scenes with pixel-level annotations. Toward this end, we collect 107 equirectangular panoramas with challenging scenes and multiple object classes. Based on the consistency between FP and explicit saliency judgements, we further manually annotate 1,165 salient objects over the collected images with precise masks under the guidance of real human eye fixation maps. Six state-of-the-art SOD models are then benchmarked on the proposed fixation-based 360° image dataset (F-360iSOD), by applying a multiple cubic projection-based fine-tuning method. Experimental results show a limitation of the current methods when used for SOD in panoramic images, which indicates the proposed dataset is challenging. Key issues for 360° SOD is also discussed. The proposed dataset is available at https://github.com/PanoAsh/F-360iSOD.},
keywords={Two dimensional displays;Benchmark testing;Visualization;Object detection;Head;Measurement;Training;VR;salient object detection;360-degree image dataset;equirectangular panorama;benchmark},
doi={10.1109/ICIP40778.2020.9191158},
ISSN={2381-8549},
month={Oct},}
@INPROCEEDINGS{9190928,
abbr={ICIP},
selected={false},
bibtex_show={true},
author={Amestoy, Thomas and Hamidouche, Wassim and Bergeron, Cyril and Menard, Daniel},
booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, title={Quality-Driven Dynamic VVC Frame Partitioning for Efficient Parallel Processing},
year={2020},
volume={},
number={},
pages={3129-3133},
abstract={VVC is the next generation video coding standard, offering coding capability beyond HEVC standard. The high computational complexity of the latest video coding standards requires high-level parallelism techniques, in order to achieve real-time and low latency encoding and decoding. HEVC and VVC include tile grid partitioning that allows to process simultaneously rectangular regions of a frame with independent threads. The tile grid may be further partitioned into a horizontal sub-grid of Rectangular Slices (RSs), increasing the partitioning flexibility. The dynamic Tile and Rectangular Slice (TRS) partitioning solution proposed in this paper benefits from this flexibility. The TRS partitioning is carried-out at the frame level, taking into account both spatial texture of the content and encoding times of previously encoded frames. The proposed solution searches the best partitioning configuration that minimizes the trade-off between multi-thread encoding time and encoding quality loss. Experiments prove that the proposed solution, compared to uniform TRS partitioning, significantly decreases multi-thread encoding time, with slightly better encoding quality.},
keywords={Encoding;Minimization;Parallel processing;Video coding;Standards;Real-time systems;Complexity theory;Video Compression;VVC;High Level Parallelism;Rectangular Slices;VTM},
doi={10.1109/ICIP40778.2020.9190928},
ISSN={2381-8549},
month={Oct},}
@INPROCEEDINGS{9190797,
abbr={ICIP},
selected={false},
bibtex_show={true},
author={Tissier, A. and Hamidouche, W. and Vanne, J. and Galpin, F. and Menard, D.},
booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, title={CNN Oriented Complexity Reduction Of VVC Intra Encoder},
year={2020},
volume={},
number={},
pages={3139-3143},
abstract={The Joint Video Expert Team (JVET) is currently developing the next-generation MPEG/ITU video coding standard called Versatile Video Coding (VVC) and their ultimate goal is to double the coding efficiency over the state-of-the-art HEVC standard.The latest version of the VVC reference encoder, VTM6.1, is able to improve the intra coding efficiency by 24 % over the HEVC reference encoder HM16.20, but at the expense of 27 times the encoding time. The complexity overhead of VVC primarily stems from its novel block partitioning scheme that complements Quad-Tree (QT) split with Multi-Type Tree (MTT) partitioning in order to better fit the local variations of the video signal. This work reduces the block partitioning complexity of VTM6.1 through the use of Convolutional Neural Networks (CNNs). For each 64 × 64 Coding Unit (CU), the CNN is trained to predict a probability vector that speeds up coding block partitioning in encoding. Our solution is shown to decrease the intra encoding complexity of VTM6.1 by 51.5% with a bitrate increase of only 1.45%.},
keywords={Encoding;Complexity theory;Video coding;Rate-distortion;Artificial intelligence;Training;Standards;Versatile Video Coding (VVC);Convolutional Neural Network (CNN);Multi-Type Tree (MTT);Complexity},
doi={10.1109/ICIP40778.2020.9190797},
ISSN={2381-8549},
month={Oct},}
@INPROCEEDINGS{9181229,
abbr={ISCAS},
selected={false},
bibtex_show={true},
author={Djemai, Ibrahim and Fezza, Sid Ahmed and Hamidouche, Wassim and Déforges, Olivier},
booktitle={2020 IEEE International Symposium on Circuits and Systems (ISCAS)}, title={Extending 2D Saliency Models for Head Movement Prediction in 360-Degree Images using CNN-Based Fusion},
year={2020},
volume={},
number={},
pages={1-5},
abstract={Saliency prediction can be of great benefit for 360-degree image/video applications, including compression, streaming, rendering and viewpoint guidance. It is therefore quite natural to adapt the 2D saliency prediction methods for 360-degree images. To achieve this, it is necessary to project the 360-degree image to 2D plane. However, the existing projection techniques introduce different distortions, which provides poor results and makes inefficient the direct application of 2D saliency prediction models to 360-degree content. Consequently, in this paper, we propose a new framework for effectively applying any 2D saliency prediction method to 360-degree images. The proposed framework particularly includes a novel convolutional neural network based fusion approach that provides more accurate saliency prediction while avoiding the introduction of distortions. The proposed framework has been evaluated with five 2D saliency prediction methods, and the experimental results showed the superiority of our approach compared to the use of weighted sum or pixel-wise maximum fusion methods.},
keywords={Two dimensional displays;Head;Distortion;Training;Predictive models;Image coding;Saliency prediction;Head movement;360 image;CNN;Cubemap projection;Fusion},
doi={10.1109/ISCAS45731.2020.9181229},
ISSN={2158-1525},
month={Oct},}
@INPROCEEDINGS{9206959,
abbr={IJCNN},
selected={false},
bibtex_show={true},
author={Kherchouche, Anouar and Fezza, Sid Ahmed and Hamidouche, Wassim and Déforges, Olivier},
booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, title={Detection of Adversarial Examples in Deep Neural Networks with Natural Scene Statistics},
year={2020},
volume={},
number={},
pages={1-7},
abstract={Recent studies have demonstrated that the deep neural networks (DNNs) are vulnerable to carefully-crafted perturbations added to a legitimate input image. Such perturbed images are called adversarial examples (AEs) and can cause DNNs to misclassify. Consequently, it is of paramount importance to develop detection methods of AEs, thus allowing to reject them. In this paper, we propose to characterize the AEs through the use of natural scene statistics (NSS). We demonstrate that these statistical properties are altered by the presence of adversarial perturbations. Based on this finding, we propose three different methods that exploit these scene statistics to determine if an input is adversarial or not. The proposed detection methods have been evaluated against four prominent adversarial attacks and on three standards datasets. The experimental results have shown that the proposed methods achieve a high detection accuracy while providing a low false positive rate.},
keywords={Perturbation methods;Detectors;Neural networks;Support vector machines;Training;Measurement;Feature extraction;Adversarial examples (AEs);deep neural networks (DNNs);detection;natural scene statistics},
doi={10.1109/IJCNN48605.2020.9206959},
ISSN={2161-4407},
month={July},}
@INPROCEEDINGS{9175534,
abbr={MIPR},
selected={false},
bibtex_show={true},
author={Zhang, Yi and Zhang, Lu and Hamidouche, Wassim and Deforges, Olivier},
booktitle={2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)}, title={Key Issues for the Construction of Salient Object Datasets with Large-Scale Annotation},
year={2020},
volume={},
number={},
pages={117-122},
abstract={Salient object detection (SOD) has been extensively studied in recent decades, especially after the boom of convolutional neural networks (CNNs). To direct supervised CNN-based methods to its highest function for SOD, more challenging datasets with reasonable large-scale annotations have been proposed. However, due to a lack of verdict of defining multiple salient objects on images or sequences with complex natural scenes and objects, there are certain degrees of bias in current SOD datasets. Therefore, we survey the methods for salient object annotation and further conclude several key issues for the future SOD dataset construction. To the best of our knowledge, this is the first work that synthesizes all the existing salient object annotation methods.},
keywords={Object detection;Task analysis;Labeling;Semantics;Observers;Conferences;Information processing},
doi={10.1109/MIPR49039.2020.00031},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9102880,
abbr={ICME},
selected={false},
bibtex_show={true},
author={Bakir, Nader and Hamidouche, Wassim and Fezza, Sid Ahmed and Samrouth, Khouloud and Déforges, Olivier},
booktitle={2020 IEEE International Conference on Multimedia and Expo (ICME)}, title={Light Field Image Coding Using Dual Discriminator Generative Adversarial Network And VVC Temporal Scalability},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Light field technology represents a viable path for providing a high-quality VR content. However, such an imaging system generates a high amount of data leading to an urgent need for LF image compression solution. In this paper, we propose an efficient LF image coding scheme based on view synthesis. Instead of transmitting all the LF views, only some of them are coded and transmitted, while the remaining views are dropped. The transmitted views are coded using Versatile Video Coding (VVC) and used as reference views to synthesize the missing views at decoder side. The dropped views are generated using the efficient dual discriminator GAN model. The selection of reference/dropped views is performed using a rate distortion optimization based on the VVC temporal scalability. Experimental results show that the proposed method provides high coding performance and overcomes the state-of-the-art LF image compression solutions.},
keywords={Image coding;Decoding;Generative adversarial networks;Video coding;Training;Generators;Optimization;Light Field;Deep Learning;D2GAN;VVC;Coding Structure;RDO.},
doi={10.1109/ICME46284.2020.9102880},
ISSN={1945-788X},
month={July},}
@INPROCEEDINGS{9105956,
abbr={ICMEW},
selected={false},
bibtex_show={true},
author={Chao, Fang-Yi and Ozcinar, Cagri and Wang, Chen and Zerman, Emin and Zhang, Lu and Hamidouche, Wassim and Deforges, Olivier and Smolic, Aljosa},
booktitle={2020 IEEE International Conference on Multimedia Expo Workshops (ICMEW)}, title={Audio-Visual Perception of Omnidirectional Video for Virtual Reality Applications},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Ambisonics, which constructs a sound distribution over the full viewing sphere, improves immersive experience in omnidirectional video (ODV) by enabling observers to perceive the sound directions. Thus, human attention could be guided by audio and visual stimuli simultaneously. Numerous datasets have been proposed to investigate human visual attention by collecting eye fixations of observers navigating ODV with head-mounted displays (HMD). However, there is no such dataset analyzing the impact of audio information. In this paper, we establish a new audio-visual attention dataset for ODV with mute, mono, and ambisonics. The user behavior including visual attention corresponding to sound source locations, viewing navigation congruence between observers and fixations distributions in these three audio modalities is studied based on video and audio content. From our statistical analysis, we preliminarily found that, compared to only perceiving visual cues, perceiving visual cues with salient object sound (i.e., human voice, siren of ambulance) could draw more visual attention to the objects making sound and guide viewing behaviour when such objects are not in the current field of view. The more in-depth interactive effects between audio and visual cues in mute, mono and ambisonics still require further comprehensive study. The dataset and developed testbed in this initial work will be publicly available with the paper to foster future research on audio-visual attention for ODV.},
keywords={Visualization;MONOS devices;Resists;Streaming media;Prediction algorithms;Training;Observers;Ambisonics;omnidirectional video;virtual reality (VR);visual attention;audio-visual saliency},
doi={10.1109/ICMEW46912.2020.9105956},
ISSN={},
month={July},}
@INPROCEEDINGS{9105951,
abbr={ICMEW},
selected={false},
bibtex_show={true},
author={Chachou, Taieb and Fezza, Sid Ahmed and Belalem, Ghalem and Hamidouche, Wassim},
booktitle={2020 IEEE International Conference on Multimedia Expo Workshops (ICMEW)}, title={Effect of Video Transcoding Parameters on Visual Object Tracking for Surveillance Systems},
year={2020},
volume={},
number={},
pages={1-6},
abstract={In any video surveillance system, it is very important to provide effective remote viewing to heterogeneous users with various network conditions and viewing device. To meet this adaptability requirement, the video transcoding process is inevitable, which consists of converting a video from one compressed format to another. However, since the transcoding operation is a lossy process, this can effect the performance of video analysis techniques such as visual object tracking. Consequently, in this paper, we evaluate the impact of video transcoding parameters on the performance of visual object tracking algorithms. To address this, first, a large-scale transcoding surveillance video (TSV) dataset is constructed. Then, we design a framework for assessing the performance of trackers. Finally, we evaluate six state-of-the-art trackers on the TSV dataset and analysis their performance with regard of transcoding parameters. Experimental results show that the video transcoding can negatively affect the performance of visual object tracking.},
keywords={Transcoding;Object tracking;Bit rate;Visualization;Video surveillance;Through-silicon vias;Video surveillance;object tracking;video transcoding;transcoding parameters},
doi={10.1109/ICMEW46912.2020.9105951},
ISSN={},
month={July},}
@INPROCEEDINGS{9054716,
abbr={ICASSP},
selected={false},
bibtex_show={true},
author={Bonnineau, Charles and Hamidouche, Wassim and Travers, Jean-François and Déforges, Olivier},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Versatile Video Coding and Super-Resolution for Efficient Delivery of 8k Video with 4k Backward-Compatibility},
year={2020},
volume={},
number={},
pages={2048-2052},
abstract={In this paper, we propose, through an objective study, to compare and evaluate the performance of different coding approaches allowing the delivery of an 8K video signal with 4K backward-compatibility on broadcast networks. Presented approaches include simulcast of 8K and 4K single-layer signals encoded using High-Efficiency Video Coding (HEVC) and Versatile Video Coding (VVC) standards, spatial scalability using SHVC with 4K base layer (BL) and 8K enhancement-layer (EL), and super-resolution applied on 4K VVC signal after decoding to reach 8K resolution. For up-scaling, we selected the deep-learning-based super-resolution method called Super-Resolution with Feedback Network (SRFBN) and the Lanczos interpolation filter. We show that the deep-learning-based approach achieves visual quality gain over simulcast, especially on bit-rates lower than 30Mb/s with average gain of 0.77dB, 0.015, and 7.97 for PSNR, SSIM, and VMAF, respectively and outperforms the Lanczos filter in average by 29% of BD-rate savings.},
keywords={Visualization;Scalability;Superresolution;Tools;Decoding;Standards;High efficiency video coding;8K;HEVC;VVC;SHVC;Super-Resolution},
doi={10.1109/ICASSP40776.2020.9054716},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053997,
abbr={ICASSP},
selected={false},
bibtex_show={true},
author={LADUNE, Théo and PHILIPPE, Pierrick and HAMIDOUCHE, Wassim and ZHANG, Lu and DÉFORGES, Olivier},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Binary Probability Model for Learning Based Image Compression},
year={2020},
volume={},
number={},
pages={2168-2172},
abstract={In this paper, we propose to enhance learned image compression systems with a richer probability model for the latent variables. Previous works model the latents with a Gaussian or a Laplace distribution. Inspired by binary arithmetic coding, we propose to signal the latents with three binary values and one integer, with different probability models.A relaxation method is designed to perform gradient-based training. The richer probability model results in a better entropy coding leading to lower rate. Experiments under the Challenge on Learned Image Compression (CLIC) test conditions demonstrate that this method achieves 18 % rate saving compared to Gaussian or Laplace models.},
keywords={Training;Adaptation models;Image coding;Signal processing;Relaxation methods;Parametric statistics;Speech processing;Image Coding;Autoencoder;Entropy Coding;Convolutional Neural Network},
doi={10.1109/ICASSP40776.2020.9053997},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054281,
abbr={ICASSP},
selected={false},
bibtex_show={true},
author={Farhat, I. and Hamidouche, W. and Grill, A. and Menard, D. and Déforges, O.},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Lightweight Hardware Implementation of VVC Transform Block for ASIC Decoder},
year={2020},
volume={},
number={},
pages={1663-1667},
abstract={Versatile Video Coding (VVC) is the next generation video coding standard expected by the end of 2020. Compared to its predecessor, VVC introduces new coding tools to make compression more efficient at the expense of higher computational complexity. This rises a need to design an efficient and optimised implementation especially for embedded platforms with limited memory and logic resources. One of the newly introduced tools in VVC is the Multiple Transform Selection (MTS). This latter involves three Discrete Cosine Transform (DCT)/Discrete Sine Transform (DST) types with larger and rectangular transform blocks. In this paper, an efficient hardware implementation of all DCT/DST transform types and sizes is proposed. The proposed design uses 32 multipliers in a pipelined architecture which targets an ASIC platform. It consists in a multi-standard architecture that supports the transform block of recent MPEG standards including AVC, HEVC and VVC. The architecture is optimized and removes unnecessary complexities found in other proposed architectures by using regular multipliers instead of multiple constant multipliers. The synthesized results show that the proposed method which sustain a constant throughput of two pixels/cycle and constant latency for all block sizes can reach an operational frequency of 600 Mhz enabling to decode in real-time 4K videos at 48 fps.},
keywords={Video coding;Transforms;Computer architecture;Tools;Throughput;Hardware;Decoding;VVC;Multiple Transform Selection;Hardware implementation;ASIC;cross-standard implementation},
doi={10.1109/ICASSP40776.2020.9054281},
ISSN={2379-190X},
month={May},}


	






