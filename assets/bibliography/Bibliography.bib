@ARTICLE{9465693,
author={Nasiri, Fatemeh and Hamidouche, Wassim and Morin, Luce and Dhollande, Nicolas and Cocherel, Gildas},
journal={IEEE Open Journal of Signal Processing}, title={A CNN-based Prediction-Aware Quality Enhancement Framework for VVC},
year={2021},
volume={},
number={},
pages={1-1},
abstract={This paper presents a framework for Convolutional Neural Network (CNN)-based quality enhancement task, by taking advantage of coding information in the compressed video signal. The motivation is that normative decisions made by the encoder can significantly impact the type and strength of artifacts in the decoded images. In this paper, the main focus has been put on decisions defining the prediction signal in intra and inter frames. This information has been used in the training phase as well as input to help the process of learning artifacts that are specific to each coding type. Furthermore, to retain a low memory requirement for the proposed method, one model is used for all Quantization Parameters (QPs) with a QP-map, which is also shared between luma and chroma components. In addition to the Post Processing (PP) approach, the In-Loop Filtering (ILF) codec integration has also been considered, where the characteristics of the Group of Pictures (GoP) are taken into account to boost the performance. The proposed CNN-based Quality Enhancement (QE) framework has been implemented on top of the Versatile Video Coding (VVC) Test Model (VTM-10). Experiments show that the prediction-aware aspect of the proposed method improves the coding efficiency gain of the default CNN-based QE method by 1.52%, in terms of BD-BR, at the same network complexity compared to the default CNN-based QE filter.},
keywords={Encoding;Training;Video coding;Task analysis;Image reconstruction;Quantization (signal);Image coding;CNN;VVC;Quality Enhancement;In-Loop Filtering;Post-Processing},
doi={10.1109/OJSP.2021.3092598},
ISSN={2644-1322},
month={},}
@ARTICLE{9399291,
author={Bakir, Nader and Hamidouche, Wassim and Fezza, Sid Ahmed and Samrout, Khouloud and Deforges, Olivier},
journal={IEEE Transactions on Multimedia}, title={Light Field Image Coding Using VVC standard and View Synthesis based on Dual Discriminator GAN},
year={2021},
volume={},
number={},
pages={1-1},
abstract={Light field (LF) technology is considered as a promising way for providing a high-quality virtual reality (VR) content. However, such an imaging technology produces a large amount of data requiring efficient LF image compression solutions. In this paper, we propose a LF image coding method based on a view synthesis and view quality enhancement techniques. Instead of transmitting all the LF views, only a sparse set of reference views are encoded and transmitted, while the remaining views are synthesized at the decoder side. The transmitted views are encoded using the versatile video coding (VVC) standard and are used as reference views to synthesize the dropped views. The selection of non-reference dropped views is performed using a rate-distortion optimization based on the VVC temporal scalability. The dropped views are reconstructed using the LF dual discriminator GAN (LF-D2GAN) model. In addition, to ensure that the quality of the views is consistent, at the decoder, a quality enhancement procedure is performed on the reconstructed views allowing smooth navigation across views. Experimental results show that the proposed method provides high coding performance and overcomes the state-of-the-art LF image compression methods by -36.22% in terms of BD-BR and 1.35 dB in BD-PSNR.},
keywords={Image coding;Encoding;Cameras;Decoding;Transforms;Transform coding;Standards;Light Field;View Synthesis;Deep Learning;VVC;Coding Structure;RDO;Quality Enhancement},
doi={10.1109/TMM.2021.3068563},
ISSN={1941-0077},
month={},}
@ARTICLE{9122430,
author={Chao, Fang-Yi and Zhang, Lu and Hamidouche, Wassim and Déforges, Olivier},
journal={IEEE Transactions on Multimedia}, title={A Multi-FoV Viewport-Based Visual Saliency Model Using Adaptive Weighting Losses for 360$^\circ$ Images},
year={2021},
volume={23},
number={},
pages={1811-1826},
abstract={360$^\circ$ media allows observers to explore the scene in all directions. The consequence is that the human visual attention is guided by not only the perceived area in the viewport but also the overall content in 360$^\circ$. In this paper, we propose a method to estimate the 360$^\circ$ saliency map which extracts salient features from the entire 360$^\circ$ image in each viewport in three different Field of Views (FoVs). Our model is first pretrained with a large-scale 2D image dataset to enable the interpretation of semantic contents, then fine-tuned with a relative small 360$^\circ$ image dataset. A novel weighting loss function attached with stretch weighted maps is introduced to adaptively weight the losses of three evaluation metrics and attenuate the impact of stretched regions in equirectangular projection during training process. Experimental results demonstrate that our model achieves better performance with the integration of three FoVs and its diverse viewport images. Results also show that the adaptive weighting losses and stretch weighted maps effectively enhance the evaluation scores compared to the fixed weighting losses solutions. Comparing to other state of the art models, our method surpasses them on three different datasets and ranks the top using 5 performance evaluation metrics on the Salient360! benchmark set. The code is available at https://github.com/FannyChao/MV-SalGAN360.},
keywords={Two dimensional displays;Feature extraction;Adaptation models;Visualization;Measurement;Predictive models;Videos;Human eye fixation;saliency;omnidirectional image;convolutional neural network;deep learning},
doi={10.1109/TMM.2020.3003642},
ISSN={1941-0077},
month={},}
@INPROCEEDINGS{9301884,
author={Nasiri, Fatemeh and Hamidouche, Wassim and Morin, Luce and Dhollande, Nicolas and Cocherel, Gildas},
booktitle={2020 IEEE International Conference on Visual Communications and Image Processing (VCIP)}, title={Prediction-Aware Quality Enhancement of VVC Using CNN},
year={2020},
volume={},
number={},
pages={310-313},
abstract={The upcoming video coding standard, Versatile Video Coding (VVC), has shown great improvement compared to its predecessor, High Efficiency Video Coding (HEVC), in terms of bitrate saving. Despite its substantial performance, compressed videos might still suffer from quality degradation at low bitrates due to coding artifacts such as blockiness, blurriness and ringing. In this work, we exploit Convolutional Neural Networks (CNN) to enhance quality of VVC coded frames after decoding in order to reduce low bitrate artifacts. The main contribution of this work is the use of coding information from the compressed bitstream. More precisely, the prediction information of intra frames is used for training the network in addition to the reconstruction information. The proposed method is applied on both luminance and chrominance components of intra coded frames of VVC. Experiments on VVC Test Model (VTM) show that, both in low and high bitrates, the use of coding information can improve the BD-rate performance by about 1% and 6% for luma and chroma components, respectively.},
keywords={Encoding;Training;Convolutional codes;Image reconstruction;Distortion;Streaming media;Decoding;CNN;Intra VVC;quality enhancement},
doi={10.1109/VCIP49819.2020.9301884},
ISSN={2642-9357},
month={Dec},}
@INPROCEEDINGS{9301766,
author={Chao, Fang-Yi and Ozcinar, Cagri and Zhang, Lu and Hamidouche, Wassim and Deforges, Olivier and Smolic, Aljosa},
booktitle={2020 IEEE International Conference on Visual Communications and Image Processing (VCIP)}, title={Towards Audio-Visual Saliency Prediction for Omnidirectional Video with Spatial Audio},
year={2020},
volume={},
number={},
pages={355-358},
abstract={Omnidirectional videos (ODVs) with spatial audio enable viewers to perceive 360° directions of audio and visual signals during the consumption of ODVs with head-mounted displays (HMDs). By predicting salient audio-visual regions, ODV systems can be optimized to provide an immersive sensation of audio-visual stimuli with high-quality. Despite the intense recent effort for ODV saliency prediction, the current literature still does not consider the impact of auditory information in ODVs. In this work, we propose an audio-visual saliency (AVS360) model that incorporates 360° spatial-temporal visual representation and spatial auditory information in ODVs. The proposed AVS360 model is composed of two 3D residual networks (ResNets) to encode visual and audio cues. The first one is embedded with a spherical representation technique to extract 360° visual features, and the second one extracts the features of audio using the log mel-spectrogram. We emphasize sound source locations by integrating audio energy map (AEM) generated from spatial audio description (i.e., ambisonics) and equator viewing behavior with equator center bias (ECB). The audio and visual features are combined and fused with AEM and ECB via attention mechanism. Our experimental results show that the AVS360 model has significant superiority over five state-of-the-art saliency models. To the best of our knowledge, it is the first w ork that develops the audio-visual saliency model in ODVs. The code will be publicly available to foster future research on audio-visual saliency in ODVs.},
keywords={Visualization;Feature extraction;Two dimensional displays;Three-dimensional displays;Solid modeling;Predictive models;MONOS devices;Audio-visual saliency;spatial sound;ambisonics;omnidirectional video (ODV);virtual reality (VR)},
doi={10.1109/VCIP49819.2020.9301766},
ISSN={2642-9357},
month={Dec},}
@ARTICLE{9305217,
author={Herrou, Glenn and Bonnineau, Charles and Bonnineau, Charles and Hamidouche, Wassim and Dumenil, Patrick and Fournier, Jérôme and Morin, Luce},
journal={IEEE Transactions on Circuits and Systems for Video Technology}, title={Quality-driven Variable Frame-Rate for Green Video Coding in Broadcast Applications},
year={2020},
volume={},
number={},
pages={1-1},
abstract={The Digital Video Broadcasting (DVB) has proposed to introduce the Ultra-High Definition services in three phases: UHD-1 phase 1, UHD-1 phase 2 and UHD-2. The UHD-1 phase 2 specification includes several new features such as High Dynamic Range (HDR) and High Frame-Rate (HFR). It has been shown in several studies that HFR (+100 fps) enhances the perceptual quality and that this quality enhancement is content-dependent. On the other hand, HFR brings several challenges to the transmission chain including codec complexity increase and bit-rate overhead, which may delay or even prevent its deployment in the broadcast echo-system. In this paper, we propose a Variable Frame Rate (VFR) solution to determine the minimum (critical) frame-rate that preserves the perceived video quality of HFR video. The frame-rate determination is modeled as a 3-class classification problem which consists in dynamically and locally selecting one frame-rate among three: 30, 60 and 120 frames per second. Two random forests classifiers are trained with a ground truth carefully built by experts for this purpose. The subjective results conducted on ten HFR video contents, not included in the training set, clearly show the efficiency of the proposed solution enabling to locally determine the lowest possible frame-rate while preserving the quality of the HFR content. Moreover, our VFR solution enables significant bit-rate savings and complexity reductions at both encoder and decoder sides.},
keywords={UHDTV;Encoding;Complexity theory;Cameras;Visualization;Video recording;TV;High Frame-Rate (HFR);variable frame-rate;Ultra-High Definition (UHD);High Efficiency Video Coding (HEVC)},
doi={10.1109/TCSVT.2020.3046881},
ISSN={1558-2205},
month={},}
@INPROCEEDINGS{9287056,
author={Kherchouche, Anouar and Fezza, Sid Ahmed and Hamidouche, Wassim and Déforges, Olivier},
booktitle={2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)}, title={Natural Scene Statistics for Detecting Adversarial Examples in Deep Neural Networks},
year={2020},
volume={},
number={},
pages={1-6},
abstract={The deep neural networks (DNNs) have been adopted in a wide spectrum of applications. However, it has been demonstrated that their are vulnerable to adversarial examples (AEs): carefully-crafted perturbations added to a clean input image. These AEs fool the DNNs which classify them incorrectly. Therefore, it is imperative to develop a detection method of AEs allowing the defense of DNNs. In this paper, we propose to characterize the adversarial perturbations through the use of natural scene statistics. We demonstrate that these statistical properties are altered by the presence of adversarial perturbations. Based on this finding, we design a classifier that exploits these scene statistics to determine if an input is adversarial or not. The proposed method has been evaluated against four prominent adversarial attacks and on three standards datasets. The experimental results have shown that the proposed detection method achieves a high detection accuracy, even against strong attacks, while providing a low false positive rate.},
keywords={Perturbation methods;Neural networks;Tools;Signal processing;Feature extraction;Robustness;Standards;Adversarial examples;deep neural networks;detection;natural scene statistics},
doi={10.1109/MMSP48831.2020.9287056},
ISSN={2473-3628},
month={Sep.},}
@INPROCEEDINGS{9287049,
author={Ladune, Théo and Philippe, Pierrick and Hamidouche, Wassim and Zhang, Lu and Déforges, Olivier},
booktitle={2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)}, title={Optical Flow and Mode Selection for Learning-based Video Coding},
year={2020},
volume={},
number={},
pages={1-6},
abstract={This paper introduces a new method for inter-frame coding based on two complementary autoencoders: MOFNet and CodecNet. MOFNet aims at computing and conveying the Optical Flow and a pixel-wise coding Mode selection. The optical flow is used to perform a prediction of the frame to code. The coding mode selection enables competition between direct copy of the prediction or transmission through CodecNet.The proposed coding scheme is assessed under the Challenge on Learned Image Compression 2020 (CLIC20) P-frame coding conditions, where it is shown to perform on par with the state-of-the-art video codec ITU/MPEG HEVC. Moreover, the possibility of copying the prediction enables to learn the optical flow in an end-to-end fashion i.e. without relying on pre-training and/or a dedicated loss term.},
keywords={Video coding;Optical losses;Image coding;Streaming media;Task analysis;Video codecs;Optical flow;Video Coding;Deep Learning;Mode Selection;Optical Flow},
doi={10.1109/MMSP48831.2020.9287049},
ISSN={2473-3628},
month={Sep.},}
@INPROCEEDINGS{9286717,
author={Nasiri, Fatemeh and Hamidouche, Wassim and Morin, Luce and Cocherel, Gildas and Dhollande, Nicolas},
booktitle={2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA)}, title={A Study on the Impact of Training Data in CNN-Based Super-Resolution for Low Bitrate End-to-End Video Coding},
year={2020},
volume={},
number={},
pages={1-5},
abstract={In this study, the effectiveness of Super Resolution (SR) methods based on Convolutional Neural Network (CNN) in low bitrate video coding, with a focus on the Versatile Video Coding Standard (VVC), is investigated. Video transmission over networks with limited bandwidth is a common challenge for different applications. One solution is to adopt SR methods where the main principle is to spatially down-sample the input sequence prior to the encoding, then up-sampling the decoded sequence before displaying it. For a fixed target bandwidth, a finer quantization is applied on the low-resolution sequence compared to high-resolution, so that the high quality reconstructed pixels help in retrieving the lost information. However, most CNN-based SR methods are designed for single images and merely focus on the original input signal. Therefore, their trained networks lack understanding of compression artifacts. In this study, we test a hypothesis that training CNN-based SR methods with compressed sequences outperforms training with uncompressed ones. The assumption is that such training allows the SR methods to learn compression artifacts and differentiate them from actual texture information. To this end, state-of-the-art CNN-based SR methods are tested with compressed and uncompressed training set. Experiments show that the use of compressed training data brings, on average, an additional bitrate saving of 6%, in terms of BD-Rate.},
keywords={Training;Video coding;Image coding;Bit rate;Training data;Bandwidth;Convolutional neural networks;Convolutional Neural Networks;Super Resolution;Low Bitrate Video Coding},
doi={10.1109/IPTA50016.2020.9286717},
ISSN={2154-512X},
month={Nov},}
@INPROCEEDINGS{9231841,
author={Ladune, Théo and Philippe, Pierrick and Hamidouche, Wassim and Zhang, Lu and Déforges, Olivier},
booktitle={2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)}, title={ModeNet: Mode Selection Network for Learned Video Coding},
year={2020},
volume={},
number={},
pages={1-6},
abstract={In this paper, a mode selection network (ModeNet) is proposed to enhance deep learning-based video compression. Inspired by traditional video coding, ModeNet purpose is to enable competition among several coding modes. The proposed ModeNet learns and conveys a pixel-wise partitioning of the frame, used to assign each pixel to the most suited coding mode. ModeNet is trained alongside the different coding modes to minimize a rate-distortion cost. It is a flexible component which can be generalized to other systems to allow competition between different coding tools. Mod-eNet interest is studied on a P-frame coding task, where it is used to design a method for coding a frame given its prediction. ModeNet-based systems achieve compelling performance when evaluated under the Challenge on Learned Image Compression 2020 (CLIC20) P-frame coding track conditions.},
keywords={Image coding;Training;Channel coding;Rate-distortion;Entropy;Convolutional codes;Video compression;Video Coding;Autoencoder;Mode Selection},
doi={10.1109/MLSP49062.2020.9231841},
ISSN={1551-2541},
month={Sep.},}
@INPROCEEDINGS{9191158,
author={Zhang, Yi and Zhang, Lu and Hamidouche, Wassim and Deforges, Olivier},
booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, title={A Fixation-Based 360° Benchmark Dataset For Salient Object Detection},
year={2020},
volume={},
number={},
pages={3458-3462},
abstract={Fixation prediction (FP) in panoramic contents has been widely investigated along with the booming trend of virtual reality (VR) applications. However, another issue within the field of visual saliency, salient object detection (SOD), has been seldom explored in 360° or omnidirectional) images due to the lack of datasets representative of real scenes with pixel-level annotations. Toward this end, we collect 107 equirectangular panoramas with challenging scenes and multiple object classes. Based on the consistency between FP and explicit saliency judgements, we further manually annotate 1,165 salient objects over the collected images with precise masks under the guidance of real human eye fixation maps. Six state-of-the-art SOD models are then benchmarked on the proposed fixation-based 360° image dataset (F-360iSOD), by applying a multiple cubic projection-based fine-tuning method. Experimental results show a limitation of the current methods when used for SOD in panoramic images, which indicates the proposed dataset is challenging. Key issues for 360° SOD is also discussed. The proposed dataset is available at https://github.com/PanoAsh/F-360iSOD.},
keywords={Two dimensional displays;Benchmark testing;Visualization;Object detection;Head;Measurement;Training;VR;salient object detection;360-degree image dataset;equirectangular panorama;benchmark},
doi={10.1109/ICIP40778.2020.9191158},
ISSN={2381-8549},
month={Oct},}
@INPROCEEDINGS{9190928,
author={Amestoy, Thomas and Hamidouche, Wassim and Bergeron, Cyril and Menard, Daniel},
booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, title={Quality-Driven Dynamic VVC Frame Partitioning for Efficient Parallel Processing},
year={2020},
volume={},
number={},
pages={3129-3133},
abstract={VVC is the next generation video coding standard, offering coding capability beyond HEVC standard. The high computational complexity of the latest video coding standards requires high-level parallelism techniques, in order to achieve real-time and low latency encoding and decoding. HEVC and VVC include tile grid partitioning that allows to process simultaneously rectangular regions of a frame with independent threads. The tile grid may be further partitioned into a horizontal sub-grid of Rectangular Slices (RSs), increasing the partitioning flexibility. The dynamic Tile and Rectangular Slice (TRS) partitioning solution proposed in this paper benefits from this flexibility. The TRS partitioning is carried-out at the frame level, taking into account both spatial texture of the content and encoding times of previously encoded frames. The proposed solution searches the best partitioning configuration that minimizes the trade-off between multi-thread encoding time and encoding quality loss. Experiments prove that the proposed solution, compared to uniform TRS partitioning, significantly decreases multi-thread encoding time, with slightly better encoding quality.},
keywords={Encoding;Minimization;Parallel processing;Video coding;Standards;Real-time systems;Complexity theory;Video Compression;VVC;High Level Parallelism;Rectangular Slices;VTM},
doi={10.1109/ICIP40778.2020.9190928},
ISSN={2381-8549},
month={Oct},}
@INPROCEEDINGS{9190797,
author={Tissier, A. and Hamidouche, W. and Vanne, J. and Galpin, F. and Menard, D.},
booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, title={CNN Oriented Complexity Reduction Of VVC Intra Encoder},
year={2020},
volume={},
number={},
pages={3139-3143},
abstract={The Joint Video Expert Team (JVET) is currently developing the next-generation MPEG/ITU video coding standard called Versatile Video Coding (VVC) and their ultimate goal is to double the coding efficiency over the state-of-the-art HEVC standard.The latest version of the VVC reference encoder, VTM6.1, is able to improve the intra coding efficiency by 24 % over the HEVC reference encoder HM16.20, but at the expense of 27 times the encoding time. The complexity overhead of VVC primarily stems from its novel block partitioning scheme that complements Quad-Tree (QT) split with Multi-Type Tree (MTT) partitioning in order to better fit the local variations of the video signal. This work reduces the block partitioning complexity of VTM6.1 through the use of Convolutional Neural Networks (CNNs). For each 64 × 64 Coding Unit (CU), the CNN is trained to predict a probability vector that speeds up coding block partitioning in encoding. Our solution is shown to decrease the intra encoding complexity of VTM6.1 by 51.5% with a bitrate increase of only 1.45%.},
keywords={Encoding;Complexity theory;Video coding;Rate-distortion;Artificial intelligence;Training;Standards;Versatile Video Coding (VVC);Convolutional Neural Network (CNN);Multi-Type Tree (MTT);Complexity},
doi={10.1109/ICIP40778.2020.9190797},
ISSN={2381-8549},
month={Oct},}
@INPROCEEDINGS{9181229,
author={Djemai, Ibrahim and Fezza, Sid Ahmed and Hamidouche, Wassim and Déforges, Olivier},
booktitle={2020 IEEE International Symposium on Circuits and Systems (ISCAS)}, title={Extending 2D Saliency Models for Head Movement Prediction in 360-Degree Images using CNN-Based Fusion},
year={2020},
volume={},
number={},
pages={1-5},
abstract={Saliency prediction can be of great benefit for 360-degree image/video applications, including compression, streaming, rendering and viewpoint guidance. It is therefore quite natural to adapt the 2D saliency prediction methods for 360-degree images. To achieve this, it is necessary to project the 360-degree image to 2D plane. However, the existing projection techniques introduce different distortions, which provides poor results and makes inefficient the direct application of 2D saliency prediction models to 360-degree content. Consequently, in this paper, we propose a new framework for effectively applying any 2D saliency prediction method to 360-degree images. The proposed framework particularly includes a novel convolutional neural network based fusion approach that provides more accurate saliency prediction while avoiding the introduction of distortions. The proposed framework has been evaluated with five 2D saliency prediction methods, and the experimental results showed the superiority of our approach compared to the use of weighted sum or pixel-wise maximum fusion methods.},
keywords={Two dimensional displays;Head;Distortion;Training;Predictive models;Image coding;Saliency prediction;Head movement;360 image;CNN;Cubemap projection;Fusion},
doi={10.1109/ISCAS45731.2020.9181229},
ISSN={2158-1525},
month={Oct},}
@INPROCEEDINGS{9206959,
author={Kherchouche, Anouar and Fezza, Sid Ahmed and Hamidouche, Wassim and Déforges, Olivier},
booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, title={Detection of Adversarial Examples in Deep Neural Networks with Natural Scene Statistics},
year={2020},
volume={},
number={},
pages={1-7},
abstract={Recent studies have demonstrated that the deep neural networks (DNNs) are vulnerable to carefully-crafted perturbations added to a legitimate input image. Such perturbed images are called adversarial examples (AEs) and can cause DNNs to misclassify. Consequently, it is of paramount importance to develop detection methods of AEs, thus allowing to reject them. In this paper, we propose to characterize the AEs through the use of natural scene statistics (NSS). We demonstrate that these statistical properties are altered by the presence of adversarial perturbations. Based on this finding, we propose three different methods that exploit these scene statistics to determine if an input is adversarial or not. The proposed detection methods have been evaluated against four prominent adversarial attacks and on three standards datasets. The experimental results have shown that the proposed methods achieve a high detection accuracy while providing a low false positive rate.},
keywords={Perturbation methods;Detectors;Neural networks;Support vector machines;Training;Measurement;Feature extraction;Adversarial examples (AEs);deep neural networks (DNNs);detection;natural scene statistics},
doi={10.1109/IJCNN48605.2020.9206959},
ISSN={2161-4407},
month={July},}
@INPROCEEDINGS{9175534,
author={Zhang, Yi and Zhang, Lu and Hamidouche, Wassim and Deforges, Olivier},
booktitle={2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)}, title={Key Issues for the Construction of Salient Object Datasets with Large-Scale Annotation},
year={2020},
volume={},
number={},
pages={117-122},
abstract={Salient object detection (SOD) has been extensively studied in recent decades, especially after the boom of convolutional neural networks (CNNs). To direct supervised CNN-based methods to its highest function for SOD, more challenging datasets with reasonable large-scale annotations have been proposed. However, due to a lack of verdict of defining multiple salient objects on images or sequences with complex natural scenes and objects, there are certain degrees of bias in current SOD datasets. Therefore, we survey the methods for salient object annotation and further conclude several key issues for the future SOD dataset construction. To the best of our knowledge, this is the first work that synthesizes all the existing salient object annotation methods.},
keywords={Object detection;Task analysis;Labeling;Semantics;Observers;Conferences;Information processing},
doi={10.1109/MIPR49039.2020.00031},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9102880,
author={Bakir, Nader and Hamidouche, Wassim and Fezza, Sid Ahmed and Samrouth, Khouloud and Déforges, Olivier},
booktitle={2020 IEEE International Conference on Multimedia and Expo (ICME)}, title={Light Field Image Coding Using Dual Discriminator Generative Adversarial Network And VVC Temporal Scalability},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Light field technology represents a viable path for providing a high-quality VR content. However, such an imaging system generates a high amount of data leading to an urgent need for LF image compression solution. In this paper, we propose an efficient LF image coding scheme based on view synthesis. Instead of transmitting all the LF views, only some of them are coded and transmitted, while the remaining views are dropped. The transmitted views are coded using Versatile Video Coding (VVC) and used as reference views to synthesize the missing views at decoder side. The dropped views are generated using the efficient dual discriminator GAN model. The selection of reference/dropped views is performed using a rate distortion optimization based on the VVC temporal scalability. Experimental results show that the proposed method provides high coding performance and overcomes the state-of-the-art LF image compression solutions.},
keywords={Image coding;Decoding;Generative adversarial networks;Video coding;Training;Generators;Optimization;Light Field;Deep Learning;D2GAN;VVC;Coding Structure;RDO.},
doi={10.1109/ICME46284.2020.9102880},
ISSN={1945-788X},
month={July},}
@INPROCEEDINGS{9105956,
author={Chao, Fang-Yi and Ozcinar, Cagri and Wang, Chen and Zerman, Emin and Zhang, Lu and Hamidouche, Wassim and Deforges, Olivier and Smolic, Aljosa},
booktitle={2020 IEEE International Conference on Multimedia Expo Workshops (ICMEW)}, title={Audio-Visual Perception of Omnidirectional Video for Virtual Reality Applications},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Ambisonics, which constructs a sound distribution over the full viewing sphere, improves immersive experience in omnidirectional video (ODV) by enabling observers to perceive the sound directions. Thus, human attention could be guided by audio and visual stimuli simultaneously. Numerous datasets have been proposed to investigate human visual attention by collecting eye fixations of observers navigating ODV with head-mounted displays (HMD). However, there is no such dataset analyzing the impact of audio information. In this paper, we establish a new audio-visual attention dataset for ODV with mute, mono, and ambisonics. The user behavior including visual attention corresponding to sound source locations, viewing navigation congruence between observers and fixations distributions in these three audio modalities is studied based on video and audio content. From our statistical analysis, we preliminarily found that, compared to only perceiving visual cues, perceiving visual cues with salient object sound (i.e., human voice, siren of ambulance) could draw more visual attention to the objects making sound and guide viewing behaviour when such objects are not in the current field of view. The more in-depth interactive effects between audio and visual cues in mute, mono and ambisonics still require further comprehensive study. The dataset and developed testbed in this initial work will be publicly available with the paper to foster future research on audio-visual attention for ODV.},
keywords={Visualization;MONOS devices;Resists;Streaming media;Prediction algorithms;Training;Observers;Ambisonics;omnidirectional video;virtual reality (VR);visual attention;audio-visual saliency},
doi={10.1109/ICMEW46912.2020.9105956},
ISSN={},
month={July},}
@INPROCEEDINGS{9105951,
author={Chachou, Taieb and Fezza, Sid Ahmed and Belalem, Ghalem and Hamidouche, Wassim},
booktitle={2020 IEEE International Conference on Multimedia Expo Workshops (ICMEW)}, title={Effect of Video Transcoding Parameters on Visual Object Tracking for Surveillance Systems},
year={2020},
volume={},
number={},
pages={1-6},
abstract={In any video surveillance system, it is very important to provide effective remote viewing to heterogeneous users with various network conditions and viewing device. To meet this adaptability requirement, the video transcoding process is inevitable, which consists of converting a video from one compressed format to another. However, since the transcoding operation is a lossy process, this can effect the performance of video analysis techniques such as visual object tracking. Consequently, in this paper, we evaluate the impact of video transcoding parameters on the performance of visual object tracking algorithms. To address this, first, a large-scale transcoding surveillance video (TSV) dataset is constructed. Then, we design a framework for assessing the performance of trackers. Finally, we evaluate six state-of-the-art trackers on the TSV dataset and analysis their performance with regard of transcoding parameters. Experimental results show that the video transcoding can negatively affect the performance of visual object tracking.},
keywords={Transcoding;Object tracking;Bit rate;Visualization;Video surveillance;Through-silicon vias;Video surveillance;object tracking;video transcoding;transcoding parameters},
doi={10.1109/ICMEW46912.2020.9105951},
ISSN={},
month={July},}
@INPROCEEDINGS{9054716,
author={Bonnineau, Charles and Hamidouche, Wassim and Travers, Jean-François and Déforges, Olivier},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Versatile Video Coding and Super-Resolution for Efficient Delivery of 8k Video with 4k Backward-Compatibility},
year={2020},
volume={},
number={},
pages={2048-2052},
abstract={In this paper, we propose, through an objective study, to compare and evaluate the performance of different coding approaches allowing the delivery of an 8K video signal with 4K backward-compatibility on broadcast networks. Presented approaches include simulcast of 8K and 4K single-layer signals encoded using High-Efficiency Video Coding (HEVC) and Versatile Video Coding (VVC) standards, spatial scalability using SHVC with 4K base layer (BL) and 8K enhancement-layer (EL), and super-resolution applied on 4K VVC signal after decoding to reach 8K resolution. For up-scaling, we selected the deep-learning-based super-resolution method called Super-Resolution with Feedback Network (SRFBN) and the Lanczos interpolation filter. We show that the deep-learning-based approach achieves visual quality gain over simulcast, especially on bit-rates lower than 30Mb/s with average gain of 0.77dB, 0.015, and 7.97 for PSNR, SSIM, and VMAF, respectively and outperforms the Lanczos filter in average by 29% of BD-rate savings.},
keywords={Visualization;Scalability;Superresolution;Tools;Decoding;Standards;High efficiency video coding;8K;HEVC;VVC;SHVC;Super-Resolution},
doi={10.1109/ICASSP40776.2020.9054716},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9053997,
author={LADUNE, Théo and PHILIPPE, Pierrick and HAMIDOUCHE, Wassim and ZHANG, Lu and DÉFORGES, Olivier},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Binary Probability Model for Learning Based Image Compression},
year={2020},
volume={},
number={},
pages={2168-2172},
abstract={In this paper, we propose to enhance learned image compression systems with a richer probability model for the latent variables. Previous works model the latents with a Gaussian or a Laplace distribution. Inspired by binary arithmetic coding, we propose to signal the latents with three binary values and one integer, with different probability models.A relaxation method is designed to perform gradient-based training. The richer probability model results in a better entropy coding leading to lower rate. Experiments under the Challenge on Learned Image Compression (CLIC) test conditions demonstrate that this method achieves 18 % rate saving compared to Gaussian or Laplace models.},
keywords={Training;Adaptation models;Image coding;Signal processing;Relaxation methods;Parametric statistics;Speech processing;Image Coding;Autoencoder;Entropy Coding;Convolutional Neural Network},
doi={10.1109/ICASSP40776.2020.9053997},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{9054281,
author={Farhat, I. and Hamidouche, W. and Grill, A. and Menard, D. and Déforges, O.},
booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Lightweight Hardware Implementation of VVC Transform Block for ASIC Decoder},
year={2020},
volume={},
number={},
pages={1663-1667},
abstract={Versatile Video Coding (VVC) is the next generation video coding standard expected by the end of 2020. Compared to its predecessor, VVC introduces new coding tools to make compression more efficient at the expense of higher computational complexity. This rises a need to design an efficient and optimised implementation especially for embedded platforms with limited memory and logic resources. One of the newly introduced tools in VVC is the Multiple Transform Selection (MTS). This latter involves three Discrete Cosine Transform (DCT)/Discrete Sine Transform (DST) types with larger and rectangular transform blocks. In this paper, an efficient hardware implementation of all DCT/DST transform types and sizes is proposed. The proposed design uses 32 multipliers in a pipelined architecture which targets an ASIC platform. It consists in a multi-standard architecture that supports the transform block of recent MPEG standards including AVC, HEVC and VVC. The architecture is optimized and removes unnecessary complexities found in other proposed architectures by using regular multipliers instead of multiple constant multipliers. The synthesized results show that the proposed method which sustain a constant throughput of two pixels/cycle and constant latency for all block sizes can reach an operational frequency of 600 Mhz enabling to decode in real-time 4K videos at 48 fps.},
keywords={Video coding;Transforms;Computer architecture;Tools;Throughput;Hardware;Decoding;VVC;Multiple Transform Selection;Hardware implementation;ASIC;cross-standard implementation},
doi={10.1109/ICASSP40776.2020.9054281},
ISSN={2379-190X},
month={May},}
@ARTICLE{8907817,
author={Kammoun, Ahmed and Hamidouche, Wassim and Philippe, Pierrick and Déforges, Olivier and Belghith, Fatma and Masmoudi, Nouri and Nezan, Jean-François},
journal={IEEE Transactions on Circuits and Systems for Video Technology}, title={Forward-Inverse 2D Hardware Implementation of Approximate Transform Core for the VVC Standard},
year={2020},
volume={30},
number={11},
pages={4340-4354},
abstract={The future video coding standard named Versatile Video Coding (VVC) is expected by the end of 2020. VVC will enable better coding efficiency than the current High Efficiency Video Coding (HEVC) standard. This coding gain is brought by several coding tools. The Multiple Transform Selection (MTS) is one of the key coding tools that have been introduced in VVC. The MTS concept relies on three transform types including Discrete Cosine Transform (DCT)-II, Discrete Sine Transform (DST)-VII and DCT-VIII. Unlike the DCT-II that has fast computing algorithms, the DST-VII and DCT-VIII rely on more complex matrix multiplication. In this paper an approximation approach is proposed to reduce the computational cost of the DST-VII and DCT-VIII. The approximation consists in applying adjustment stages, based on sparse block-band matrices, to a variant of DCT-II family mainly DCT-II and its inverse. Genetic algorithm is used to derive the optimal coefficients of the adjustment matrices. Moreover, an efficient hardware implementation of the forward and inverse approximate transform module is proposed. The architecture design includes a pipelined and reconfigurable forward-inverse DCT-II core transform as it is the main core for DST-VII and DCT-VIII computations. The proposed 32-point 1D architecture including low cost adjustment stages allows the processing of a video in 2K and 4K resolutions at 1095 and 273 frames per second, respectively. A unified 2D implementation of forward-inverse DCT-II, approximate DST-VII and DCT-VIII is also presented. The synthesis results show that the design is able to sustain a video in 2K and 4K resolutions at 386 and 96 frames per second, respectively, while using only 12% of Alms, 22% of registers and 30% of DSP blocks of the Arria10 SoC platform.},
keywords={Transforms;Hardware;Encoding;Two dimensional displays;Video coding;Standards;Computer architecture;Versatile video coding;hardware implementation;approximation;DCT-II;adjustment stages;FPGA},
doi={10.1109/TCSVT.2019.2954749},
ISSN={1558-2205},
month={Nov},}
@ARTICLE{8854328,
author={Biatek, Thibaud and Hamidouche, Wassim and Cabarat, Pierre-Loup and Travers, Jean-François and Déforges, Olivier},
journal={IEEE Transactions on Broadcasting}, title={Scalable Video Coding for Backward-Compatible 360° Video Delivery Over Broadcast Networks},
year={2020},
volume={66},
number={2},
pages={322-332},
abstract={Recently, the coding and transmission of immersive 360° video has been intensely studied. The technologies provided by standards developing organizations mainly address requirements coming from over-the-top services. The terrestrial broadcast remains in many countries the mainstream medium to deliver high quality contents to a wide audience. To enable seamless introduction of immersive 360° video services over terrestrial broadcast, the deployed technologies shall fulfill requirements such as backward compatibility to legacy receivers and high bandwidth efficiency. While bandwidth efficiency is addressed by existing techniques, none of them enables legacy video services decoding. In this paper, a novel scalable coding scheme is proposed to enable immersive 360° video services introduction over broadcast networks. The experiments show that the proposed scalable coding scheme provides substantial coding gains of 14.99% compared to simulcast coding and introduces a limited coding overhead of 5.15% compared to 360° single-layer coding. A real-time decoding implementation is proposed, highlighting the relevance of the proposed design. Eventually, an end-to-end demonstrator illustrates how the proposed solution could be integrated in a real terrestrial broadcast environment.},
keywords={Encoding;Streaming media;Decoding;Two dimensional displays;Bandwidth;Video coding;Transform coding;Video coding;scalability;HEVC;SHVC;UHD;4K;360??;broadcast;broadband},
doi={10.1109/TBC.2019.2941073},
ISSN={1557-9611},
month={June},}
@ARTICLE{8826595,
author={Amestoy, Thomas and Mercat, Alexandre and Hamidouche, Wassim and Menard, Daniel and Bergeron, Cyril},
journal={IEEE Transactions on Image Processing}, title={Tunable VVC Frame Partitioning Based on Lightweight Machine Learning},
year={2020},
volume={29},
number={},
pages={1313-1328},
abstract={Block partition structure is a critical module in video coding scheme to achieve significant gap of compression performance. Under the exploration of the future video coding standard, named Versatile Video Coding (VVC), a new Quad Tree Binary Tree (QTBT) block partition structure has been introduced. In addition to the QT block partitioning defined in High Efficiency Video Coding (HEVC) standard, new horizontal and vertical BT partitions are enabled, which drastically increases the encoding time compared to HEVC. In this paper, we propose a lightweight and tunable QTBT partitioning scheme based on a Machine Learning (ML) approach. The proposed solution uses Random Forest classifiers to determine for each coding block the most probable partition modes. To minimize the encoding loss induced by misclassification, risk intervals for classifier decisions are introduced in the proposed solution. By varying the size of risk intervals, tunable trade-off between encoding complexity reduction and coding loss is achieved. The proposed solution implemented in the JEM-7.0 software offers encoding complexity reductions ranging from 30% to 70% in average for only 0.7% to 3.0% Bjøntegaard Delta Rate (BD-BR) increase in Random Access (RA) coding configuration, with very slight overhead induced by Random Forest. The proposed solution based on Random Forest classifiers is also efficient to reduce the complexity of the Multi-Type Tree (MTT) partitioning scheme under the VTM-5.0 software, with complexity reductions ranging from 25% to 61% in average for only 0.4% to 2.2% BD-BR increase.},
keywords={Complexity theory;Image coding;Encoding;Copper;High efficiency video coding;Radio frequency;Video compression;VVC;JEM;VTM;QTBT;complexity reduction;machine learning;random forest},
doi={10.1109/TIP.2019.2938670},
ISSN={1941-0042},
month={},}
@INPROCEEDINGS{8976000,
author={Hobloss, Nour and Purica, Andrei and Fiandrotti, Attilio and Cagnazzo, Marco and Cozot, Remi and Hamidouche, Wassim},
booktitle={2019 International Conference on 3D Immersion (IC3D)}, title={A Hybrid Approach to Wide Baseline View Synthesis with Convolutional Neural Networks},
year={2019},
volume={},
number={},
pages={1-7},
abstract={Convolutional Neural Networks (CNN) have been recently employed for implementing complete end-to-end view synthesis architectures, from reference view warping to target view blending while dealing with occlusions as well. However, the convolutional sizes filters must increase with the distance between reference views, making all-convolutional approaches prohibitively complex for wide baseline setups. In this work we propose a hybrid approach to view synthesis where we first warp the reference views resolving the occlusions, and then we train a simpler convolutional architecture for blending the preprocessed views. By warping the reference views, we reduce the equivalent distance between reference views, allowing the use of smaller convolutional filters and thus lower network complexity. We experimentally show that our method performs favorably against both traditional and convolutional synthesis methods while retaining lower complexity with respect to the latter.},
keywords={Index Terms: View synthesis;Blending;DIBR;CNN;super resolution;holes filling;ghosting},
doi={10.1109/IC3D48390.2019.8976000},
ISSN={2379-1780},
month={Dec},}
@INPROCEEDINGS{8954535,
author={Hamidouche, W. and Philippe, P. and Mohamed, C.-E. and Kammoun, A. and Menard, D. and Déforges, O.},
booktitle={2019 Picture Coding Symposium (PCS)}, title={Hardware-friendly DST-VII/DCT-VIII approximations for the Versatile Video Coding Standard},
year={2019},
volume={},
number={},
pages={1-5},
abstract={Versatile Video Coding (VVC) is the next generation video coding standard expected by the end of 2020. The new concept of Multiple-Transform Selection (MTS) has been introduced in VVC. MTS enables the VVC encoder to select the transform that minimizes the rate-distortion cost among a set of pre-defined trigonometric transforms including the well known Discrete Cosine Transform (DCT)-II, DCT-VIII and Discrete Sine Transform (DST)-VII. Unlike the DCT-II that has fast computing algorithms, the DST-VII and DCT-VIII rely on more complex matrix multiplication.This paper tackles the problem of DST-VII and DCT-VIII approximations based on the DCT-II and an adjustment stage. This latter consists in a multiplication by a band-matrix with low number of non-zero coefficients per row. The approximation problem is first modeled as a constrained integer optimization problem minimizing both error and orthogonality. The genetic algorithm is then used to solve the optimization problem and find the adjustment band-matrix that minimizes a trade-off between error and orthogonality. The proposed solution enables to preserve the coding gain achieved by the MTS and considerably reduces the complexity in terms of required number of multiplications by coefficient. Moreover, the proposed approach is hardwarefriendly and will provide a lightweight shared hardware module for DST-II, DST-VII and DCT-VIII transforms.},
keywords={Multiple-Transform Selection;DCT;DST;Approximation;VVC},
doi={10.1109/PCS48520.2019.8954535},
ISSN={2472-7822},
month={Nov},}
@INPROCEEDINGS{8954562,
author={Sidaty, Naty and Hamidouche, Wassim and Déforges, Olivier and Philippe, Pierrick and Fournier, Jérôme},
booktitle={2019 Picture Coding Symposium (PCS)}, title={Compression Performance of the Versatile Video Coding: HD and UHD Visual Quality Monitoring},
year={2019},
volume={},
number={},
pages={1-5},
abstract={Video compression and content quality have become one of the most research topic in the recent years. Predominantly, trends obviously signpost that the video usage over the Internet is on the upsurge. Simultaneously, users' requirement for enlarged resolution and higher quality is rising. Consequently, a huge effort has been made for video coding technologies and quality monitoring. In this paper, we present a subjective-based comparison as well as an objective measurement between the newest Versatile Video Coding (VVC) and the well-known High Efficiency Video Coding (HEVC) standards. Several videos of various content are selected as tested sequences. Both High Definition (HD) and Ultra High Definition (UHD) resolutions are used in this experiment. An extensive range of bit-rates from low to high bit-rates were selected. These sequences are encoded using both HEVC reference software (HM-16.2) and the latest reference software of VVC (VTM-5.0). Obtained results have shown that VVC outperforms consistently HEVC, for realistic bit rates and quality levels, in the range of 40% on the subjective scale. For the objective measurements, using PSNR, SSIM and VMAF as quality metrics, the quality enhancement of VVC over HEVC is ranging from 31% to 40%, depending on video content and spatial resolution.},
keywords={Quality enhancement;HEVC;VVC;subjective assessment.},
doi={10.1109/PCS48520.2019.8954562},
ISSN={2472-7822},
month={Nov},}
@INPROCEEDINGS{8976000,
author={Hobloss, Nour and Purica, Andrei and Fiandrotti, Attilio and Cagnazzo, Marco and Cozot, Remi and Hamidouche, Wassim},
booktitle={2019 International Conference on 3D Immersion (IC3D)}, title={A Hybrid Approach to Wide Baseline View Synthesis with Convolutional Neural Networks},
year={2019},
volume={},
number={},
pages={1-7},
abstract={Convolutional Neural Networks (CNN) have been recently employed for implementing complete end-to-end view synthesis architectures, from reference view warping to target view blending while dealing with occlusions as well. However, the convolutional sizes filters must increase with the distance between reference views, making all-convolutional approaches prohibitively complex for wide baseline setups. In this work we propose a hybrid approach to view synthesis where we first warp the reference views resolving the occlusions, and then we train a simpler convolutional architecture for blending the preprocessed views. By warping the reference views, we reduce the equivalent distance between reference views, allowing the use of smaller convolutional filters and thus lower network complexity. We experimentally show that our method performs favorably against both traditional and convolutional synthesis methods while retaining lower complexity with respect to the latter.},
keywords={Index Terms: View synthesis;Blending;DIBR;CNN;super resolution;holes filling;ghosting},
doi={10.1109/IC3D48390.2019.8976000},
ISSN={2379-1780},
month={Dec},}
@INPROCEEDINGS{8918771,
author={Aouayeb, Mouath and Hamidouche, Wassim and Kpalma, Kidiyo and Benazza-Benyahia, Amel},
booktitle={2019 IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP)}, title={A Spatiotemporal Deep Learning Solution For Automatic Micro-Expressions Recognition From Local Facial Regions},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Humans always try to hide their Macro-Expressions (MaE) to conceal their real emotion, and it is hard to distinguish between true and false emotions even with artificial intelligence. Micro-Expressions (MiEs), on the contrary, are spontaneous and fast, undetectable with the naked eye and thus always inform us of true feelings. Therefore, there is plenty of studies to generate an automatic system of detecting and analyzing these MiEs.In this paper we propose a new solution that relies on a combination of Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM) applied on particular regions of the face to extract relevant spatial and temporal features, respectively, for MiEs recognition. The proposed solution achieves high recognition accuracy of 90% precision on a different databases including SMIC, CASME II and SAMM. Moreover, under the conditions of Micro-Expression Grand Challenge (MEGC) 2019, our approach performs better than the state of the art solutions including the ones proposed in the challenge.},
keywords={Feature extraction;Face;Machine learning;Convolution;Support vector machines;Gold;Databases;Micro-Expression;CNN;LSTM;Regions of Interest},
doi={10.1109/MLSP.2019.8918771},
ISSN={1551-2541},
month={Oct},}
@INPROCEEDINGS{8901754,
author={Tissier, A. and Mercat, A. and Amestoy, T. and Hamidouche, W. and Vanne, J. and Menard, D.},
booktitle={2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)}, title={Complexity Reduction Opportunities in the Future VVC Intra Encoder},
year={2019},
volume={},
number={},
pages={1-6},
abstract={The Joint Video Expert Team (JVET) is developing the next-generation video coding standard called Versatile Video Coding (VVC) and their ultimate goal is to double the coding efficiency over the current state-of-the-art standard HEVC without letting complexity get out of hand. This work addresses the complexity of the VVC reference encoder called VVC Test Model (VTM) under All Intra coding configuration. The VTM3.0 is able to improve intra coding efficiency by 21% over the latest HEVC reference encoder HM16.19. This coding gain primarily stems from three new coding tools. First, the HEVC Quad-Tree (QT) structure extension with Multi-Type Tree (MTT) partitioning. Second, the duplication of intra prediction modes from 35 to 67. And third, the Multiple Transform Selection (MTS) scheme with two new discrete cosine/sine transforms (DCT-VIII and DST-VII). However, these new tools also play an integral part in making VTM intra encoding around 20 times as complex as that of HM. The purpose of this work is to analyze these tools individually and specify theoretical upper limits for their complexity reduction. According to our evaluations, the complexity reduction opportunity of block partitioning is up to 97%, i.e., the encoding complexity would drop down to 3% for the same coding efficiency if the optimal block partitioning could be directly predicted. The respective percentages for intra mode reduction and MTS optimization are 65% and 55%. We believe these results motivate VVC codec designers to develop techniques that are able to take most out of these opportunities.},
keywords={Versatile Video Coding (VVC);Complexity reduction;CTU partitioning;Multi-Type Tree (MTT);Intra mode prediction;Multiple Transform Selection (MTS)},
doi={10.1109/MMSP.2019.8901754},
ISSN={2473-3628},
month={Sep.},}
@INPROCEEDINGS{8902594,
author={Kammoun, Ahmed and Hamidouche, Wassim and Philipp, Pierrick and Belghith, Fatma and Massmoudi, Nouri and Nezan, Jean-François},
booktitle={2019 27th European Signal Processing Conference (EUSIPCO)}, title={Hardware Acceleration of Approximate Transform Module for the Versatile Video Coding Standard},
year={2019},
volume={},
number={},
pages={1-5},
abstract={Versatile Video Coding (VVC) is the next generation video coding standard expected by the end of 2020. VVC introduces several new coding tools that enable better coding performance compared to the High Efficiency Video Coding (HEVC) standard. The Multiple Transform Selection (MTS) concept, as introduced in VVC, relies on three trigonometrical transforms, and at the encoder side, selects the couple of horizontal and vertical transforms that maximises the RateDistortion cost. However, the new Discrete Sine Transform (DST)VII and Discrete Cosine Transform (DCT)-VIII do not have fast computing algorithms and rely on matrix multiplication, which requires high hardware resources especially for large block sizes.This paper tackles the hardware implementation of an approximation of MTS module. This approximation consists in applying adjustment stages, based on sparse block-band matrices, to a variants of DCT-II family mainly DCT-II and its inverse. Therefore, an efficient 2D hardware implementation of the forward and inverse approximate transform module is proposed. The architecture design includes a pipelined and reconfigurable forward-inverse DCT-II core transform. A unified 2D implementation of 16 and 32-point forward-inverse DCTII, approximate DST-VII and DCT-VIII is also presented. The synthesis results show that the design is able to sustain 2K and 4K videos at 377 and 94 frames per second, respectively, while using only 18% of Alms, 40% of registers and 34% of Digital Signal Processing (DSP) blocks of the ArrialO SoC platform.},
keywords={Discrete cosine transforms;Two dimensional displays;Computer architecture;Hardware;Encoding;Sparse matrices;Versatile Video Coding;Hardware implementation;Approximation;DCT-II;DST-VII and DCT-VIII},
doi={10.23919/EUSIPCO.2019.8902594},
ISSN={2076-1465},
month={Sep.},}
@INPROCEEDINGS{8902614,
author={Bakir, Nader and Fezza, Sid Ahmed and Hamidouche, Wassim and Samrouth, Khouloud and Déforges, Olivier},
booktitle={2019 27th European Signal Processing Conference (EUSIPCO)}, title={Subjective Evaluation of Light Field Image Compression Methods based on View Synthesis},
year={2019},
volume={},
number={},
pages={1-5},
abstract={Light field (LF) images provide rich visual information enabling amazing applications, from post-capture image processing to immersive applications. However, this rich information requires significant storage and bandwidth capabilities thus urgently raises the question of their compression. Many studies have investigated the compression of LF images using both spatial and angular redundancies existing in the LF images. Recently, interesting LF compression approaches based on view synthesis technique have been proposed. In these approaches, only sparse samples of LF views are encoded and transmitted, while the other views are synthesized at decoder side. Different techniques have been proposed to synthesize the dropped views. In this paper, we describe subjective quality evaluation of two recent compression methods based on view synthesis and comparing them to two pseudo-video sequence based coding approaches. Results show that view synthesis based approaches provide higher visual quality than the naive LF coding approaches. In addition, the database as well as subjective scores are publicly available to help designing new objective metrics or can be used as a benchmark for future development of LF coding methods.},
keywords={Image coding;Encoding;Visualization;Decoding;Standards;Image color analysis;Europe;Light field;Image compression;View synthesis;Subjective evaluation;CNN;Linear approximation},
doi={10.23919/EUSIPCO.2019.8902614},
ISSN={2076-1465},
month={Sep.},}
@ARTICLE{8890816,
author={Bakhti, Yassine and Fezza, Sid Ahmed and Hamidouche, Wassim and Déforges, Olivier},
journal={IEEE Access}, title={DDSA: A Defense Against Adversarial Attacks Using Deep Denoising Sparse Autoencoder},
year={2019},
volume={7},
number={},
pages={160397-160407},
abstract={Given their outstanding performance, the Deep Neural Networks (DNNs) models have been deployed in many real-world applications. However, recent studies have demonstrated that they are vulnerable to small carefully crafted perturbations, i.e., adversarial examples, which considerably decrease their performance and can lead to devastating consequences, especially for safety-critical applications, such as autonomous vehicles, healthcare and face recognition. Therefore, it is of paramount importance to offer defense solutions that increase the robustness of DNNs against adversarial attacks. In this paper, we propose a novel defense solution based on a Deep Denoising Sparse Autoencoder (DDSA). The proposed method is performed as a pre-processing step, where the adversarial noise of the input samples is removed before feeding the classifier. The pre-processing defense block can be associated with any classifier, without any change to their architecture or training procedure. In addition, the proposed method is a universal defense, since it does not require any knowledge about the attack, making it usable against any type of attack. The experimental results on MNIST and CIFAR-10 datasets have shown that the proposed DDSA defense provides a high robustness against a set of prominent attacks under white-, gray- and black-box settings, and outperforms state-of-the-art defense methods.},
keywords={Perturbation methods;Noise reduction;Training;Neural networks;Robustness;Licenses;Training data;Deep neural network;security;adversarial attacks;defense;sparse autoencoder;denoising},
doi={10.1109/ACCESS.2019.2951526},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8803588,
author={Herrou, Glenn and Hamidouche, Wassim and Morin, Luce},
booktitle={2019 IEEE International Conference on Image Processing (ICIP)}, title={Low-Complexity Scalable Encoder Based on Local Adaptation of the Spatial Resolution},
year={2019},
volume={},
number={},
pages={3552-3556},
abstract={A two-layer low-complexity scalable encoding scheme based on local Adaptive Spatial Resolution (ASR) is proposed. This scheme relies on a block-level spatial resolution adaptation in the enhancement layer encoder. For each block, the optimal resolution is either obtained via a rate-distortion optimization followed by a decision refinement process or by a prediction via motion compensation exploiting the base layer motion vectors. The proposed architecture has been integrated over of the High Efficiency Video Coding (HEVC) reference software (HM16.12) which is used as a base layer encoder. Compared to SHVC, the scalable extension of HEVC, experimental results show bitrate savings of 0.76 % as well as encoding complexity reductions of 47 % for the whole scalable encoder and 96 % for the enhancement layer encoder.},
keywords={Spatial resolution;Encoding;Computer architecture;Signal resolution;Motion compensation;Rate-distortion;Video compression;spatial scalability;low-complexity;adaptive spatial resolution;SHVC.},
doi={10.1109/ICIP.2019.8803588},
ISSN={2381-8549},
month={Sep.},}
@INPROCEEDINGS{8803403,
author={Filali, Amira and Ricordel, Vincent and Normand, Nicolas and Hamidouche, Wassim},
booktitle={2019 IEEE International Conference on Image Processing (ICIP)}, title={Rate-Distortion Optimized Tree-Structured Point-Lattice Vector Quantization for Compression of 3D Point Clouds Geometry},
year={2019},
volume={},
number={},
pages={1099-1103},
abstract={This paper deals with the current trends of new compression methods for 3-D point cloud contents required to ensure efficient transmission and storage. The representation of 3D point clouds geometry remains a challenging problem, since this signal is unstructured. In this paper, we introduce a new hierarchical geometry representation based on adaptive Tree-Structured Point-Lattice Vector Quantization (TSPLVQ). This representation enables hierarchically structured 3D content that improves the compression performance for static point clouds. The novelty of the proposed scheme lies in adaptive selection of the optimal quantization scheme of the geometric information, that better leverage the intrinsic correlations in point cloud. Based on its adaptive and multiscale structure, two quantization schemes are dedicated to project recursively the 3D point clouds into a series of embedded truncated cubic lattices. At each step of the process, the optimal quantization scheme is selected according to a rate-distortion cost in order to achieve the best trade-off between coding rate and geometry distortion, such that the compression flexibility and performance can be greatly improved. Experimental results show the interest of the proposed multi-scale method for lossy compression of geometry.},
keywords={Three-dimensional displays;Lattices;Distortion;Quantization (signal);Geometry;Rate-distortion;Rate distortion theory;3D point cloud geometry;lattice vector quantization;rate-distortion optimization},
doi={10.1109/ICIP.2019.8803403},
ISSN={2381-8549},
month={Sep.},}
@INPROCEEDINGS{8743213,
author={Fezza, Sid Ahmed and Bakhti, Yassine and Hamidouche, Wassim and Déforges, Olivier},
booktitle={2019 Eleventh International Conference on Quality of Multimedia Experience (QoMEX)}, title={Perceptual Evaluation of Adversarial Attacks for CNN-based Image Classification},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Deep neural networks (DNNs) have recently achieved state-of-the-art performance and provide significant progress in many machine learning tasks, such as image classification, speech processing, natural language processing, etc. However, recent studies have shown that DNNs are vulnerable to adversarial attacks. For instance, in the image classification domain, adding small imperceptible perturbations to the input image is sufficient to fool the DNN and to cause misclassification. The perturbed image, called adversarial example, should be visually as close as possible to the original image. However, all the works proposed in the literature for generating adversarial examples have used the Lp norms (L0, L2 and L∞) as distance metrics to quantify the similarity between the original image and the adversarial example. Nonetheless, the Lp norms do not correlate with human judgment, making them not suitable to reliably assess the perceptual similarity/fidelity of adversarial examples. In this paper, we present a database for visual fidelity assessment of adversarial examples. We describe the creation of the database and evaluate the performance of fifteen state-of-the-art full-reference (FR) image fidelity assessment metrics that could substitute Lp norms. The database as well as subjective scores are publicly available to help designing new metrics for adversarial examples and to facilitate future research works.},
keywords={deep neural network;adversarial attack;adversarial example;subjective evaluation;perturbation},
doi={10.1109/QoMEX.2019.8743213},
ISSN={2472-7814},
month={June},}
@INPROCEEDINGS{8743284,
author={Fezza, Sid Ahmed and Hamidouche, Wassim and Kamraoui, Reda Abdellah and Déforges, Olivier},
booktitle={2019 Eleventh International Conference on Quality of Multimedia Experience (QoMEX)}, title={Visual Security Assessment of Selective Video Encryption},
year={2019},
volume={},
number={},
pages={1-3},
abstract={Given the wide use of videos in various applications and across different devices, this raises the question of their security and confidentiality. In the last decade, many video encryption methods have been proposed in the literature. Accordingly, it becomes necessary to have a reliable assessment tool allowing evaluation of the efficiency of these video encryption methods, especially from the visual security point of view. Usually, the visual security is evaluated through the classical objective signal-based metrics. However, these metrics showed their limits as visual security metric, since they are not designed to deal with the security requirements, such as the determination of content intelligibility. Despite its obvious importance, very few visual security metrics have been proposed for the assessment of video encryption methods. This is mainly due to the lack of ground truth with subjective human scores for video encryption applications. In this paper, we present a new database for visual security assessment of selective video encryption. The database including unencrypted and encrypted video contents generated using different selective encryption schemes, as well as subjective scores, is publicly available to help designing new visual security metrics1.},
keywords={Encryption;Visualization;Measurement;Video sequences;Streaming media;Selective video encryption;subjective test;visual security;HEVC;scrambling.},
doi={10.1109/QoMEX.2019.8743284},
ISSN={2472-7814},
month={June},}
@ARTICLE{8728159,
author={Bichon, Maxime and Le Tanou, Julien and Ropert, Michael and Hamidouche, Wassim and Morin, Luce},
journal={IEEE Transactions on Image Processing}, title={Optimal Adaptive Quantization Based on Temporal Distortion Propagation Model for HEVC},
year={2019},
volume={28},
number={11},
pages={5419-5434},
abstract={Optimal adaptive quantization is one of the key points to optimize the coding efficiency of video encoders. The latest block-based video compression standards, such as high-efficiencyvideo coding (HEVC), extensively use predictive coding techniques that create dependencies between blocks and increase the complexity of optimal block quantizers search. Specifically, the motion compensation is responsible for a dependency network connecting all blocks of the same GOP together. In this paper, this dependency network is estimated by a temporal distortion propagation model and an accurate estimation of Inter and Skip modes probabilities. Optimal quantizers are then designed per block in order to achieve global optimization in terms of rate-distortion efficiency. By implementing the algorithm into the HEVC reference model (HM), we report -16.51% PSNR-based and -26.26% SSIM-based average bitrate savings compared to no adaptive quantization. The proposed algorithm outperforms several related methods from the state-of-the-art. Moreover, along with the demonstration of an optimal quantizer solution, we propose an in-depth analysis of the algorithm behavior. This analysis includes, among others, the relative distribution of rates between frames and the control of quantizers dynamic range.},
keywords={Distortion;Image coding;Encoding;Standards;Quantization (signal);Optimization;Video coding;Local quantization;rate-distortion (R-D) optimization;HEVC;temporal distortion propagation;skip probability},
doi={10.1109/TIP.2019.2919180},
ISSN={1941-0042},
month={Nov},}
@INPROCEEDINGS{8712638,
author={Reuze, Kevin and Hamidouche, Wassim and Philippe, Pierrick and Deforges, Olivier},
booktitle={2019 Data Compression Conference (DCC)}, title={Dynamic Lists for Efficient Coding of Intra Prediction Modes in the Future Video Coding Standard},
year={2019},
volume={},
number={},
pages={601-601},
abstract={The next generation MPEG video coding standard is under development by the Joint Video Coding Experts Team (JVET). This new standard, called Versatile Video Coding (VVC), is expected by the end of 2020 and will offer better coding efficiency than its predecessor High Efficiency Video Coding (HEVC) standard. This coding gain is enabled by new coding tools such as more flexible block partitioning, more accurate Intra/Inter predictions, multiple transforms and adaptive in-loop filtering. In this paper we focus on the coding of the Intra Prediction Modes (IPM) that have been increased from 35 modes in HEVC to 67 modes in VVC. We propose a solution based on genetic algorithms to build an ordered list for the coding of IPM in the Joint Exploration Model (JEM) codec. We first give the theoretical upper bound performance in terms of required bits per IPM to encode the IPM using the available contextual information. The new ordering of the labels associated with more efficient codes is then proposed to efficiently leverage contextual informations available in the encoder and construct the Most Probable Modes (MPM) list. The proposed coding scheme enables to increase the BD-BR performance in average by 0.09% for the same level of complexity compared to the JEM.},
keywords={Encoding;Standards;Next generation networking;Transform coding;High efficiency video coding;Dynamic Lists;Intra Prediction Modes;Future Video Coding;Most Probable Modes;Decision Tree},
doi={10.1109/DCC.2019.00113},
ISSN={2375-0359},
month={March},}
@INPROCEEDINGS{8712614,
author={Bakir, Nader and Hamidouche, Wassim and Déforges, Olivier and Samrouth, Khouloud and Fezza, Sid Ahmed and Khalil, Mohamad},
booktitle={2019 Data Compression Conference (DCC)}, title={RDO-Based Light Field Image Coding Using Convolutional Neural Networks and Linear Approximation},
year={2019},
volume={},
number={},
pages={554-554},
abstract={The increasing penetration of acquisition and display devices for Light Field (LF) content in the consumer market leads to the high proliferation of this new immersive media. This growing interest to LF images thus urgently raises the question of their compression. In this paper, we propose a convolutional neural networks (CNN)-based LF image coding scheme including both Rate Distortion Optimization (RDO) and post-processing steps. First, at the encoder side, the views are rearranged in sparse and dropped set of views. The former are compressed with a standard encoder and transmitted, while the dropped views are either linearly approximated or synthesized by a CNN using the encoded views as input. This choice is made on the basis of the proposed RDO process. At the decoder side, once the dropped views are either linearly approximated or synthesized by a CNN block, a post-processing step is performed to further enhance the quality of the reconstructed views. This post-processing block is based on superpixel to pixel-matching. Experimental results show that the proposed scheme provides views with high visual quality and overcomes the state-of-the-art LF image compression solutions by -30% in terms of BD-BR and 0.62 dB in BD-PSNR.},
keywords={Image coding;Light fields;Linear approximation;Two dimensional displays;Convolutional neural networks;Decoding;Rate-distortion;Light Field;Image Coding;CNN;Linear approximation;RDO},
doi={10.1109/DCC.2019.00066},
ISSN={2375-0359},
month={March},}
@INPROCEEDINGS{8683413,
author={Amestoy, Thomas and Mercat, Alexandre and Hamidouche, Wassim and Bergeron, Cyril and Menard, Daniel},
booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Random Forest Oriented Fast QTBT Frame Partitioning},
year={2019},
volume={},
number={},
pages={1837-1841},
abstract={Block partition structure is a critical module in video coding scheme to achieve significant gap of compression performance. Under the exploration of future video coding standard by the Joint Video Exploration Team (JVET), named Versatile Video Coding (VVC), a new Quad Tree Binary Tree (QTBT) block partition structure has been introduced. In addition to the QT block partitioning defined by High Efficiency Video Coding (HEVC) standard, new horizontal and vertical BT partitions are enabled, which drastically increases the encoding time compared to HEVC. In this paper, we propose a fast QTBT partitioning scheme based on a Machine Learning approach. Complementary to techniques proposed in literature to reduce the complexity of HEVC Quad Tree (QT) partitioning, the propose solution uses Random Forest classifiers to determine for each block which partition modes between QT and BT is more likely to be selected. Using uncertainty zones of classifier decisions, the proposed complexity reduction technique is able to reduce in average by 30% the encoding time of JEM-v7.0 software in Random Access configuration with only 0.57% Bjøntegaard Delta Rate (BD-BR) increase.},
keywords={Training;Encoding;Radio frequency;Vegetation;Complexity theory;Random forests;Video coding;Video Compression;VVC;QTBT;JEM;Complexity Reduction;Machine Learning;Random Forest},
doi={10.1109/ICASSP.2019.8683413},
ISSN={2379-190X},
month={May},}
@ARTICLE{8664002,
author={Parois, Ronan and Hamidouche, Wassim and Cabarat, Pierre-Loup and Raulet, Mickael and Sidaty, Naty and Déforges, Olivier},
journal={IEEE Access}, title={4K Real Time Software Solution of Scalable HEVC for Broadcast Video Application},
year={2019},
volume={7},
number={},
pages={46748-46762},
abstract={Scalable high-efficiency video coding (SHVC) is the scalable extension of the high-efficiency video coding (HEVC) standard. SHVC enables spatial, quality, bit-depth, color gamut, and codec scalability. The architecture of the SHVC encoder is based on multiple instances of the HEVC encoder where each instance encodes one video layer. This architecture offers several advantages of being modular and close to the native HEVC coding block scheme. However, the close-loop SHVC architecture requires the complete decoding of the reference lower layer frames to decode a higher quality layer, which considerably increases the complexity of both encoder and decoder processes. In this paper, we propose an end-to-end 4K real-time SHVC solution, including both software encoder and decoder, for video broadcast applications. The SHVC codec relies on low-level optimizations for specific Intel ×86 platform and parallel processing to speed up the encoding and decoding processes. The proposed encoder enables real-time processing of 4Kp30 video in 2× spatial scalabilities on the 4 × 10 cores Intel Xeon processor (E5-4627V3) running at 2.6 GHz. In addition, the SHVC decoder enables to decode, respectively, the lower quality layer in full HD (1920 × 1080p30) resolution, an advanced RISC machine (ARM) Neon mobile platform, and the enhancement layer in UHD (3840 × 2160p30), on a fitted laptop, with 4 cores Intel i7 processor running at 2.7 GHz. Finally, experimental results have shown that the proposed solution can reach a high rate-distortion performance close to the reference SHVC reference software model (SHM) with a speedup of 37 and 66 in intra and inter coding configurations.},
keywords={Encoding;Decoding;Real-time systems;Software;Streaming media;Scalability;Standards;Scalable video coding;HEVC;SHVC extension;real time video codecs},
doi={10.1109/ACCESS.2019.2904196},
ISSN={2169-3536},
month={},}
@ARTICLE{8239670,
author={Liu, Yi and Sidaty, Naty and Hamidouche, Wassim and Déforges, Olivier and Valenzise, Giuseppe and Zerman, Emin},
journal={IEEE Transactions on Circuits and Systems for Video Technology}, title={An Adaptive Quantizer for High Dynamic Range Content: Application to Video Coding},
year={2019},
volume={29},
number={2},
pages={531-545},
abstract={In this paper, we propose an adaptive perceptual quantization method to convert the representation of high dynamic range (HDR) content from the floating point data type to integer, which is compatible with the current image/video coding and display systems. The proposed method considers the luminance distribution of the HDR content, as well as the detectable contrast threshold of the human visual system, in order to preserve more contrast information than the perceptual quantizer (PQ) in integer representation. Aiming to demonstrate the effectiveness of this quantizer for HDR video compression, we implemented it in a mapping function on the top of the HDR video coding system based on high efficiency video coding standard. Moreover, a comparison function is also introduced to decrease the additional bit-rate of side information, generated by the mapping function. Objective quality measurements and subjective tests have been conducted in order to evaluate the quality of the reconstructed HDR videos. Subjective test results have shown that the proposed method can improve, in a significant manner, the perceived quality of some reconstructed HDR videos. In the objective assessment, the proposed method achieves improvements over PQ in terms of the average bit-rate gain for metrics used in the measurement.},
keywords={Quantization (signal);Encoding;Dynamic range;High efficiency video coding;Transfer functions;Standards;High dynamic range (HDR);perceptual quantization;HDR video coding;HEVC;objective quality metric;subjective test},
doi={10.1109/TCSVT.2017.2786746},
ISSN={1558-2205},
month={Feb},}
@INPROCEEDINGS{8598306,
author={Sidaty, Naty and Cabarat, Pierre-Loup and Hamidouche, Wassim and Menard, Daniel and Deforges, Olivier},
booktitle={2018 IEEE International Workshop on Signal Processing Systems (SiPS)}, title={Performance and Computational Complexity of the Future Video Coding},
year={2018},
volume={},
number={},
pages={31-36},
abstract={The drastic increasing of multimedia applications, such as IPTV, Virtual Reality (VR, 360°) and Light Field videos has led to a high computing complexity in video compression and content quality assessment. In the last five years, HEVC standard has been widely used in the industrial community due to its bit-rate gain compared to its predecessor H.264/AVC. Recently, a new coding tools have been developed under the Joint Exploration Model (JEM) software, with the main goal to provide high bit rate saving compared to the HEVC standard. In this paper we investigate the performance and the associated computational complexity of these emerging video coding tools, from both encoding and decoding sides. Two spatial resolutions (HD & 4K) and several video contents have been used in this study. Results have shown that despite the bit-rate saving, a considerable computational complexity can be noticed. A bit-rate saving up to 37% and quality enhancements up to 30% can be achieved by JEM. However, these emerging tools are time consuming between x5 and x12 times compared to the HM reference software, depending on video sequence and encoding/decoding processes.},
keywords={Tools;Encoding;Bit rate;Software;Complexity theory;Decoding;Video coding;Future Video Coding;HM;JEM;Complexity;Video Quality;Subjective Assessment},
doi={10.1109/SiPS.2018.8598306},
ISSN={2374-7390},
month={Oct},}
@INPROCEEDINGS{8553038,
author={Taha, Mohammed Abu and Sidaty, N. and Hamidouche, W. and Dforges, O. and Vanne, J. and Viitanen, M.},
booktitle={2018 26th European Signal Processing Conference (EUSIPCO)}, title={End-to-End Real-Time ROI-Based Encryption in HEVC Videos},
year={2018},
volume={},
number={},
pages={171-175},
abstract={In this paper, we present an end-to-end real-time encryption of Region of Interest (ROI) in HEVC videos. The proposed ROI encryption makes use of the independent tile concept of HEVC that splits the video frame into separable rectangular areas. Tiles are used to extract the ROI from the background and only the tiles forming the ROI are encrypted. The selective encryption is performed for a set of HEVC syntax elements in a format compliant with the HEVC standard. Thus, the bit-stream can be decoded with a standard HEVC decoder where a secret key is only needed for ROI decryption. In Inter coding, tiles independency is guaranteed by restricting the motion vectors to use only unencrypted tiles in the reference frames. The proposed solution is validated by integrating the encryption into the open-source Kvazaar HEVC encoder and the decryption into the open-source openHEVC decoder, respectively. The results show that this solution performs secure encryption of ROI in real time and with diminutive bitrate and complexity overheads.},
keywords={Encryption;Videos;Encoding;Generators;Decoding;Syntactics;User identity management;High Efficiency Video Coding (HEVC);tiles;Region of Interest (ROI);selective encryption;quality assessments},
doi={10.23919/EUSIPCO.2018.8553038},
ISSN={2076-1465},
month={Sep.},}
@INPROCEEDINGS{8551543,
author={Chao, Fang-Yi and Zhang, Lu and Hamidouche, Wassim and Deforges, Olivier},
booktitle={2018 IEEE International Conference on Multimedia Expo Workshops (ICMEW)}, title={Salgan360: Visual Saliency Prediction On 360 Degree Images With Generative Adversarial Networks},
year={2018},
volume={},
number={},
pages={01-04},
abstract={Understanding visual attention of observers on 360° images gains interest along with the booming trend of Virtual Reality applications. Extending existing saliency prediction methods from traditional 2D images to 360° images is not a direct approach due to the lack of a sufficient large 360° image saliency database. In this paper, we propose to extend the SalGAN, a 2D saliency model based on the generative adversarial network, to SalGAN360 by fine tuning the SalGAN with our new loss function to predict both global and local saliency maps. Our experiments show that the SalGAN360 outperforms the tested state-of-the-art methods.},
keywords={Measurement;Visualization;Observers;Two dimensional displays;Solid modeling;Predictive models;Adaptation models;360 image;omnidirectional image;saliency prediction;deep convolutional neuron network;generative adversarial network (GAN)},
doi={10.1109/ICMEW.2018.8551543},
ISSN={},
month={July},}
@ARTICLE{8489967,
author={Kammoun, Ahmed and Hamidouche, Wassim and Belghith, Fatma and Nezan, Jean-François and Masmoudi, Nouri},
journal={IEEE Transactions on Consumer Electronics}, title={Hardware Design and Implementation of Adaptive Multiple Transforms for the Versatile Video Coding Standard},
year={2018},
volume={64},
number={4},
pages={424-432},
abstract={Versatile video coding is the next generation video coding standard expected by the end of 2020. Several new contributions have been proposed to enhance the coding efficiency beyond the high efficiency video coding standard. One of these tools is the adaptive multiple transform (AMT) as a new approach of the transform core design. The AMT involves five discrete cosine transform/discrete sine transform types with larger and more flexible partitioning block sizes. However, the AMT coding efficiency comes with the cost of higher computational complexity, especially at the encoder side. In this paper, a efficient pipelined hardware implementation of the AMT including the five types of sizes 4 × 4, 8 × 8, 16 × 16 and 32 × 32 is proposed. The architecture design takes advantage of the internal software/hardware resources of the target field-programmable gate array device such as library of parametrized modules core intellectual properties and digital signal processing blocks. The proposed 1-D 32-point AMT design allows to process 4K video at 44 frames/s. A unified 2-D implementation of the 4, 8, 16, and 32-point AMT design is also presented.The implementation takes into account all the asymmetric 2-D block size combinations from 4 to 32. The 2D architecture design is able to sustain 2K video coding at 50 frames/s with an operational frequency up to 147 MHz.},
keywords={Transforms;Two dimensional displays;Video coding;Field programmable gate arrays;Encoding;Future video coding;hardware implementation;FPGA;adaptive multiple transform;pipeline;DSP},
doi={10.1109/TCE.2018.2875528},
ISSN={1558-4127},
month={Nov},}
@INPROCEEDINGS{8462489,
author={Bichon, Maxime and Tanou, Julien Le and Ropert, Michael and Hamidouche, Wassim and Morin, Luce and Zhang, Lu},
booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Low Complexity Joint RDO of Prediction Units Couples for HEVC Intra Coding},
year={2018},
volume={},
number={},
pages={1733-1737},
abstract={HEVC is the latest block-based video compression standard, outperforming H.264/AVC by 50% bitrate savings for the same perceptual quality. An HEVC encoder provides Rate-Distortion optimization coding tools for block-wise compression. Because of complexity limitations, Rate-Distortion Optimization (RDO) is usually performed independently for each block, assuming coding efficiency losses to be negligible. In this paper, we propose an acceleration solution for the Intra coding scheme named Dual-JRDO, which takes advantage of Inter-Block dependencies related to both predictive coding and CABAC. The Dual-JRDO improves Intra coding efficiency at the expense of higher computational complexity. The acceleration of the Dual-JRDO scheme includes adaptive use of the Dual-JRDO model based on source analysis, short-listing and early decisions strategies. The proposed Fast Dual-JRDO reduces the original model complexity by 89.54%, while providing tractable computation for average R-D gains of -0.45% (up to -0.82%) in the HM16.12 reference software model.},
keywords={Encoding;Acceleration;Optimization;Computational modeling;Estimation;Complexity theory;Distortion;Intra Coding;Joint Block Optimization;HEVC;Dual-JRDO},
doi={10.1109/ICASSP.2018.8462489},
ISSN={2379-190X},
month={April},}
@INPROCEEDINGS{8463299,
author={Outtas, Meriem and Zhang, Lu and Deforges, Olivier and Hamidouche, Wassim and Serir, Amina},
booktitle={2018 Tenth International Conference on Quality of Multimedia Experience (QoMEX)}, title={Evaluation of No-reference quality metrics for Ultrasound liver images},
year={2018},
volume={},
number={},
pages={1-3},
abstract={Although assessing post-processed medical images is still done by radiologists (rather than computers), numerous algorithms dedicated to medical image processing are developed without taking into consideration the expert's perceived quality scores. In order to evaluate these algorithms, we study in this paper four No-Reference(NR) quality assessment metrics in terms of correlation with perceived scores of experts. These scores were obtained through subjective tests conducted on ultrasound (US) livers images. Results show that one NR metric among the four evaluated performs the best for assessing the quality of US images. However, further study is needed for the development of more suitable NR metrics.},
keywords={Measurement;Biomedical imaging;Speckle;Ultrasonic imaging;Liver;Correlation;Image quality;Medical Ultrasound images;Subjective Quality assessment;Objective Quality},
doi={10.1109/QoMEX.2018.8463299},
ISSN={2472-7814},
month={May},}
@INPROCEEDINGS{8451597,
author={Bakir, Nader and Hamidouche, Wassim and Déforges, Olivier and Samrouth, Khouloud and Khalil, Mohamad},
booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)}, title={Light Field Image Compression Based on Convolutional Neural Networks and Linear Approximation},
year={2018},
volume={},
number={},
pages={1128-1132},
abstract={Computer vision applications such as refocusing, segmentation and classification become one of the most advanced imaging services. Light Field (LF) imaging systems provide a rich semantic information of the scene. Using a dense set of cameras and microlens arrays (Plenoptic camera), the direction of each ray coming from the scene toward the LF capture system can be extracted and represented by spatial and angular coordinates. However, such imaging system induces many drawbacks including the large amount of data produced and complexity increase for scene representation. In this paper, we propose an efficient LF image coding scheme. This scheme first encodes a sparse set of views using the latest hybrid video encoder (JEM). Then, it estimates a second sparse set of views using a linear approximation. At the decoder side, we use a Deep Learning (DL) approach to estimate the whole LF image from the reconstructed sparse sets of views. Experimental results show that the proposed scheme provides higher visual quality and overcomes the state of the art LF image compression solution by 30 % bitrate gain.},
keywords={Decoding;Image coding;Encoding;Cameras;Linear approximation;Machine learning;Light Field;Machine Learning;Linear approximation;CNN;future video coding},
doi={10.1109/ICIP.2018.8451597},
ISSN={2381-8549},
month={Oct},}
@INPROCEEDINGS{8456261,
author={Mercat, Alexandre and Arrestier, Florian and Pelcat, Maxime and Hamidouche, Wassim and Menard, Daniel},
booktitle={2018 Picture Coding Symposium (PCS)}, title={Machine Learning Based Choice of Characteristics for the One-Shot Determination of the HEVC Intra Coding Tree},
year={2018},
volume={},
number={},
pages={263-267},
abstract={In the last few years, the Internet of Things (IoT) has become a reality. Forthcoming applications are likely to boost mobile video demand to an unprecedented level. A large number of systems are likely to integrate the latest MPEG video standard High Efficiency Video Coding (HEVC) in the long run and will particularly require energy efficiency. In this context, constraining the computational complexity of embedded HEVC encoders is a challenging task, especially in the case of software encoders. The most energy consuming part of a software intra encoder is the determination of the coding tree partitioning, i.e. the size of pixel blocks. This determination usually requires an iterative process that leads to repeating some encoding tasks. State-of-the-art studies have focused on predicting, from “easily” computed characteristics, an efficient coding tree. They have proposed and evaluated independently many characteristics for one-shot quad-tree prediction. In this paper, we present a fair comparison of these characteristics using a Machine Learning approach and a real-time HEVC encoder. Both computational complexity and information gain are considered, showing that characteristics are far from equivalent in terms of coding tree prediction performance.},
keywords={Encoding;Machine learning;Copper;Training;Computational complexity;Video sequences},
doi={10.1109/PCS.2018.8456261},
ISSN={2472-7822},
month={June},}
@INPROCEEDINGS{8456307,
author={Herrou, Glenn and Hamidouche, Wassim and Morin, Luce},
booktitle={2018 Picture Coding Symposium (PCS)}, title={Wavelet Decomposition Pre-processing for Spatial Scalability Video Compression Scheme},
year={2018},
volume={},
number={},
pages={149-153},
abstract={Scalable video coding enables to compress the video at different formats within a single layered bitstream. SHVC, the scalable extension of the High Efficiency Video Coding (HEVC) standard, enables x2 spatial scalability, among other additional features. The closed-loop architecture of the SHVC codec is based on the use of multiple instances of the HEVC codec to encode the video layers, which considerably increases the encoding complexity. With the arrival of new immersive video formats, like 4K, 8K, High Frame Rate (HFR) and 360° videos, the quantity of data to compress is exploding, making the use of high-complexity coding algorithms unsuitable. In this paper, we propose a lowcomplexity scalable coding scheme based on the use of a single HEVC codec instance and a wavelet-based decomposition as pre-processing. The pre-encoding image decomposition relies on well-known simple Discrete Wavelet Transform (DWT) kernels, such as Haar or Le Gall 5/3. Compared to SHVC, the proposed architecture achieves a similar rate distortion performance with a coding complexity reduction of 50%.},
keywords={Encoding;Wavelet transforms;Spatial resolution;Scalability;Video coding},
doi={10.1109/PCS.2018.8456307},
ISSN={2472-7822},
month={June},}
@INPROCEEDINGS{8456275,
author={Bichon, Maxime and Le Tanou, Julien and Ropert, Michael and Hamidouche, Wassim and Morin, Luce and Zhang, Lu},
booktitle={2018 Picture Coding Symposium (PCS)}, title={Temporal Adaptive Quantization using Accurate Estimations of Inter and Skip Probabilities},
year={2018},
volume={},
number={},
pages={81-85},
abstract={Hybrid video coding systems use spatial and temporal predictions in order to remove redundancies within the video source signal. These predictions create coding-scheme-related dependencies, often neglected for sake of simplicity. The R-D Spatio-Temporal Adaptive Quantization (RDSTQ) solution uses such dependencies to achieve better coding efficiency. It models the temporal distortion propagation by estimating the probability of a Coding Unit (CU) to be Inter coded. Uased on this probability, each CU is given a weight depending on its relative importance compared to other CUs. However, the initial approach roughly estimates the Inter probability and does not take into account the Skip mode characteristics in the propagation. It induces important Target uitrate Deviation (TBD) compared to the reference target rate. This paper provides undeniable improvements of the original RDSTQ model in using a more accurate estimation of the Inter probability. Then a new analytical solution for local quantizers is obtained by introducing the Skip probability of a CU into the temporal distortion propagation model. The proposed solution brings -2.05% BD-BR gain in average over the RDSTQ at low rate, which corresponds to -13.54% BD-BR gain in average against no local quantization. Moreover, the TBD is reduced from 38% to 14%.},
keywords={Distortion;Quantization (signal);Encoding;Bit rate;Copper;Estimation;Optimization},
doi={10.1109/PCS.2018.8456275},
ISSN={2472-7822},
month={June},}
@INPROCEEDINGS{8456301,
author={Biatek, Thibaud and Travers, Jean-Francois and Cabarat, Pierre-Loup and Hamidouche, Wassim},
booktitle={2018 Picture Coding Symposium (PCS)}, title={Backward Compatible Layered Video Coding for 360° Video Broadcast},
year={2018},
volume={},
number={},
pages={318-322},
abstract={Recently, coding of 360° video contents has been investigated in the context of over-the-top streaming services. To be delivered using terrestrial broadcast, it is required to provide backward compatibility of such content to legacy receivers. In this paper, a novel layered coding scheme is proposed to address the delivery of 360° video content over terrestrial broadcast networks. One or several views are extracted from the 360° video and coded as base layers using standard HEVC encoding. Inter-layer reference pictures are built based on projected base-layers and are used in the enhancement layer to encode the 360° video. Experimental results show that the proposed approach provides substantial coding gains of 14.99% compared to simulcast coding and enables limited coding overhead of 5.15% compared to 360° single-layer coding.},
keywords={Encoding;Decoding;Streaming media;Two dimensional displays;Transform coding;Interpolation;Video coding},
doi={10.1109/PCS.2018.8456301},
ISSN={2472-7822},
month={June},}
@INPROCEEDINGS{8416628,
author={Herrou, Glenn and Hamidouche, Wassim and Morin, Luce},
booktitle={2018 Data Compression Conference}, title={Low-Complexity Spatial Scalability Scheme Using HEVC for 4K and VR Videos},
year={2018},
volume={},
number={},
pages={411-411},
abstract={Scalable video coding enables to compress the video at different formats within a single layered bitstream. SHVC, the scalable extension of the High Efficiency Video Coding (HEVC) standard, enables x2 spatial scalability, among other additional features. The closed-loop architecture of the SHVC codec is based on the use of multiple instances of the HEVC codec to encode the video layers, which considerably increases the encoding complexity. With the arrival of new immersive video formats, like 4K, 8K, High Frame Rate (HFR) and 360° videos, the quantity of data to compress is exploding, making the use of high-complexity coding algorithms unsuitable. In this paper, we propose a low-complexity scalable coding scheme based on the use of a single HEVC codec instance and a wavelet-based decomposition as preprocessing. The pre-encoding image decomposition relies on well-known simple Discrete Wavelet Transform (DWT) kernels, such as Haar or Le Gall 5/3. Compared to SHVC, the proposed architecture achieves a similar rate distortion performance with a coding complexity reduction of 50%.},
keywords={Encoding;Scalability;Videos;Video coding;Complexity theory;Image resolution;Standards;spatial scalability;HEVC;SHVC;low complexity;wavelet transforms},
doi={10.1109/DCC.2018.00064},
ISSN={2375-0359},
month={March},}
@INPROCEEDINGS{8364448,
author={Kammoun, A. and Ben Jdidia, S. and Belghith, F. and Hamidouche, W. and Nezan, J. F. and Masmoudi, N.},
booktitle={2018 4th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)}, title={An optimized hardware implementation of 4-point adaptive multiple transform design for post-HEVC},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Under the exploration of the future video coding standard, a new transform design called Adaptive Multiple Transform (AMT) has been proposed. It involves five DCT/DST-based transform types known as: DCT-II, DCT-VIII, DCT-V, DST-I and DST-VII. This work, proposes multiplierless hardware architectures of size 4 for all considered transform types. These architectures are implemented in FPGA benefitting from both correlation and symmetry properties of the matrices coefficients. This paper presents and compares two different architecture aspects without and with involving state-machines to evaluate their effect on the proposed hardware design. The experimental and synthesis results show that the two methods, supporting all five transform types, require less than 3% of the offered FPGA device area and provide respectively 318 MHz and 285 MHz as maximum operation frequency. With adding the pipelining operation, the proposed designs can support real time coding of 4Kp30 and 2Kp60 videos, respectively. Moreover, the implementation based on state-machines offers about 45% operations number and hardware area reduction.},
keywords={Computer architecture;Hardware;Standards;Video coding;Discrete cosine transforms;Encoding},
doi={10.1109/ATSIP.2018.8364448},
ISSN={},
month={March},}
@INPROCEEDINGS{8351775,
author={Sidaty, Naty and Viitanen, Marko and Hamidouche, Wassim and Vanne, Jarno and Déforges, Olivier},
booktitle={2018 IEEE International Symposium on Circuits and Systems (ISCAS)}, title={Live Demonstration: End-to-End Real-Time ROI-based Encryption in HEVC Videos},
year={2018},
volume={},
number={},
pages={1-1},
abstract={This paper presents a demonstration setup for live HEVC video coding with Region of Interest (ROI) encryption. The showcased approach splits video frames into independent HEVC tiles and encrypts those belonging to the ROI. This end-to-end content protection scheme is put into practice by integrating the algorithms of selective encryption into Kvazaar HEVC encoder and decryption into openHEVC decoder. The shown implementation performs secure encryption of the ROI in real time with small bit rate and complexity overhead.},
keywords={Videos;Encryption;Real-time systems;Decoding;Electronic mail;Bit rate;user identity management;High Efficiency Video Coding (HEVC);tiles;Region of Interest (ROI);selective encryption},
doi={10.1109/ISCAS.2018.8351775},
ISSN={2379-447X},
month={May},}
@ARTICLE{8114259,
author={Pelcat, Maxime and Mercat, Alexandre and Desnos, Karol and Maggiani, Luca and Liu, Yanzhou and Heulot, Julien and Nezan, Jean-François and Hamidouche, Wassim and Ménard, Daniel and Bhattacharyya, Shuvra S.},
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, title={Reproducible Evaluation of System Efficiency With a Model of Architecture: From Theory to Practice},
year={2018},
volume={37},
number={10},
pages={2050-2063},
abstract={Current trends in high performance and embedded computing include design of increasingly complex hardware architectures with high parallelism, heterogeneous processing elements, and nonuniform communication resources. In order to take hardware and software design decisions, early evaluations of the system nonfunctional properties are needed. These evaluations of system efficiency require electronic system-level information on both algorithms and architecture. Contrary to algorithm models for which a major body of work has been conducted on defining formal models of computation (MoCs), architecture models from the literature are mostly empirical models from which reproducible experimentation requires the accompanying software. In this paper, a precise definition of a model of architecture (MoA) is proposed that focuses on reproducibility and abstraction and removes the overlap previously existing between the notions of MoA and MoC. A first MoA, called the linear system-level architecture model (LSLA), is presented. To demonstrate the generic nature of the proposed new architecture modeling concepts, we show that the LSLA model can be integrated flexibly with different MoCs. LSLA is then used to model the energy consumption of a state-of-the-art multiprocessor system-on-chip (MPSoC) when running an application described using the synchronous dataflow MoC. A method to automatically learn LSLA model parameters from platform measurements is introduced. Despite the high complexity of the underlying hardware and software, a simple LSLA model is demonstrated to estimate the energy consumption of the MPSoC with a fidelity of 86%.},
keywords={Computational modeling;Computer architecture;Ports (Computers);Hardware;Algorithm design and analysis;Energy consumption;Complexity theory;Architecture;design space exploration (DSE);hardware/software co-design;modeling;multiprocessor SoC (MPSoC);performance optimization;power modeling and estimation;system on chip (SoC)},
doi={10.1109/TCAD.2017.2774822},
ISSN={1937-4151},
month={Oct},}
@INPROCEEDINGS{8296832,
author={Sidaty, Naty and Hamidouche, Wassim and Deforges, Olivier and Philippe, Pierrick},
booktitle={2017 IEEE International Conference on Image Processing (ICIP)}, title={Compression efficiency of the emerging video coding tools},
year={2017},
volume={},
number={},
pages={2996-3000},
abstract={With the drastic increasing of multimedia applications and video coarse consumption, video compression and content quality evaluation have become an exciting and challenging topic. Recently, a new coding tool has been developed under the Joint Exploration Model (JEM) software with the main goal to provide a high bit rate saving compared to the HEVC standard. In this paper we present a performance-based comparison between the JEM and HEVC reference software (HM) through an objective measurements and a subjective quality assessments. A set of video sequences, in two spatial resolutions High Definition (HD) and Ultra-High Definition (UHD), have been used in this study. These videos are encoded using both JEM and HM software at different bitrates. Results have shown that the JEM codec enables, subjectively, a quality enhancement up to 40% at similar low bit rates. Objectively, this quality improvement is ranging from 35% to 37% depending on the spatial resolution. However, at high bit rate, the HM reference software enables a high video quality and thus its becomes more difficult to perceive the quality enhancement is about brought by the JEM codec. In addition, some video contents are difficult to encode and, consequently, the JEM enables only slight perceived quality improvement especially at the highest considered bitrates and for 4K resolutions.},
keywords={Transforms;Tools;Bit rate;Encoding;High definition video;Software;Video sequences;Video quality evaluation;HEVC;JEM;subjective assessment;future video coding},
doi={10.1109/ICIP.2017.8296832},
ISSN={2381-8549},
month={Sep.},}
@INPROCEEDINGS{8296511,
author={Outtas, M. and Zhang, L. and Deforges, O. and Serir, A. and Hamidouche, W.},
booktitle={2017 IEEE International Conference on Image Processing (ICIP)}, title={Multi-output speckle reduction filter for ultrasound medical images based on multiplicative multiresolution decomposition},
year={2017},
volume={},
number={},
pages={1397-1401},
abstract={Ultrasonographic examination, either as visual inspection or quantitative analysis, is less effective than other medical imaging systems due to speckle noise. The state-of-the-art speckle reduction methods often offers an effective speckle reduction but generally they suffer from oversmoothig, blurring effect and man-made/artificial appearance. In this paper, a new Multi-Output Filter based on a Multiplicative Multiresolution Decomposition (MOF-MMD) is proposed. This multiscale based method, particularly efficient in the case of multiplicative noise, enhances distinctively three outputs: edges, texture and the global image. The multi-output filter aims at offering an enhanced images according to the features desired by radiologists. The different structures, textures and edges are filtered according to the contour image obtained by morphological operators. Finally, we compare the MOF-MMD method with two state-of-the-art speckle reduction methods in terms of speckle reduction capacity and image quality improvement. The results show that the proposed method offers an effective speckle reduction with an improvement of the image quality without blurry and over-smoothing effect.},
keywords={Speckle;Image edge detection;Image resolution;Biomedical imaging;Image segmentation;Image reconstruction;Microsoft Windows;Speckle reduction;Ultrasound medical images;MMD},
doi={10.1109/ICIP.2017.8296511},
ISSN={2381-8549},
month={Sep.},}
@INPROCEEDINGS{8296437,
author={Liu, Y. and Sidaty, N. and Hamidouche, W. and Déforges, O. and Valenzise, G. and Zerman, E.},
booktitle={2017 IEEE International Conference on Image Processing (ICIP)}, title={An adaptive perceptual quantization method for HDR video coding},
year={2017},
volume={},
number={},
pages={1027-1031},
abstract={This paper presents a new adaptive perceptual quantization method for the High Dynamic Range (HDR) content. This method considers the luminance distribution of the HDR image as well as the Minimum Detectable Contrast (MDC) thresholds to preserve the contrast information during quantization. Base on this method, we develop a mapping function for HDR video compression and apply it to a HEVC Main 10 Profile-based video coding chain. Our experiments show that the proposed mapping function can efficiently improve the quality of the reconstructed HDR video in both objective and subjective assessments.},
keywords={Video coding;Quantization (signal);Encoding;Bit rate;Image reconstruction;Resource management;Dynamic range;High Dynamic Range (HDR);minimum detectable contrast (MDC);HEVC;HDR video coding},
doi={10.1109/ICIP.2017.8296437},
ISSN={2381-8549},
month={Sep.},}
@ARTICLE{8246791,
author={Liu, Yi and Hamidouche, Wassim and Déforges, Olivier and Pescador, Fernando},
journal={IEEE Transactions on Consumer Electronics}, title={A multi-modeling electro-optical transfer function for display and transmission of high dynamic range content},
year={2017},
volume={63},
number={4},
pages={350-358},
abstract={This paper proposes a multi-modeling Electro-Optical Transfer Function (EOTF) enabling efficient use of the code word to preserve the contrast details in the quantized High Dynamic Range (HDR) content. Different from the classic EOTF that employs one single model, the proposed EOTF considers four models to adjust the reproducible contrast according to the luminance level of the HDR content. Furthermore, the proposed EOTF and its inverse process functions can be integrated on HDR cameras and displays with a low computational cost. In order to evaluate the performance of the proposed EOTF, it has been applied into a HDR video transmission chain based on the High Efficiency Video Coding (HEVC) codec. The standardized Perceptual Quantizer (PQ) EOTF and Hybrid Log-Gamma (HLG) Opto-Electronic Transfer Function (OETF) are also introduced and compared with the proposed EOTF in terms of the reproducible contrast. The objective evaluation results show that the proposed EOTF brings a better quality of the decoded HDR video than that generated by the standard function. A subjective test was also conducted in a commercial HDR television and the results show that the proposed EOTF can present more visible details than PQ, particularly for the HDR content with dark scenes. Therefore, the proposed multi-modeling EOTF is useful for the transmission and display of the HDR content in the consumer electronics.},
keywords={Transfer functions;Dynamic range;Standards;Electrooptical waveguides;High efficiency video coding;TV;Production;EOTF;High Dynamic Range (HDR);High Efficiency Video Coding (HEVC);Hybrid Log-Gamma (HLG);Perceptual Quantizer (PQ).},
doi={10.1109/TCE.2017.015068},
ISSN={1558-4127},
month={November},}
@INPROCEEDINGS{8110020,
author={Jallouli, Anas and Belghith, Fatma and Ben Ayed, Mohamed Ali and Hamidouche, Wassim and Nezan, Jean-François and Masmoudi, Nouri},
booktitle={2017 IEEE International Workshop on Signal Processing Systems (SiPS)}, title={Statistical analysis of Post-HEVC encoded videos},
year={2017},
volume={},
number={},
pages={1-6},
abstract={The Post-HEVC is the emerging video coding standard beyond the High Efficiency Video Coding (HEVC) standard. It is more complex in transformation and prediction steps but it offers the opportunity of 3D and 360° videos coding and compression. This paper presents different statistical analyzes of Post-HEVC encoded videos especially analysis on 1D and 2D transformation types and analysis on intra and inter prediction types of some test videos for different classes and resolutions. Analyzes are carried out at the decoder level where the coding decision has already been taken by the encoder. Results show that the choice of transformation (type and size) and the prediction type (intra or inter) depends on the nature of video: motion and texture. This work can be considered as a milestone for proposing intelligent algorithms based on video characteristics to perform fast decision in the Post-HEVC encoding process.},
keywords={Videos;Transforms;Two dimensional displays;Encoding;Standards;Video coding;Statistical analysis},
doi={10.1109/SiPS.2017.8110020},
ISSN={2374-7390},
month={Oct},}
@INPROCEEDINGS{8110025,
author={Mercat, Alexandre and Arrestier, Florian and Pelcat, Maxime and Hamidouche, Wassim and Menard, Daniel},
booktitle={2017 IEEE International Workshop on Signal Processing Systems (SiPS)}, title={Prediction of quad-tree partitioning for budgeted energy HEVC encoding},
year={2017},
volume={},
number={},
pages={1-6},
abstract={High Efficiency Video Coding (Hevc), the newest video encoding standard, provides up to 50% bitrate savings compared to the state-of-art H.264/AVC standard for the same perceptual video quality. In the last few years, the Internet of Things (IoT) has become a reality. Forthcoming applications are likely to boost mobile video demand to an unprecedented level. A large number of systems are likely to integrate HEVC codec in the long run and will need to be energy aware. In this context, constraining the energy consumption of HEVC encoder becomes a challenging task for embedded applications based on a software encoder. The most frequent approach to overcome this issue consists in optimising the coding tree structure to balance compression efficiency and energy consumption. In the purpose of budgeting the energy consumption of real-time HEVC encoder, we propose in this paper a variance-aware quad-tree prediction to limit the recursive RDO process. The experimental results show that the proposed energy reduction scheme achieve on average 60% of energy reduction for a slight bit rate increase of 3.4%.},
keywords={Complexity theory;Copper;Sociology;Statistics;Encoding;Energy consumption;Real-time systems},
doi={10.1109/SiPS.2017.8110025},
ISSN={2374-7390},
month={Oct},}
@INPROCEEDINGS{8096144,
author={Bergeron, Cyril and Sidaty, Naty and Hamidouche, Wassim and Boyadjis, Benoit and Le Feuvre, Jean and Lim, Youngkwon},
booktitle={2017 22nd International Conference on Digital Signal Processing (DSP)}, title={Real-time selective encryption solution based on ROI for MPEG-A visual identity management AF},
year={2017},
volume={},
number={},
pages={1-5},
abstract={As part of a new MPEG-A standardization activity, called Visual Identity Management Application Format (VIMAF), this paper presents an end-to-end encryption solution of Region of Interest (ROI) in both AVC and HEVC encoded streams for privacy protection applications. This solution uses a selective encryption method that encrypts only the most sensitive information of the video and proposes a new adapted syntax in order to facilitate interoperability between equipment. Objective video quality measurements have shown the robustness of the proposed selective encryption solution with only a slight bitrate increase.},
keywords={Encryption;Streaming media;Standards;Video coding;Transform coding;Privacy;Visual Identity Management;AVC;HEVC;ROI;selective encryption;ISOBMFF},
doi={10.1109/ICDSP.2017.8096144},
ISSN={2165-3577},
month={Aug},}
@INPROCEEDINGS{8081452,
author={Parois, Ronan and Hamidouche, Wassim and Vieron, Jérôme and Raulet, Mickaël and Deforges, Olivier},
booktitle={2017 25th European Signal Processing Conference (EUSIPCO)}, title={Efficient parallel architecture for a real-time UHD scalable HEVC encoder},
year={2017},
volume={},
number={},
pages={1465-1469},
abstract={The scalable extension (SHVC) of the High Efficiency Video Coding (HEVC) allows encoding in layers a video with multiple quality level such as resolution, bit-depth or Signal to Noise Ratio (SNR). Compared to the equivalent HEVC simulcast, the SHVC extension provides inter-layer prediction mechanisms enabling significant bit-rate savings. Moreover these inter-layer prediction mechanisms are less complex than those from former standards. Therefore, SHVC seems a promising solution for both broadcast and storage applications and is considered in the ATSC 3.0 as video coding solution. Indeed the spatial scalability is an application use-case considered in the ATSC 3.0. This paper proposes a scalable multi-layer architecture combining pipelined software HEVC encoders. The proposed architecture provides a good trade-off between parallelism over layer and latency. Moreover two configurations are proposed for Live or File encodings with real-time or best coding efficiency targets, respectively. Results present a 2× spatial scalability application of this architecture achieving in a low-delay configuration real-time video encodings of 1080p60 and 1600p30 sequences. Moreover the proposed SHVC solution also demonstrated real-time encodings of UHD contents at an ATSC 3.0 meeting in random-access configuration.},
keywords={Encoding;Real-time systems;Video coding;Pipelines;Standards;Scalability;Computer architecture},
doi={10.23919/EUSIPCO.2017.8081452},
ISSN={2076-1465},
month={Aug},}
@INPROCEEDINGS{8081363,
author={Sidaty, Naty and Heulot, Julien and Hamidouche, Wassim and Nogues, Erwan and Pelcat, Maxime and Menard, Daniel},
booktitle={2017 25th European Signal Processing Conference (EUSIPCO)}, title={Reducing computational complexity in HEVC decoder for mobile energy saving},
year={2017},
volume={},
number={},
pages={1026-1030},
abstract={With the growing development of video applications and services for mobile devices, saving energy consumption when managing video is becoming a more and more important issue. The challenge is then to deliver video with high quality while reducing the energy consumption. In this paper, we investigate the relationship between subjective video quality and energy consumption in an HEVC decoder. By reducing the computational complexity of the decoder, drastic energy savings can be achieved without affecting the visual quality. In this paper, two computation methods and several filter configurations are tested. Results show that at least 10% of energy savings are obtained with the same subjective perceived quality. In addition, objective measurements have shown that only a slight quality degradation has been noticed.},
keywords={Decoding;Quality assessment;Energy consumption;Video recording;Standards;Computational complexity},
doi={10.23919/EUSIPCO.2017.8081363},
ISSN={2076-1465},
month={Aug},}
@INPROCEEDINGS{7965664,
author={Sidaty, Naty and Hamidouche, Wassim and Deforges, Olivier and Philippe, Pierrick},
booktitle={2017 Ninth International Conference on Quality of Multimedia Experience (QoMEX)}, title={Emerging video coding performance: 4K quality monitoring},
year={2017},
volume={},
number={},
pages={1-3},
abstract={The new coding tools, developed under the Joint Exploration Model (JEM) software, have been proposed with the main goal to explore their potential coding gain in the perspective to develop a new video coding standard. In this paper we present a performance-based comparison between the JEM and HEVC reference software (HM) through a set of subjective quality assessments. Different video sequences, encoded using both JEM and HM software at different bitrates, have been used in this experiment. Results have shown that the JEM codec enables, subjectively, a quality enhancement up to 40% at similar low bit rates.},
keywords={Bit rate;Encoding;Software;Video coding;Tools;Quality assessment;Video sequences;Video quality evaluation;HEVC;JEM;subjective assessment;future video coding;4K},
doi={10.1109/QoMEX.2017.7965664},
ISSN={2472-7814},
month={May},}
@INPROCEEDINGS{7952415,
author={Sidaty, Naty and Hamidouche, Wassim and Deforges, Olivier},
booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={A new perceptual assessment methodology for selective HEVC video encryption},
year={2017},
volume={},
number={},
pages={1542-1546},
abstract={Video data security is one of the most research topic in the recent years. It is widely used in the multimedia applications such as video-conferencing, Video on Demand and Pay-TV services. Although many video encryption methods and objective measurements have been employed, few real time schemes and no subjective studies have been proposed. In this paper we investigate a set of selective video encryption schemes by encrypting only a few parameters in HEVC video streams. Firstly, we carried out an in-depth subjective study of three proposed selective HEVC video encryption schemes. A panel of observers has participated in this test campaign in order to evaluate the degree of visibility of the encrypted videos at different bitrates. Experimental results are presented and analysed, showing therefore that two proposed selected encryption schemes allow a high perceptual security level by masking the whole details of the video content, while the third scheme achieves a high security level, with a content nearly unidentifiable. In addition, subjective scores can be used as ground truth for assessing selective video encryption methods, instead of classical objective signal-based metrics, which are not correlated with human judgment.},
keywords={Encryption;Streaming media;Encoding;Measurement;Standards;selective video encryption;HEVC standard;subjective visual security assessment},
doi={10.1109/ICASSP.2017.7952415},
ISSN={2379-190X},
month={March},}
@INPROCEEDINGS{7952339,
author={Mercat, Alexandre and Arrestier, Florian and Hamidouche, Wassim and Pelcat, Maxime and Menard, Daniel},
booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Constrain the Docile CTUs: An In-Frame complexity allocator for HEVC Intra encoders},
year={2017},
volume={},
number={},
pages={1163-1167},
abstract={High Efficiency Video Coding (HEVC) is one of the latest released video standards and offers up to 40% bitrate savings when compared to the widespread H.264/AVC standard, at the cost of a substantial complexity growth. Constraining the complexity of HEVC encoding is a challenging task for embedded applications based on a software encoder. The most frequent approach to solve this problem is to optimise the coding tree structure to balance compression efficiency and computational complexity. In this context, we propose and assess a method to adequately allocate the computational complexity among coding units in a frame encoded in Intra mode. By studying an open-source real-time HEVC encoder, correlations are observed between Rate-Distortion (RD)-cost and encoding complexity that motivate a new complexity allocation technique. This technique, called “Constrain the Docile CTUs” (CDC), consists of allocating less computational complexity to units with low RD-costs and using RD-costs from preceding images as predictors for the current RD-costs. Experimental results demonstrate substantial gains, up to 36% of Bjøntegaard Delta Bit Rate (BD-BR), when using CDC method instead of other allocation methods.},
keywords={Encoding;Complexity theory;Correlation;Bit rate;Copper;Niobium;Video coding;Complexity allocator;HEVC;quad-tree partitioning;embedded platforms;Intra encoding;RD-cost},
doi={10.1109/ICASSP.2017.7952339},
ISSN={2379-190X},
month={March},}
@INPROCEEDINGS{7952414,
author={Bichon, M. and Le Tanou, J. and Ropert, M. and Hamidouche, W. and Morin, L. and Zhang, L.},
booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Inter-block dependencies consideration for intra coding in H.264/AVC and HEVC standards},
year={2017},
volume={},
number={},
pages={1537-1541},
abstract={Recent MPEG video compression standards are still block-based: blocks of pixels are sequentially coded using spatial or temporal prediction schemes. For each block, a vector of coding parameters has to be selected. In order to limit the complexity of this decision, independence between blocks is assumed, and coding parameters are locally optimized to maximize the coding efficiency. Few studies have investigated the benefits of inter-block dependencies consideration using Joint Rate-Distortion Optimization (JRDO), especially in Intra coding. To the best of our knowledge, maximum achievable gains of such approaches have never been exhibited. In this paper, we propose two JRDO models performing joint optimization of multiple blocks applied to intra prediction mode decision. The proposed models have been evaluated in both H.264/AVC and HEVC standards. These two models enables a bitrate saving with respect to the classical RDO model up to -3.10% and -2.31% in H.264/AVC and HEVC, respectively.},
keywords={Encoding;Copper;Optimization;Distortion;Context;Standards;Complexity theory;Inter-Block Dependencies;Joint Rate-Distortion Optimization;Intra Coding;H.264/AVC;HEVC},
doi={10.1109/ICASSP.2017.7952414},
ISSN={2379-190X},
month={March},}
@INPROCEEDINGS{7952716,
author={Cabarat, Pierre-Loup and Hamidouche, Wassim and Déforges, Olivier},
booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Real-time and parallel SHVC hybrid codec AVC to HEVC decoder},
year={2017},
volume={},
number={},
pages={3046-3050},
abstract={Scalable High efficiency Video Coding (SHVC) is the scalable extension of the latest video coding standard High Efficiency Video Coding (HEVC). One of the key novelties introduced by SHVC is that it enables hybrid codec scalability. This basically means that the video layers can be encoded with different video standards providing backward compatibility between codecs. In this paper, we propose a software parallel SHVC decoder in hybrid codec scalability configuration. The proposed design consists of an Advanced Video Coding (AVC) decoder for the Base Layer (BL) and a HEVC decoder for the Enhanced Layer (EL). In order to perform Inter Layer Prediction (ILP), a communication of decoding states and outputs is established between the two decoders. While the native frame based parallelism is still allowed within the two decoders, the proposed design also enables the use of frame based parallelism between the two decoders. The proposed software design enables a real time decoding of the HEVC EL at 2160p60 while the AVC base layer is decoded at 1080p60 for ×2 spatial scalability.},
keywords={Decoding;Codecs;Scalability;Video coding;Encoding;Standards;Software;Real-time;SHVC;hybrid codec;HEVC;AVC},
doi={10.1109/ICASSP.2017.7952716},
ISSN={2379-190X},
month={March},}
@INPROCEEDINGS{7952338,
author={Mercat, Alexandre and Arrestier, Florian and Hamidouche, Wassim and Pelcat, Maxime and Menard, Daniel},
booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Energy reduction opportunities in an HEVC real-time encoder},
year={2017},
volume={},
number={},
pages={1158-1162},
abstract={High Efficiency Video Coding (HEVC) is one of the latest released video standards and offers up to 40% bitrate savings when compared to the widespread H.264/AVC standard, at the cost of a substantial complexity growth. Constraining the complexity of HEVC encoding is a challenging task for embedded applications based on a software encoder. In the last few years, the Internet of Thingss (IoTs) has become a reality. Forecoming applications are likely to boost mobile video demand to an unprecedented level. In this context, designing energy-efficient HEVC real-time encoders is becoming a major challenge for software and hardware designers. In this paper, an analysis is conducted of the energy reduction opportunities offered by an HEVC encoder. The energy reduction search space is demonstrated, and the impact on energy consumption of encoding tools at various levels of granularity is measured.},
keywords={Encoding;Energy consumption;Complexity theory;Real-time systems;Streaming media;Energy resolution;Software;Energy consumption;HEVC;real-time encoder;embedded platforms},
doi={10.1109/ICASSP.2017.7952338},
ISSN={2379-190X},
month={March},}
@INPROCEEDINGS{7927039,
author={Mercat, Alexandre and Bonnot, Justine and Pelcat, Maxime and Hamidouche, Wassim and Menard, Daniel},
booktitle={Design, Automation Test in Europe Conference Exhibition (DATE), 2017}, title={Exploiting computation skip to reduce energy consumption by approximate computing, an HEVC encoder case study},
year={2017},
volume={},
number={},
pages={494-499},
abstract={Approximate computing paradigm provides methods to optimize algorithms with considering both computational accuracy and complexity. This paradigm can be exploited at different levels of abstraction, from technological to application levels. Approximate computing at algorithm level aims at reducing computational complexity by approximating or skipping block functions of the computation. Numerous applications in the signal and image processing domain integrate algorithms based on discrete optimization techniques. These techniques minimize a cost function by exploring the search space. In this paper, a new approach is proposed to exploit the computation-skipping approximate computing concept by using the Smart Search Space Reduction (Sssr) technique. Sssr enables early selection of the best candidate configurations to reduce the search space. An efficient SSSR technique adjusts configuration selectivity to reduce execution complexity while selecting the most suitable functions to skip. The High Efficiency Video Coding (HEVC) encoder in All Intra (AI) profile is used as a case study to illustrate the benefits of SSSR. In this application, two functions use discrete optimization to explore different solutions and select the one leading to the minimal cost in terms of bitrate/quality and computational energy: coding-tree partitioning and intra-mode prediction. By applying SSSR to this use case, energy reductions from 20% to 70% are explored through Pareto in Rate-Energy space.},
keywords={Approximation algorithms;Signal processing algorithms;Approximate computing;Space exploration;Prediction algorithms;Energy consumption;Complexity theory},
doi={10.23919/DATE.2017.7927039},
ISSN={1558-1101},
month={March},}
@INPROCEEDINGS{7921914,
author={Reuzé, Kevin and Philippe, Pierrick and Hamidouche, Wassim and Déforges, Olivier},
booktitle={2017 Data Compression Conference (DCC)}, title={Cluster Adapted Signalling for Intra Prediction in HEVC},
year={2017},
volume={},
number={},
pages={191-200},
abstract={The High Efficiency Video Coding (HEVC) standard defines 35 Intra Prediction Modes (IPM) to provide an efficient compression of intra coded blocks. Those IPMs are signalled to the decoder through the use of three compression tools: prediction, clustering and coding. In this paper we provide improvements to these three tools through: new labels for the prediction, new tests for the clustering and new coding schemes for the coding. The most significant improvement consists in the provision of a cluster-dependent code: adapting the coding scheme to the available information enables the average symbol cost to get within close margin of the entropy of the data. The system providing the best compression efficiency based on these improvements is then computed, enabling significant reduction in the average cost required to code the IPMs. The proposed method builds a new coding system with the same complexity as HEVC with 0.41% bit-rates savings in All Intra coding configuration.},
keywords={Encoding;Tools;Decoding;Syntactics;Training;High efficiency video coding;Intra Prediction;Compression;Video Coding;HEVC;Signalling},
doi={10.1109/DCC.2017.19},
ISSN={2375-0359},
month={April},}
@INPROCEEDINGS{7906387,
author={Reuze, Kevin and Philippe, Pierrick and Deforges, Olivier and Hamidouche, Wassim},
booktitle={2016 Picture Coding Symposium (PCS)}, title={Intra prediction modes signalling in HEVC},
year={2016},
volume={},
number={},
pages={1-5},
abstract={The High Efficiency Video Coding (HEVC) standard defines 35 Intra Prediction Modes (IPM) to provide an efficient compression of intra coded blocks. To signal these IPMs to the decoder a list of 3 Most Probable Modes (MPM) is created based on the IPMs of the neighbour Intra coded blocks. These MPMs will be transmitted in the bit stream with a reduced number of bits. However, the signalling scheme used in HEVC is not optimal and can be further improved. In this paper a method is proposed to enhance the Intra mode signalling in HEVC. The IPM signaling is first modelled as a tree with forks and leaves representing tests and labels, respectively. The proposed solution introduces new decision tree process by adding new tests and new labels not considered in HEVC. This solution provides a systematic way to find the best signalling scheme for a given set of data. Experimental results show that the proposed solution enables to reduce the BD-Rate by 0.38% in all Intra coding configuration.},
keywords={Decision trees;Encoding;Redundancy;High efficiency video coding;Systematics;Standards;HEVC;Intra Prediction;Decision Tree},
doi={10.1109/PCS.2016.7906387},
ISSN={2472-7822},
month={Dec},}
@INPROCEEDINGS{7906332,
author={Herrou, Glenn and Hamidouche, Wassim and Ducloux, Xavier},
booktitle={2016 Picture Coding Symposium (PCS)}, title={HDR video quality evaluation of HEVC and VP9 codecs},
year={2016},
volume={},
number={},
pages={1-5},
abstract={Current increasing effort in the television industry towards High Dynamic Range (HDR) imaging has raised the issue of the compression of HDR content. Offering a higher peak luminance and wider color gamut, HDR video introduces new challenges to the state-of-the-art video codecs such as High Efficiency Video Coding (HEVC) or VP9, which have been designed and optimized for the compression of Standard Dynamic Range (SDR) content. This study presents a performance comparison between HEVC and VP9 in the HDR context through both objective and subjective evaluations. The experimental objective results have shown that HEVC offers from 0.6% to 38.2% bit rate savings over VP9 depending on the objective metric which is used. The subjective study demonstrated that, on average, bit rate savings greater than 47.7% can be achieved by HEVC for the same perceived quality as VP9.},
keywords={Encoding;Video coding;Color;Dynamic range;Video codecs;Bit rate;HDR;video compression;HEVC;VP9;subjective evaluation;video quality assessment},
doi={10.1109/PCS.2016.7906332},
ISSN={2472-7822},
month={Dec},}
@INPROCEEDINGS{7906334,
author={Biatek, Thibaud and Hamidouche, Wassim and Travers, Jean-Francois and Déforges, Olivier},
booktitle={2016 Picture Coding Symposium (PCS)}, title={Pre-encoding based statistical-multiplexing for hybrid delivery of UHD services using SHVC},
year={2016},
volume={},
number={},
pages={1-5},
abstract={The scalable video coding consists in encoding the video content into multiple representations, called layers, where each one refers to a specific version of the content. The scalable extension of the High Efficiency Video Coding standard SHVC is currently considered in ATSC and DVB to carry out layered and scalable programs which enables to target multiple equipments, and is also used to ensure backward compatibility with legacy receivers. In the case of hybrid delivery of HD and UHD services using SHVC, these services are encoded in two layers including base and enhancement layers, which are then broadcasted over separated channels. In this paper, a statistical multiplexing method is proposed for broadcasting of UHD services in this hybrid scenario. This innovative method considers both variable bitrate among programs and optimal SHVC layers coding, which was not considered in the existing approaches. The proposed method enables to reduce the overhead introduced by SHVC compared to the single-layer encoding by 3.3% in average while maintaining smooth quality variations among programs.},
keywords={Bit rate;Encoding;Scalability;Multiplexing;Bandwidth;Static VAr compensators;Resource management},
doi={10.1109/PCS.2016.7906334},
ISSN={2472-7822},
month={Dec},}
@INPROCEEDINGS{7853791,
author={Raffin, Erwan and Hamidouche, Wassim and Nogues, Erwan and Pelcat, Maxime and Menard, Daniel},
booktitle={2016 Conference on Design and Architectures for Signal and Image Processing (DASIP)}, title={Scalable HEVC decoder for mobile devices: Trade-off between energy consumption and quality},
year={2016},
volume={},
number={},
pages={18-25},
abstract={Scalable video coding offers a large choice of configurations when decoding a compressed video. A single encoded bitstream can be decoded in multiple modes, from a full video quality mode to different degraded video quality modes. In the bitstream, data is separated into layers, each layer containing the information relative to a quality level and depending on information from other layers. In the context of an energy constrained scalable video decoder executed on an embedded multicore platform, this paper investigates the energy consumption of an optimized decoder relative to the decoded layers and decoded video quality. These numbers show that a large set of trade-offs between energy and quality is offered by SHVC and can be used to precisely adapt the decoder to its energy constraints.},
keywords={Decoding;Scalability;Video coding;Encoding;Static VAr compensators;Video recording;Quality assessment},
doi={10.1109/DASIP.2016.7853791},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7853830,
author={Parois, Ronan and Hamidouche, Wassim and Mora, Elie Gabriel and Raulet, Mickaël and Deforges, Olivier},
booktitle={2016 Conference on Design and Architectures for Signal and Image Processing (DASIP)}, title={Demo: UHD live video streaming with a real-time scalable HEVC encoder},
year={2016},
volume={},
number={},
pages={235-236},
abstract={In this paper we present a real-time streaming demonstration with a multi-layer architecture of a pipelined software High Efficiency Video Coding (HEVC) encoders with inter-layer prediction enabling Scalable HEVC (SHVC) encodings. This SHVC encoder is implemented on an innovative platform performing real-time encodings that already demonstrated promising performance with HDR, HFR and SHVC implementation in previous demonstrations [1], [2]. The transmitted content consists of a spatial SHVC bitstream composed of a High Definition (HD) Base Layer (BL) and an Ultra HD (UHD) Enhancement Layer (EL). The encoder reads an UHD video sequences through Serial Digital Interface (SDI) ports and broadcasts the SHVC bitstream through an Internet Protocol (IP) channel. The bitstream is then decoded using a GPAC player with a real-time decoder.},
keywords={Encoding;Real-time systems;High definition video;Streaming media;High efficiency video coding;Software},
doi={10.1109/DASIP.2016.7853830},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7853792,
author={Mercat, Alexandre and Hamidouche, Wassim and Pelcat, Maxime and Menard, Daniel},
booktitle={2016 Conference on Design and Architectures for Signal and Image Processing (DASIP)}, title={Estimating encoding complexity of a real-time embedded software HEVC codec},
year={2016},
volume={},
number={},
pages={26-33},
abstract={The High Efficiency Video Coding (HEVC) standard provides up to 40% bitrate savings compared to the state-of-art H.264/AVC standard for the same perceptual video quality. Power consumption constraints represent a serious challenge for embedded applications based on a software design. A large number of systems are likely to integrate the HEVC codec in the long run and will need to be energy aware. In this context, we carry out a complexity study of the HEVC coding trees encoding process. This study shows that the complexity of encoding a Coding Unit (CU) of a given size has a non trivial probability density shape and thus can hardly be predicted with accuracy. However, we propose a model that linearly links the ratios between the complexities of coarse-grain and lower-grain CU encodings with a precision error under 6%. This model is valid for a wide range of video contents coded in Intra configurations at different bitrates. This information is useful to control encoder energy during the encoding process on battery limited devices.},
keywords={Encoding;Mathematical model;Computational modeling;Real-time systems;Computational complexity;Video coding;Complexity prediction;HEVC;quad-tree partitioning;embedded platforms},
doi={10.1109/DASIP.2016.7853792},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7853790,
author={Parois, Ronan and Hamidouche, Wassim and Mora, Elie Gabriel and Raulet, Mickaël and Deforges, Olivier},
booktitle={2016 Conference on Design and Architectures for Signal and Image Processing (DASIP)}, title={Efficient parallel architecture of an intra-only scalable multi-layer HEVC encoder},
year={2016},
volume={},
number={},
pages={11-17},
abstract={The High Efficiency Video Coding (HEVC) standard enables meeting new video quality demands such as Ultra High Definition (UHD). Its scalable extension (SHVC) allows encoding simultaneously different representations of a video, organised in layers. Thanks to inter-layer predictions, SHVC provides bit-rate savings compared to the equivalent HEVC simulcast encoding. Therefore, SHVC seems a promising solution for both broadcast and storage applications. This paper proposes a multi-layer architecture of a pipelined software HEVC encoders with two main settings: a live setting for real-time encoding and a file setting for encoding with better fidelity. The proposed architecture provides a good trade-off between coding rate and coding efficiency achieving real-time performance of 1080p60 and 1600p30 sequences in 2× spatial scalability. Moreover, experimental results show more than a 26× and 300× speed-up for the file and live settings, respectively, with respect to the scalable reference software (SHM) in an intra-only configuration.},
keywords={Encoding;Pipelines;Video coding;Streaming media;Standards;Video recording;Quality assessment},
doi={10.1109/DASIP.2016.7853790},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7786174,
author={Biatek, T. and Hamidouche, W. and Travers, J.-F. and Deforges, O.},
booktitle={2016 Data Compression Conference (DCC)}, title={Optimal Bitrate Allocation for High Dynamic Range and Wide Color Gamut Services Deployment Using SHVC},
year={2016},
volume={},
number={},
pages={299-308},
abstract={The scalable video coding enables to compress video contents into a hierarchical layered representation, each layer depicts an enhanced version of the underlying layer. SHVC is the scalable extension of HEVC and enables spatial, SNR, color-gamut, codec and bitdepth scalability. It has been proved, in the MPEG investigations prior to the recent Call for Evidence, that SHVC can support SDR-to-HDR scalability by using the color gamut scalability, when SDR and HDR signals are placed in different color gamuts. This way, SHVC can be used to address future backward compatible issues in the HDR and WCG services deployment. In this paper, we exploit the impact of bitrate ratio over performance in scalable schemes to design an adaptive rate control algorithm suitable for such deployment, considering adjustable quality and bandwidth constraints. Our method dynamically adjusts the bitrate ratio between two layers during encoding in the most quality-related optimal way under specified constraints. The proposed method is tested on scalable combinations of HD/UHD, R.709/DCI-P3/R.2020 and SDR/HDR video contents, and reduces the average overhead introduced by SHVC compared to the single-layer HEVC encoding by 23%.},
keywords={Bit rate;Encoding;Scalability;Video coding;Standards;Static VAr compensators;Transform coding},
doi={10.1109/DCC.2016.26},
ISSN={1068-0314},
month={March},}
@INPROCEEDINGS{7760458,
author={Parois, Ronan and Hamidouche, Wassim and Mora, Elie Gabriel and Raulet, Mickael and Deforges, Olivier},
booktitle={2016 24th European Signal Processing Conference (EUSIPCO)}, title={Real-time UHD scalable multi-layer HEVC encoder architecture},
year={2016},
volume={},
number={},
pages={1298-1302},
abstract={The High Efficiency Video Coding (HEVC) standard enables meeting new video quality demands such as Ultra High Definition (UHD). Its scalable extension (SHVC) allows encoding simultaneously different versions of a video, organised in layers. Thanks to inter-layer predictions, SHVC provides bit-rate savings over an equivalent HEVC simulcast encoding. Therefore, SHVC seems a promising solution for both broadcast and storage purposes. This paper proposes a multi-layer architecture of a pipeline of software HEVC encoder to perform real-time UHD spatially-scalable SHVC encoding. Inter-layer predictions are furthermore implemented to provide bit-rate savings with a minimum impact on complexity. The proposed architecture provides a good trade-off between coding gains and coding speed achieving real-time performance for 1080p60 and 1600p30 sequences in 2× spatial scalability. Moreover, experimental results show more than a 1000× speed-up compared to the SHVC reference software (SHM) and an introduced delay only reaching 14% of the equivalent HEVC coding speed.},
keywords={Encoding;Pipelines;Real-time systems;Video coding;Streaming media;Standards;Software},
doi={10.1109/EUSIPCO.2016.7760458},
ISSN={2076-1465},
month={Aug},}
@ARTICLE{7559706,
author={Biatek, Thibaud and Hamidouche, Wassim and Travers, Jean-Francois and Deforges, Olivier},
journal={IEEE Transactions on Broadcasting}, title={Optimal Bitrate Allocation in the Scalable HEVC Extension for the Deployment of UHD Services},
year={2016},
volume={62},
number={4},
pages={826-841},
abstract={Ultra high definition (UHD) is the latest trend in broadcasting area, which enables new services with 3840×2160 resolution and comes with enhanced color-gamut, frame-rate, dynamic range, and better audio system compared to the currently deployed HD services. The UHD format for broadcasting is already under standardization in the digital video broadcasting consortium which plans to introduce UHD services in three phases. The increase in data brought by these services requires more efficient compression and transmission systems. The recent scalable video coding standard scalable High Efficiency Video Coding (SHVC) is a promising candidate to handle these three phases while ensuring backward compatibility. Moreover, delivering such contents over networks needs an accurate control of the output bitrate from encoder engines to match rigid constraints on bandwidth and QoS. Several contributions have already been proposed to jointly encode scalable stream, but without considering the impact of bitrate ratio between layers on the compression performance. In this paper, the impact of the bitrate ratio between layers on the coding performance is first investigated for several UHD scalable schemes including spatial, color-gamut, and SDR-to-HDR scalability in SHVC. Based on this investigation, an adaptive rate control algorithm which dynamically allocates the bitrate between two layers is proposed to optimize the performance under quality and bitrate constraints. The algorithm has been implemented in the SHVC reference software (SHM9.0) and tested over 15 video sequences under two industrial usecases. The performance shows an average BD-BR improvement of 7.51% and 3.35% for these two use-cases.},
keywords={Bit rate;HDTV;Video coding;High definition video;Encoding;Standards;Scalability;HEVC;SHVC;rate-control;bit-allocation;HDTV;UHDTV},
doi={10.1109/TBC.2016.2599266},
ISSN={1557-9611},
month={Dec},}
@INPROCEEDINGS{7532734,
author={Blestel, Mederic and Ropert, Michael and Hamidouche, Wassim},
booktitle={2016 IEEE International Conference on Image Processing (ICIP)}, title={Generic statistical multiplexer with a parametrized bitrate allocation criteria},
year={2016},
volume={},
number={},
pages={2127-2131},
abstract={In this paper, we address the problem of the statistical multiplexing of video streams. Dynamic bitrate allocation is used to improve the overall video quality of a pool of channels. The balance is obtained by providing more bits to complex channels, while deprivations are applied to non-complex ones. In this study, the error minimization optimization of several compressed video is considered along with different metrics in order to exhibit a repartition key for bitrate sharing among all the channels. The goal of this approach is to introduce a reactivity parameter able to manage the bit transfer between channels. The validity of the parametric model is verified on two particular values, and compared to a static repartition solution.},
keywords={Bit rate;Complexity theory;Resource management;Distortion;Multiplexing;Minimization;Encoding;Statistical Multiplexeur;video coding;video streaming;AVC and HEVC},
doi={10.1109/ICIP.2016.7532734},
ISSN={2381-8549},
month={Sep.},}
@INPROCEEDINGS{7471903,
author={Biatek, T. and Hamidouche, W. and Travers, J.-F. and Deforges, O.},
booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Adaptive rate control algorithm for SHVC: Application to HD/UHD},
year={2016},
volume={},
number={},
pages={1382-1386},
abstract={Scalable video coding consists in compressing the video sequence into a layered bitstream where each layer refers to different spatial, temporal or quality representation of the video. Scalability enables compression gain compared to the simulcast encoding of layers thanks to inter-layer predictions. The scalable HEVC extension (SHVC) is the latest scalable technology promising up to 30% bitrate gains under the common test conditions, defined by JCT-VC. These conditions do not consider UHD and use fixed quantization step, which is not relevant in operational environment. In this paper, we propose an innovative adaptive rate control algorithm for SHVC. We consider HD as a base layer and UHD as an enhancement layer, with a constant global bitrate and a dynamic bitrate ratio adjustment between layers. The proposed algorithm is evaluated on a UHD data set where enables on average a BD-BR gain of 4.25% compared to a fixed-ratio encoding.},
keywords={Bit rate;Encoding;Video coding;Heuristic algorithms;Standards;Static VAr compensators;Scalability;HEVC;SHVC;UHD;HD;Rate-Control},
doi={10.1109/ICASSP.2016.7471903},
ISSN={2379-190X},
month={March},}
@ARTICLE{7273890,
author={Hamidouche, Wassim and Raulet, Mickael and Déforges, Olivier},
journal={IEEE Transactions on Circuits and Systems for Video Technology}, title={4K Real-Time and Parallel Software Video Decoder for Multilayer HEVC Extensions},
year={2016},
volume={26},
number={1},
pages={169-180},
abstract={Two High Efficiency Video Coding (HEVC) extensions, namely, the scalable HEVC (SHVC) extension and multiview HEVC (MV-HEVC) extension, have been finalized in July 2014 by the Moving Picture Experts Group and Video Coding Experts Group. These two extensions enable additional features not covered in the first version of the HEVC standard such as spatial, fidelity, bitdepth, and color gamut scalability, as well as stereoscopic and multiview representations. In this paper, we propose a software parallel decoder architecture for the HEVC standard and its multilayer extensions, including SHVC and MV-HEVC extensions. The decoder consists of multiple instances of the OpenHEVC decoder, one instance to decode each layer with a communication between dependent layers to perform inter-layer predictions. The proposed multilayer HEVC decoder is parallel friendly and supports both wavefront parallelism to simultaneously process adjacent rows of the frame and frame-based parallelism to decode a set of temporal and spatial frames in parallel. Moreover, the most time-consuming operation introduced in the SHVC extension, namely, the resampling of the inter-layer reference picture in spatial scalability, is optimized in single instruction multiple data for x86 platform. We assess the complexity of the multilayer HEVC decoder with respect to the simulcast configuration. The multilayer decoder decoding two SHVC layers introduces in average 40%-71% additional complexity compared with the single layer HEVC decoder. Moreover, the low level optimizations with a hybrid parallel processing solution enable a real-time decoding of 4Kp60 enhancement layer on a 6-core Intel i7 processor running at 3.4 GHz.},
keywords={Decoding;Parallel processing;Encoding;Scalability;Standards;Complexity theory;Streaming media;HEVC;multi-layer extensions;SHVC;MVHEVC;decoder complexity;low level optimizations and parallel processing;Decoder complexity;High Efficiency Video Coding (HEVC);low level optimizations and parallel processing;multilayer extensions;multiview HEVC (MV-HEVC);scalable HEVC (SHVC)},
doi={10.1109/TCSVT.2015.2478705},
ISSN={1558-2205},
month={Jan},}
@INPROCEEDINGS{7391264,
author={Biatek, Thibaud and Hamidouche, Wassim and Travers, Jean-Francois and Deforges, Olivier},
booktitle={2015 IEEE 5th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)}, title={Toward optimal bitrate allocation in the scalable HEVC extension: Application to UHDTV},
year={2015},
volume={},
number={},
pages={303-307},
abstract={Scalable video encoders compress a single video sequence to produce a bitstream composed of several layers, corresponding to different temporal, spatial and quality representations of the input video sequence. This technique improves the coding efficiency compared to simulcast encoding of each representation by exploiting additional correlations through inter-layer predictions. The latest scalable video coding standard SHVC -the extension of the recent HEVC standard-announces up to 30% bandwidth reduction. However, this gain is valid under the common test conditions, established by the JCT-VC expert group, which are not necessarily relevant in broadcasting environment and do not include video sequences in UHD resolution. In this paper, we expose the results of an extended study about the optimum gains that scalability can bring in concrete broadcast use-cases. Indeed, we consider SHVC with HDTV/UHDTV as spatial enhancement layers. Then, we search the optimum balance of layers' dedicated bitratres in sense of coding efficiency and objective quality for different UHDTV video sequences and use-cases.},
keywords={Encoding;Bit rate;Standards;Scalability;Video sequences;Video coding;HDTV;SHVC;HEVC;Bit Allocation;Rate Control;UHD},
doi={10.1109/ICCE-Berlin.2015.7391264},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7351373,
author={Farajallah, Mousa and Hamidouche, Wassim and Déforges, Olivier and Assad, Safwan El},
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, title={ROI encryption for the HEVC coded video contents},
year={2015},
volume={},
number={},
pages={3096-3100},
abstract={In this paper we investigate privacy protection for the HEVC standard based on the tile concept. Tiles in HEVC enable the video to be split into independent rectangular regions. Two solutions are proposed to encrypt the tiles containing the Region Of Interest (ROI). The first solution performs encryption at the bitstream level by encrypting all HEVC syntax elements within the ROI tiles. The second solution enables a selective encryption of the ROI tiles under constant bitrate and format compliant requirements. To avoid temporal propagation of the encryption outside the ROI boundaries caused by inter prediction, the motion vectors of non ROI regions are restricted inside the non encrypted tiles in the reference frames. Simulation results show that the proposed solutions perform secure and adaptive encryption of ROI in the HEVC video. Moreover, the bitrate overhead caused by the MVs restriction window varies between 1%-2.5% depending on both the video content and the number of tiles within the frame.},
keywords={Encryption;Encoding;Standards;Syntactics;Yttrium;Privacy},
doi={10.1109/ICIP.2015.7351373},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7178273,
author={Hamidouche, W. and Farajallah, M. and Raulet, M. and Déforges, O. and El Assad, S.},
booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Selective video encryption using chaotic system in the SHVC extension},
year={2015},
volume={},
number={},
pages={1762-1766},
abstract={In this paper we investigate a selective video encryption in the scalable HEVC extension (SHVC). The SHVC extension encodes the video in several layers corresponding to different spatial and quality representations of the video. We propose a selective encryption solution using a chaotic-based encryption system. The proposed solution encrypts a set of sensitive parameters with a minimum complexity overhead, at constant bitrate and SHVC format compliant. Experimental results compare the performance of three encryption schemes: encrypt only the lowest layer, all layers, and only the highest layer. The first two schemes achieve a high security level with a drastic degradation in the decoded video, while the last scheme enables a perceptual video encryption by decreasing the quality of the highest layer below the quality of the clear layers.},
keywords={Encryption;Chaos;Streaming media;Syntactics;Bit rate;Video coding;Selective video encryption;SHVC},
doi={10.1109/ICASSP.2015.7178273},
ISSN={2379-190X},
month={April},}
@INPROCEEDINGS{7025442,
author={Martin, Benoit and Hamidouche, Wassim and Le Feuvre, Jean and Raulet, Michael},
booktitle={2014 IEEE International Conference on Image Processing (ICIP)}, title={Unified real time software decoder for HEVC extensions},
year={2014},
volume={},
number={},
pages={2186-2188},
abstract={There are several High Efficiency Video Coding (HEVC) extensions providing new tools on the top of a common HEVC base layer. These HEVC extensions enable higher temporal, spatial or quality of the video, 3-D rendering and range extension. In addition to the conforming HEVC decoder, the end-user need to support the decoding of the HEVC extension to benefits from its tools. In this paper we propose an unified software decoder enabling to decode all HEVC extensions. This solution is based on the open source project OpenHEVC which implements a conforming HEVC decoder. The new tools defined in HEVC extension are implemented and integrated into the OpenHEVC decoder. We show the first end-to-end video streaming demonstration of the HEVC extensions with the OpenHEVC decoder and the GPAC player. The GPAC server streams one HEVC base layer and two enhancement layers. At the client side, GPAC player uses the OpenHEVC to decode the base layer for HD resolution and can also decode whether the first EL for 4K resolution or the second EL for 3-D rendering.},
keywords={Decoding;Video coding;Standards;Software;Three-dimensional displays;High definition video;Encoding;Real time decoder;HEVC;SHVC;MV-HEVC;3D-HEVC},
doi={10.1109/ICIP.2014.7025442},
ISSN={2381-8549},
month={Oct},}
@INPROCEEDINGS{7025426,
author={Hamidouche, Wassim and Raulet, Michael and Déforges, Olivier},
booktitle={2014 IEEE International Conference on Image Processing (ICIP)}, title={Real time SHVC decoder: Implementation and complexity analysis},
year={2014},
volume={},
number={},
pages={2125-2129},
abstract={The Scalable High efficiency Video Coding (SHVC) standard is developed to offer spatial and quality scalability with high coding efficiency. In this paper we investigate a complexity analysis of a real time and parallel SHVC decoder. We first provide details on the implementation of the SHVC decoder including its low level optimizations. Furthermore, we introduce parallelism tools integrated in the SHVC software for parallel decoding. These tools include frame-based parallelism to decode a set of temporal and spatial frames in parallel as well as wavefront parallelism to simultaneously process separated regions of a picture. We assessed through experimental results the complexity of the real time SHVC decoder in different coding configurations. The SHVC decoder with two layers introduces in average an additional complexity of 43 to 80% in respect to a simulcast configuration. The low level optimizations together with a hybrid parallelism solution enables a real time decoding of 1600p40 enhancement layer on an Intel i7 processor.},
keywords={Decoding;Parallel processing;Scalability;Complexity theory;Signal to noise ratio;Instruction sets;Video coding;SHVC;software implementation},
doi={10.1109/ICIP.2014.7025426},
ISSN={2381-8549},
month={Oct},}
@INPROCEEDINGS{6890613,
author={Hamidouche, W. and Cocherel, G. and Le Feuvre, J. and Raulet, M. and Déforges, O.},
booktitle={2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)}, title={4K real time video streaming with SHVC decoder and GPAC player},
year={2014},
volume={},
number={},
pages={1-2},
abstract={This paper presents the first 4Kp30 end-to-end video streaming demonstration based on the upcoming Scalable High efficiency Video Coding (SHVC) standard. The optimized and parallel SHVC decoder is used under the GPAC player to decode and display in real time the received SHVC layers. The SHVC reference software model (SHM) is used to encode the 4K original video in two spatial scalability layers: the base layer at 1080p resolution and the enhancement layer at 2160p resolution. The SHVC bitstream is encapsulated with the GPAC multimedia library into MP4 file format. The GPAC player at the server side broadcasts the MP4 content in MPEG-2 TS. At the client side, the GPAC player receives the SHVC video packets which are decoded by the SHVC decoder and then rendered in real time by the player. The GPAC player provides an interactive interface enabling to switch between displaying the base and the enhancement layers.},
keywords={Decoding;Streaming media;Standards;High definition video;Digital video broadcasting;Satellites;Satellite broadcasting;End-to-end 4K video streaming;SHVC standard;parallel processing;DVB-T2;DVB-S2},
doi={10.1109/ICMEW.2014.6890613},
ISSN={1945-7871},
month={July},}
@INPROCEEDINGS{6890300,
author={Hamidouche, Wassim and Raulet, Michael and Deforges, Olivier},
booktitle={2014 IEEE International Conference on Multimedia and Expo (ICME)}, title={Parallel SHVC decoder: Implementation and analysis},
year={2014},
volume={},
number={},
pages={1-6},
abstract={The new Scalable High efficiency Video Coding (SHVC) standard is based on a multi-loop coding structure which requires the total decoding of all intermediate layers. The decoding complexity becomes then a real issue, especially for a real time decoding of ultra high video resolutions. A parallel processing architecture is proposed to reduce both the decoding time and the latency of the SHVC decoder. The proposed solution combines the high level parallel processing solutions defined in the HEVC standard with an extension of the frame-based parallelism. The latter solution enables the decoding of several spatial and temporal SHVC frames in parallel to enhance both decoding frame rate and latency. The wavefront parallel processing solution is used for more coarse level of granularity. The proposed hybrid parallel processing approach achieves a near optimal speedup and provides a good trade-off between decoding time, latency and memory usage. On a 6 cores Xeon processor, the parallel SHVC decoder performs a real time decoding of 1600p60 video resolution.},
keywords={Decoding;Parallel processing;Encoding;Instruction sets;Standards;Memory management;Scalability;HEVC;SHVC;parallel processing},
doi={10.1109/ICME.2014.6890300},
ISSN={1945-788X},
month={July},}
@INPROCEEDINGS{6855067,
author={Hamidouche, Wassim and Raulet, Michael and Déforges, Olivier},
booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Multi-core software architecture for the scalable HEVC decoder},
year={2014},
volume={},
number={},
pages={7545-7549},
abstract={The scalable high efficiency video coding (SHVC) standard aims to provide features of temporal, spatial and quality scalability. In this paper we investigate a pipeline and parallel software architecture for the SHVC decoder. The proposed architecture is based on the OpenHEVC software which implements the high efficiency video coding (HEVC) decoder. The architecture of the SHVC decoder enables two levels of parallelism. The first level decodes the base layer and the enhancement layers in parallel. The second level of parallelism performs the decoding of both the base layer and enhancement layers in parallel through the HEVC high level parallel processing solutions, including tile and wavefront. Up to the best of our knowledge, it is the first real time and parallel software implementation of the SHVC decoder. On an Intel Xeon processor running at 3.2 GHz, the SHVC decoder reaches the decoding of 1600p enhancement layer at 40 fps for x1.5 spatial scalability with using six concurent threads.},
keywords={Decoding;Pipelines;Video coding;Encoding;Standards;Parallel processing;Scalability;HEVC;Scalable HEVC;High level parallel processing and wavefront parallel processing},
doi={10.1109/ICASSP.2014.6855067},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{5496085,
author={Hamidouche, Wassim and Perrine, Clency and Pousset, Yannis and Olivier, Christian},
booktitle={2010 IEEE International Conference on Acoustics, Speech and Signal Processing}, title={Optimal solution for SVC-based video transmission over a realistic mimo channel using precoder designs},
year={2010},
volume={},
number={},
pages={2346-2349},
abstract={In this paper we propose a novel scheme for SVC-based video transmission over MIMO channels using precoder solutions. On the one hand, H.264/SVC codec ensures spatial, temporal and quality scalabilities and provides an intrinsic hierarchy over transmitted bitstreams. On the other hand, precoder designs, which decouple a MIMO channel into parallel and independent SISOsub-channels, offer a high BER performance with a hierarchy over the sub-channels. The proposed scheme exploits the scalability of the H.264/SVC codec jointly with four precoder solutions providing an UEP without any surplus redundancy. Besides, some of these precoders such as QoS and E-dmin allow a high flexibility on the power allocation across the sub-channels. This scalability is used to analytically draw the bandwidth allocation problem for H.264/SVC transmission over MIMO channels. The solution of this problem allows an optimal selection of each transmission bloc parameter approaching the optimal solution in term of Rate Distortion (RD) criterion. The simulation results show the performance of the proposed scheme over both statistical and realistic MIMO channels. Moreover, the accuracy of these precoder solutions against the Channel Estimation (CE) errors is investigated in different user mobility speeds.},
keywords={MIMO;Static VAr compensators;Scalability;Error correction codes;Bit error rate;Channel allocation;Codecs;Redundancy;OFDM;Signal to noise ratio;H.264/SVC;MIMO;Precoder solutions;Optimal bandwidth allocation;QoS precoder;E-dmin precoder;OFDM},
doi={10.1109/ICASSP.2010.5496085},
ISSN={2379-190X},
month={March},}
@INPROCEEDINGS{5450260,
author={Hamidouche, Wassim and Vauzelle, Rodolphe and Olivier, Christian and Pousset, Yannis and Perrine, Clency},
booktitle={2009 IEEE 20th International Symposium on Personal, Indoor and Mobile Radio Communications}, title={Impact of realistic MIMO physical layer on video transmission over mobile Ad Hoc network},
year={2009},
volume={},
number={},
pages={187-191},
abstract={In this paper we investigate the impact of a realistic physical layer on the H.264/AVC video transmission over Ad Hoc networks in urban environment. We propose a realistic Multiple Input Multiple Output (MIMO) physical layer which combines a determinist propagation model and a fine-grained model of wireless transmission errors. The determinist propagation model takes into account all the environmental characteristics (geometric and electric) and provides all the information of the multi-path channel (received power, complex impulse response). The wireless transmission errors model is based on a BER computation. The BER is calculated according to 802.11n standard to evaluate MIMO wireless links and, then, is compared to both SISO configuration and an existing wireless errors model using empirical propagation models. In the case of a SISO configuration, the BER is computed according to 802.11a standard. The simulation results show clearly a significant difference in term of QoS for the video transmission using realistic and empirical physical layer. In addition, the MIMO system, compared to a SISO one, improves the quality of links in the network and, thus, provides a better QoS for video transmission over Ad Hoc networks.},
keywords={MIMO;Physical layer;Mobile ad hoc networks;Ad hoc networks;Error correction codes;Automatic voltage control;Bit error rate;Videoconference;Video compression;Power system modeling;Physical Layer;MIMO;IEEE 802.11a/n;Outdoor environment;3D Ray Tracing;H.264/AVC},
doi={10.1109/PIMRC.2009.5450260},
ISSN={2166-9589},
month={Sep.},}
@INPROCEEDINGS{5399384,
author={Combeau, P. and Paillot, J. M. and Vauzelle, R. and Cordeau, D. and Pousset, Y. and Hamidouche, W.},
booktitle={2009 9th International Conference on Intelligent Transport Systems Telecommunications, (ITST)}, title={Conception and application of smart antennas for transport applications},
year={2009},
volume={},
number={},
pages={45-50},
abstract={This paper presents, in a first time, an original method of smart antenna design, based on an antenna array driven by vector modulators. Different radiation patterns are measured to validate the method. In a second time, we show the interest in using such antenna array in transports domain. Thus, we first integrate these radiation patterns into a 3D propagation simulator based on Ray-Tracing, to achieve deterministic simulations of wave propagation in site specific environment (dense urban, urban, suburban). Then, we perform some simulations of an OFDM communication on a mobility scenario in a realistic urban environment taking into account antennas and channel behavior. Significant gains in terms of power consumption, electromagnetic de-pollution and Bit Error Rate (BER) are provided with smart antennas in comparison with dipole antenna.},
keywords={Antennas and propagation;Dipole antennas;Antenna arrays;Antenna radiation patterns;Bit error rate;Antenna measurements;Ray tracing;Electromagnetic propagation;OFDM;Energy consumption;Smart antennas;waves propagation modeling;channel behavior;digital communication;intelligent transport;Antennas and Propagation for ITS},
doi={10.1109/ITST.2009.5399384},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5399323,
author={Poussard, Anne-Marie and Hamidouche, Wassim and Vauzelle, Rodolphe and Pousset, Yannis and Parrein, Benoît},
booktitle={2009 9th International Conference on Intelligent Transport Systems Telecommunications, (ITST)}, title={Realistic SISO and MIMO physical layer implemented in two routing protocols for vehicular ad hoc network},
year={2009},
volume={},
number={},
pages={393-397},
abstract={In the most of ad hoc network simulators, the physical layer is considered with a simple approach. Moreover, the information routing is realized in order to only minimize the number of hops or the delay. In this paper, the authors propose two contributions: the first one consists in considering a 3D propagation model taking into account the characteristics of the propagation environment for SISO and MIMO physical layers; the second one allows to introduce the BER as a metric of quality of the radio link used in two routing protocols. The impact of these two contributions is evaluated in two environments.},
keywords={MIMO;Physical layer;Routing protocols;Ad hoc networks;Broadcasting;Databases;Road vehicles;Spread spectrum communication;Broadcast technology;Information technology;multi-paths radio channel;physical layer;BER;metric of quality;information routing;vehicle to vehicle communication},
doi={10.1109/ITST.2009.5399323},
ISSN={},
month={Oct},}

@INPROCEEDINGS{5176065,
author={Hamidouche, Wassim and Olivier, Christian and Babel, Marie and Deforges, Olivier and Boeglen, Hervé and Lorenz, Pascal},
booktitle={2009 Second International Conference on Communication Theory, Reliability, and Quality of Service}, title={LAR Image Transmission over Fading Channels: A Hierarchical Protection Solution},
year={2009},
volume={},
number={},
pages={32-36},
abstract={The aim of this paper is to present an efficient scheme to transmit a compressed digital image over a non frequency selective Rayleigh fading channel. The proposed scheme is based on the locally adaptive resolution (LAR) algorithm, and the Reed-Solomon error correcting code is used to protect the data against the channel errors. In order to optimize the protection rate and ensure better protection we introduce an unequal error protection (UEP) strategy, where we take the hierarchy of the information into account. The digital communication system also includes appropriate interleaving and differential modulation. Simulation results clearly show that our scheme presents an efficient solution for image transmission over wireless channels, and provides a high quality of service, outperforming the JPWL scheme in high bit error rate conditions.},
keywords={Image communication;Fading;Protection;Error correction codes;Image coding;Digital images;Frequency;Reed-Solomon codes;Digital communication;Interleaved codes;Fading channels;Locally Adaptive Resolution (LAR);scalability;Unequal Error Protection (UEP);(JPWL)},
doi={10.1109/CTRQ.2009.19},
ISSN={},
month={July},}